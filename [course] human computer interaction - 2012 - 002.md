# Human-Computer Interaction - 2012 - 002

(via Coursera)

## Lecture notes

### 1.1: Human Computer Interaction

-	Often iterative.
-	Focus on users.
-	Good design shifts user's attention from manipulating the interface to accomplishing a task.
-	People's tasks, goals, and values drive development.
-	Work with users, but they're just one stakeholder.

### 1.2: The Power of Prototyping

-	Reflective conversation with materials.
-	Goal is not artifact, it is feedback.
-	Question rendered as an artifact for other designers or users. Ask lots of questions.
-	Prototypes are nearly always, and should be, incomplete.
-	Prototyping is a strategy for efficiently dealing with things that are hard to predict.
-	Focus on goals, evolve the designs.
-	The rights of a prototype:
	-	Should not be required to be complete.
	-	Should be easy to change.
	-	Gets to retire.
-	Rapid prototyping is simulated annealing; local improvement isn't enough.

### 1.3: Evaluating Designs

-	Different methods achieve different goals.
-	Iteratively bring people into your office and watch them (usability studies).
	-	Setting isn't valid, but can stil learn lessons.
	-	Experimental bias; users try harder, be nicer.
	-	Psychological burden to bring someone to you.
-	Surveys
	-	Easy to execute and analyse results.
	-	Difference between what people say and what people do.
-	Focus Groups
	-	Groups of people different. People more submissive or erratic due to group psychology.
	-	Difficult for taboo topics or sexual topics.
-	Feedback from Experts
	-	Heuristic evaluation.
-	Comparative Experiments.
	-	Which option is more effective.
	-	Better then surveys because you see what users do.
	-	Better than usability studies because involves more than one option.
	-	But if online can't see them, and if in person the contrived setting may bias results.
-	Participant Observation.
	-	What users do in their actual scenario, not just short term.
-	Simulation models.
-	Issues to consider
	-	Reliability / precision
	-	Generalisability.
	-	Realism.
	-	Comparison.
	-	Work involved.

### 1.4: The Birth of HCI.

-	Vannevar Bush wrote article in 1940 in the Atlantic about Memex.
	-	Envisioned how future coulod allow people to produce and consume infomation.
	-	Camera on head, hypertext.
-	Grace Hopper invented the compiler and their interfaces.
-	Graphical User Interface.
	-	Input over output, so more intuitive.
-	Doug Engelbart invented the mouse.
-	Alan Kay, at Xerox PARC, prototyped the Dynabook, created the WIMP Windows-Icon-Mouse-Pointer (WIMP) GUI.

### 2.1: Participant Observation

-	"You can observe a lot just by watching".
-	Tacit knowledge: deep hanging out.
-	Five questions:
	-	What do people do now?
	-	What values and goals do people have?
	-	How are these activities embedded in larger ecology?
		-	e.g. buses. What leads people to need buses?
		-	Broaden your scope.
	-	Similarities and differences across people.
	-	â€¦and other types of context, like time of day.
-	Apprentice yourself.
	-	Set up partnership with observee.
	-	Be taught the process.
	-	Observer practices.
	-	Validate your observations; ask observee what they mean.
-	Pay attention to all artifacts. 
	-	**Look for workarounds and hacks.**
	-	**Errors are a goldmine**.
-	**Difference between what people say and what people do.**

### 2.2: Interviewing

-	Choosing participants.
	-	May be current users, or also non-users but potential future users.
	-	Use incentives and motivation; in SF $50-100, or gift certificate.
	-	Approximate if necessary; better than nothing.
-	Malcolm Gladwell
	-	Everything is interesting. Be curious.
	-	Don't start at the top or bottom. Start at the middle. Talk to the people who actually do the work.
	-	People at the top have power to lose, not as interested in sharing knowledge.
-	Bad questions.
	-	No: "Is the daily update an important feature to you?"
	-	Don't lead people. Use data to back up open ended questions.
	-	Yes: "Judging by logs I see that you don't use the daily update feature? Tell me more. Why is that?"
	-	No: "What would you like in a tool?".
	-	Users don't know about design, can't help you like this.
	-	Other types of question to avoid:
		-	Likes and wants in hypothetical scenarios.
		-	How often they do things.
		-	How much they like things on an absolute scale.
		-	Avoid binary questions.
-	Good questions
	-	Open ended, especially at beginning.
	-	(A little bit of) silence is golden. Let them answer.

### 2.3: Additional needfinding strategies

-	Longitudinal or sporadic behavior?
	-	**Diary studies.**
		-	Give them a diary, complete at a specified time or interval.
		-	Structured task.
		-	Written, camera, voice; but tailor to context.
		-	Scales better than direct observation.
		-	Entries must be as frictionless as possible. Better results.
		-	May require practice, training, reminding.
-	**Experience Sampling, aka pager studies**
	-	Phone beeps, then you fill in a form.
	-	Psychometric.
-	**Lead users**
	-	Individually create solutions to their problems.
	-	Designers collaborate to bring their solutions to market.
	-	Before early adopters.
	-	Don't work well if the improvement is process or knowledge based; difficult to diffuse / market.
-	**Extreme users.**
	-	Everyone gets email. But some people get a lot.
	-	Sometimes some useful ideas, but they're not representitive users.
-	Keeping users in mind.
	-	Don't forget users!
	-	**Personas**.
		-	A model of a person, an example.
		-	Demographics, motivation, beliefs, intentions, behaviour, and goals.
		-	Draw a picture or have a photo of persona.
		-	Name, occupation, background, social situation, hopes, dreams, and goals. A story!
		-	You can build empathy. Empathy leads to insights.
		-	Keep design consistent over time.
-	Ultimately, it's about design. This will help you find one, but not strict.

### 3.1: Paper Prototypes and Mockups

-	Over course of project fidelity increases.
	-	Storyboard, lowest.
	-	Paper prototypes, low.
	-	Digital mockups, low.
	-	Onwards.
-	**Don't focus on user interface before focusing on the task at hand**.
-	**Storyboarding isn't about pretty pictures, it's about communicating ideas**.
-	Star people
	-	Circle, then body like a star. Very crude. Then add sight lines, boards.
-	First objective: **illustrate a goal**.
-	Last objective: **people accomplishing goals**.
-	Storyboards should convey:
	-	**Settings**.
		-	People, environment, tasks.
	-	**Sequence**
		-	Steps.
		-	What leads someone to use the app, impetus?
		-	What task is being illustrated?
	-	**Satisfaction**
		-	What motivates people to use system?
		-	What does it enable people to accomplish?
		-	What need does the system fill?
-	Benefits of storyboarding
	-	**Holistic**. Emphasize how interfaces accomplish tasks without commiting to particular interfaces.
	-	Gets all the stakeholders on the same page with respect to the goal.
-	Time limits help, e.g. 10 minutes per panel.
-	Then, **paper prototyping**.
-	Paper prototyping tips and tricks
	-	Get a physical box and put all materials in one place! easy to lose or damage it.
	-	Work quickly, reuse by e.g. photocopying.
	-	If something is difficult to simulate allow users to ask verbal questions.
	-	Mix and match hardware and software. Print out a photo of the hardware then put software elements in it.
-	**Test multiple paper prototypes simultaneously**.
-	Get users or other stakeholders to help design.
-	Form and feedback co-evolve. Fidelity increases, feedback detail increases.
-	Further reading:
	-	Bill Buxton, Sketching User Experiences.
	-	Bill Moggridge, designing Interactions.
	-	Carolyn Snyder, Paper Prototyping.
	-	Michael Schrage, Serious Play
	-	Houde and Hill, What do Prototypes Prototype?
	-	Todd Zaki Warfel, Prototyping

### 3.2: Faking it - Wizard-of-Oz Prototyping

-	Simulate machine behaviour with human operators.
-	Make an interactive application without much code.
-	Get feedback from users.
	-	Hi fidelity => users think it's real, more reluctant to be critical.
	-	Low fidelity => more license to suggest changes.
-	Steps:
	1.	Map out scenarios and application flow.
		-	What happens in response to user behavour?
	2.	Put together interface skeletons.
	3.	Develop "hooks" for wizard input.
	4.	Where and how the wizard will provide input.
		-	Remember you'll eventually replace with software.
	5.	Rehearse wizard role with a colleague.
-	Running wizard-powered prototypes:
	-	Practice with a friend first.
	-	Once comfortable recruit "users": train stations, cafes, etc.
	-	Two roles: facilitator and wizard.
		-	Facilitator: provides tasks and takes notes.
		-	Wizard: operator interface (more authenticate if hidden or remote).
	-	User feedback could be:
		-	Think aloud.
		-	Retrospective (if thinking aloud is distracting).
		-	Heuristic evaluation
		-	Debrief and reward users.
-	Advantages:
	-	Faster, cheaper, quicker iterations.
	-	Creating multiple variations easy.
	-	More "real" than paper prototypes.
	-	Identifies bugs and problems with current design.
	-	User-centric.
	-	Can evision very difficult ideas now.
	-	Designers learn too.
-	Disadvantages:
	-	May be too optimistic about technology, e.g. perfect speech recognition.
	-	Technology may never be possible.
	-	Some features or limitations are too difficult to wizard.
-	Further reading:
	-	http://speckyboy.com/2012/06/24/10-effective-video-examples-of-paper-prototyping
	-	www.elsevierdirect.com/companion.jsp?ISBN=9780123740373
	-	Stephen Dow
	
### 3.3: Faking It - Video Prototyping

-	Benefits:
	1.	Cheap and fast.
	2.	Great communication tools.
		-	Shows context, self-explanatory.
	3.	Development spec.
	4.	Ties interface designs to tasks.
		-	Check completeness of interface.
		-	Check nothing extra there.
-	Any fidelity, but usually low.
	-	Often coupled with paper prototypes.
-	Content
	-	Like storyboard, the whole task, including motivation (impetus) and success (narrative).
	-	Draw on tasks you've observed.
	-	Illustrate important tasks, MVP.
-	Steps:
	1.	Start with outline, or your storyboards.
	2.	Camera, people, realistic location.
	3.	Focus on message rather than production value.
-	Considerations:
	-	Audio or silent? Audio can be finicky, can use cue cards instead.
	-	Interface can be paper, mock-ups, code, or invisible (just showing task)
	-	Can show both success and failure.
	-	Edit as little as possible, editing sucks.
-	Furher reading:
	-	Wendy MacKay.

### 3.4: Creating and comparing alternatives

-	Duncker: **functional fixation**
	-	Once objects are biased to a particular purpose it's difficult to see them otherwise.
-	**Prototype in parallel**, not in serial.
	-	More diverse output, simulated annealing escapes local maxima.
	-	Better rated output.
-	Separating ego from artifact.
	-	If you have different ideas, it's easier to see that judgements are based on your artifacts.
-	Parallel encourages comparison, and comparison aids learning.
-	**Create and share multiple designs**.
	-	Feel better in team environments.
	-	Alternatives provides a vocabulary for teams to discuss the space of possible solutions.

### 4.1: Heuristic Evaluation, why and how.

-	Empirical: assess with users.
-	Formal: models and formulas => measured.
-	Automated: software measures.
-	Critique: Expertise and heuristic feedback.
	-	When to get critique?
	-	**Before user testing**. Allows user testing to focus on big issues.
	-	**Before redesigning**. What to keep, what to throw away.
	-	**Problems known, but need evidence**. You get complaints, need to articulate.
	-	**Before release**: Smooth out rough edges.
-	**Begin with clear goal**.
-	**Heuristic Evaluation**.
	-	Jakob Nielsen, ten heuristics.
	-	3-5 independent evaluators, on working UI or sketches.
	-	Universal principles.
	-	Process.
		-	Give them some tasks. Execute each task several times, stepping through the design.
		-	Constantly refer to Nielsen's heuristics and category-specific heuristics.
		-	Use violations to redesign and fix.
-	Why multiple evaluators?
	-	Some evaluators find more problems than others.
	-	No evaluator finds all problems.
	-	But: decreasing returns, cost of evaluation.
-	Heuristics vs. user testing.
	-	HE is faster.
	-	HE results are pre-interpreted.
	-	UT is more accurate (by definition).
	-	HE can save participants for further testing.
-	Heuristic evaluation phases
	-	**Pre-evaluation training**. Domain knowledge, information on scenarios.
	-	**Evaluation**. Independent, then aggregate. Use **severity ratings**.
	-	**Debriefing**. Review with design team.
	-	At least two passes for each evalutor. One to get the flow, second for focus.
	-	Produce list of specific problems from a list.
		-	Issue, severity rating, heuristics violated, description.
-	Severity rating
	-	Frequency, impact, persistence.
	-	Allocate resources to problems.

### 4.2: Design Heuristics (part 1 of 2)

-	**Show system status**.
	-	wrt. response time.
		-	> 1s => spinner.
		-	>> 1s => fractional progress indication.
	-	wrt space, e.g. disk space.
	-	wrt change, e.g. unsaved changes.
	-	wrt action, e.g. traffic lights.
		-	Traffic lights are redundant; red and on top, green and on bottom. Colour blindness.
	-	wrt next steps. What happened, and what will happen next?
	-	wrt completion. e.g. dialog saying done! 
-	**Familiar metaphors and language**
	-	Identify terms and language familiar to users.
	-	Familiar categories.
	-	For esoteric errors explain what actions are available and their consequences.
-	**User control and freedom**
	-	Undo/redo.
	-	Not forcing people down certain paths. Freedom to explore.
	-	Preview paths; it helps them explore. e.g. flight ticket search with calendar of cheapest prices on days.
	-	But context-dependent. e.g. a wizard is easier if very contrained, but bad for experts.
-	**Consistency and standards**
	-	Placement of controls.
	-	Consistent names. User-centric consistency.
	-	Clear choices. Use actions and verbs, not Yes and No.
-	**Error prevention**
	-	Prevent data loss. Warn.
	-	Prevent clutter. Too much means can't see options.
	-	Prevent confusing flow and use safe defaults.
	-	Prevent bad input.
	-	Prevent unnecessary constaints. e.g. free-text shouldn't be constained by category. Offer the option but don't contrain.

### 4.3: Design Heuristics (part 2 of 2)

-	**Recognition over recall**
	-	Avoid codes.  Else you'll see post-it notes for people trying to help remember them.
	-	Lead with reasonable defaults to avoid awkward intermediary steps.
	-	Previews allow recognition and more efficient.
-	**Flexibility and efficiency**
	-	Shortcuts for experts.
	-	Use defaults but also simultaneously show options for flexibility. e.g. popular cities in a combo box, text field for other.
	-	Ambient information using icons / sparklines in a dashboard.
	-	Proactivity. Offer options based on behaviour rather than waiting for user.
		-	Task relevant, don't interrupt flow.
	-	Recommendations.
		-	Keep it relevant.
	-	Don't go overboard, options have a cost.
-	**Aethetics and minimalist design**
	-	Above the fold.  Push common and mainline information up.
	-	Signal to noise.
		-	Judicious use of colour.
		-	Keep chrome down.
		-	Collapse login and register page into one page.
	-	Avoid redundancy.
	-	Avoid unused features.
-	**Recognize, Diagnose, and Recover from Errors**
	-	Make problem clear. Where is the error?
	-	Provide a solution.
	-	Show a path forward.
	-	Propose an alternative. If nothing is found or hit an error, suggest alternatives. "Smart rexlaxation".
-	**Help**
	-	Examples.
	-	Explain choices using examples, e.g. content of a prospective newsletter.
	-	Explain the options of escaping an error.
	-	Help show the steps. If a sequence of steps required give clues about where to find them.
	-	Help point things out. Highlight elements of UI.
	-	Provide more information.
	-	Be honest and clear, e.g. human-friendly EULAs.
	-	Humour, let users have fun.

### 5.1: Direct manipulation

-	How to improve a measuring cup?
	-	Even after user survey of people not complaining about how long it takes to make a measurement, participant observation => inefficient to make readings.
	-	Be able to make measurements just be looking straight down.
-	Simply asking people what they want will miss important opportunities.
-	Go out into the field, especially with prototypes.
-	**The Gulf of Execution**
	-	How does the user know what to do? ("Do?")
-	**The Gulf of Evaluation**
	-	How does the user know what happened? ("Now?")
-	Six questions.
	-	Determine the function of the device?
	-	Tell what actions are possible?
	-	Determine mapping from intention to physical movement?
	-	Perform the action?
	-	Tell what state the system is in? Desired state?
	-	Determine mapping from system state to interpretation.
-	To reduce gulfs.
	-	**Visibility** (affordances, signifiers)
	-	**Feedback**
	-	**Consistency** (standards)
	-	**Non-destructive operations** (undo)
	-	**Discoverability** (systematic exploration possible)
	-	**Reliability** (consistent behaviour, it works).
-	Command line vs. GUI
	-	GUI offers continuous feedback.
	-	GUI options are all visisible. Discoverable.
-	GUI offers **direct manipulation**
	-	**Immediate feedback on actions**
	-	**Continuous representations of objects**
	-	Leverage metaphor.
-	But when is the command line better?
	-	**Successful indirection**.
	-	Express and combine abstract actions.
-	Don Norman, The Design of Everyday Things

### 5.2: Mental Models

-	What makes an interface learnable?
-	What leads to errors?
-	Goal: **design beacons the right model**.
	-	User's model develops through interaction with system.
	-	Designer's expect user's model to match designer's...but often no!
	-	Mismatched models lead to slow performance, errors, frustration.
-	**Mental models <= experience, metaphor, analogical reasoning.**
	-	"A text processor is like a typewriter". Encourages users to transer skills and beliefs over.
	-	User models incomplete, vary over time, superstitious.
-	**Slip**: right model, accidentally do wrong thing.
	-	Prevent/fix via ergonomics, visual design.
-	**Mistake**: do what you want to do but wrong model.
	-	Prevent/fix via feedback, improve user's perception of affordances (visibility).
-	Butterfly ballot in 2000 example of mistake.
-	**Consistency and re-use reduce mistakes.**
-	**Leverage real-world metaphors**.
	-	Direct manipulation provides this.
-	New technology necessarily different from what users used to; minimize the gap.
-	To learn more:
	-	JM Carroll, HR Olson, *Mental models in human-computer interaction: Research issues*, 1987
	-	Don Norman, *Design of Everyday Things*
	-	James Reason, *Human Error*.
	
### 5.3 Representation Matters

-	The Oranges Puzzle, The Bagels Puzzle
	-	Like Towers of Hanoi, but food.
-	Bagels => **representation of problem enforces constraints**, easier to handle, less stress for working memory.
-	The Number Game.
	-	Two players, take numbers [1, 9] without replacement, first to sum to 15 wins.
	-	Just in your head is difficult!
	-	With cards a bit easier.
-	Tic Tac Toe and Number Game isomorphs!
	-	Magic square, sums to 15.
-	"Solving a problem simply means representing it so as to make the solution transparent". - Herbert Simon.
-	**Working memory**.
	-	Recall heuristic recognition not recall.
	-	**Embed constraints in user interface**.
-	e.g. Getting Things Done
	-	One rule is when you realise you need to do something write it down. Relieves pressure on working memory.
-	**Naturalness Principle**: properties of representation match properties of the thing being represented.
-	**World in miniature**. Fit into small diagram to illustrate summary and context of errors.

### 5.4: Distributing Cognition (part 1 of 2)

-	Think more fluidly by **distributing cognition** into artifacts of the world.
	-	Encourage experimentation. (Tetris)
	-	Scaffold learning, reduce errors through redundancy. (Montessouri blocks)
	-	Show only differences that matter. (London Underground)
	-	Convert slow calculation into fast perception. (Map colouring)
	-	Chunking. (Chess and gestures)
	-	Efficiency. (diagrams, GUI)
	-	Collaboration. (cockpits)
-	**Cheap experimentation**
	-	e.g. Tetris. Experts more likely to move pieces around in attempt to determine fit. Offload cognition onto the interface.
-	**Scaffold learning**
	-	Redundantly represent abstract concepts. Reduce errors.
-	A good representation:
	-	**shows all relevant information, and nothing else.**
	-	enable comparison, exploration, problem solving.
	-	e.g. London Underground map.
		-	**focus plus context representation**		-	Where detail matters, e.g. centre, higher fidelity.
		-	where detail doesn't matter, e.g. suburbs, less fidelity.
		-	make some tasks easier (get from A to B), necessarily makes others more difficult (measure distance).
-	**Nearly all representational design is about fitness to task.**	
-	Weather Underground.
	-	Better if temperature scannable; colour or scale nodes.
-	Edward Tufte, height above sea level map.
	-	Hue not comparable (ROY G BIV), more qualitative.
	-	**Colour as a meaningful representional cue**.
	-	Earth tones above sea level, blue tones below sea level.
	-	Luminance is comparable.
-	**Chunking**.
	-	Also expertise to build if interface is chunkable, fits in memory.
	
### 5.5: Distributing Cognition (part 2 of 2)

-	**Informational Equivalence != Computational Equivalence**
	-	Same content, but different representation, => different efficiency.
	-	e.g. diagrams of geometrical proofs.
-	GUI vs. command line.
	-	Many tasks can become perception tasks requiring little inference.
-	Cockpit instruments have bugs, little physical markers.
	-	Allows pilots in cockpit to offload task of coordinating memory about key measurements.
-	Examples.
-	Form validation, do it in real time and show constraints and errors and actions.
-	Dialog boxes should be action oriented and guide users to likely next step. Provide them with necessary information.
-	Further reading:
	-	Don Norman, Things that Make Us Smart
	-	Ed Hutchings, Cognition in the Wild
	-	Herbert Simon, Sciences of the Artificial

### 6.1: Visual Design

-	**Whitespace conveys grouping.**
	-	"Some space must be narrow so that others may be wide. Some space must be empty so that other space may be filled."
-	**Use size contrasts to indicate hierarchy**.
	-	"Information consists of differences that make a difference."
-	**Vary scale and weight**.
-	Three goals for visual design
	-	*Guide*. Convey structure, relative importance, relationships.
	-	*Pace*: Draw people in, orient, provide hooks to dive deep.
	-	*Message*: express meaning and style, breath life into content.
-	Blur each page; is hierarchy still clear?
-	Three basic tools of visual design:
	-	*Typography*.
	-	*Layout*.
	-	*Color*.
-	A minute to learn, a lifetime to master.
-	Readings:
	-	Jennefier Tidwell's Designing Interfaces

### 6.2: Typography

-	Gill Sans, upper-case R.
	-	Perceptual uniformity, as opposed to actual uniformity. It looks uniform, but isn't.
-	Elements
	-	*Point size*.
		-	Implies maximum height, some fonts are less.
	-	*Leading*.
		-	Line spacing.
		-	Pronounced like periodic element; used to use lead to lay out lines.
		-	Default 20% of point size.
	-	*x-height*
		-	Higher => easier to read at lower point sizes and lower resolutions.
			-	e.g. Lucida Bright.
		-	Lower => elegance
			-	e.g. Baskerville.
	-	*ascenders* and *descenders*
		-	How far above x-height do letters extend.
		-	Typically low x-height => bigger ascenders and descenders.
	-	*weight*
		-	e.g. light, regular, bold.
	-	*serifs*
		-	Flourishes on ends of letters.
	-	*small caps*
		-	Sometimes useful for e.g. fitting in numbers with letters.
		-	By definition no ascenders and descenders.
-	"Which typeface should I use?"
	-	Hypothesis: serif more legible for body, sans-serif for headers. 
	-	No robust evidence for serif hypothesis.
	-	"Legibility is simply what you're used to" - Gill.
	-	Top-half of letters contain more information than bottom half.
	-	Expection plays an important role.
	-	For book, common font with large x-height.
	-	For logo, funkier font.
-	Experiment with different font faces for same text.
-	Look around you.
-	Readings:
	-	Robert Bringhurt, The Elements of Typographic Style
	-	Jennifer Tidwell, Designing Interfaces
	-	Edward Tufte, Envisioning Information
	-	Robin Williams, The Non-Designer's Design Book
	-	Gary Hustwit, Helvetica

### 6.3: Grids and Alignment

-	Bauhaus Revolution
	-	Revolution in use of sans-serif typefaces and grids in the 1920's.
-	**Grid**: set of invisible lines that elements snap 
to.
	-	Set of columns.
	-	Set of **gutters**: spacing between columns.
	-	Horizontally aligned using **baselines**.
	-	Add text hierarchically.
	-	Different elements can punch across columns, more dynamic.
	-	Focus plus context; make newer items at top bigger.
	-	Set of columns could be staggered.
	-	When creating templates, design for the longest text block.
-	**Alignment**
	-	In general, left-aligned text is faster to skim for languages read left-to-right.
	-	*Avoid slight misalignments*: undermine ability to beacon organization.
	-	*Deviate from patterns strategically* to draw attention.
	-	*Use visual proximity and scale* to convey semantic information.
-	*Right-alignment*
	-	For forms can right-align labels then left-align controls. Easier to determine what to fill out.
-	*Heading / subheading*
	-	Put subheading smaller and more gray scale to draw attention to heading.
-	**Color**
	-	Pay attention to it.
	-	Design in grayscale first.
	-	Keep luminance values from grayscale when moving to color. i.e. black -> gray -> white scale.
	-	All things equal, less colour is more effective.
-	Make space, guide the eye.
-	Readings:
	-	Kevin Mullet and Darrel Sano, Designing Visual Interfaces.
	-	Luke Wroblewski, Web Form Design.
	-	Jan Tschichold, The New Typography.

### 6.4: Reading and Navigating

-	Informavores! People forage and devour information.
-	Information scent
	-	Can people figure out how to get the information they want?
	-	Do they realize what options are available?
-	How do you detect poor scent?
	-	Flailing.
	-	Low confidence.
		-	Right track before and after clicking links.
		-	Before => high scent.
		-	After => information beacons intent.
	-	Back button.
-	Low scent pages exhibit e.g.:
	-	Surprising information architecture.
	-	Short links ("Transact"?)
	-	Hidden navigation, require mouseover to discover.
	-	Icons add little additional information.
-	When do icons help?
	-	Facilitate repeat recognition. Reminds you, aids recall.
	-	When you know what it looks like, but not what it's called. e.g. file extention => application.
	-	Redundant coding; recall based on either text or icon. Associative learning (icon => text, text => icon).
-	Improving scent:
	-	Lengthen links, multi-word.
	-	Specific, recognizable terms. Not clever terms.
	-	Also improves accessibility.
-	**Speaking block navigation**
	-	Add multiple different words, perhaps in subheadings, to navigation.

-	Location of information matters.
	-	The Poynter Institute, Eye Track study.
	-	Top-left is priority 1.
	-	Secondary ring around top-left, priority 2.
	-	Further is priority 3.

-	People happy to scroll if there is a scent that implies a reward.
	-	Put great content in the top fold.
	-	Put indications that content continues if you scroll.

-	People don't read. Nielsen, 1997.
-	Interlaced browsing. Multiple tasks at once.
-	Writing strategies, Nielsen 1997.
	-	Text more concise.
	-	Text more scannable using subheadings, bullet lists, paragraphs.
	-	Text more objective, less market-ese.
	-	All are effective.

-	Readings
	-	User Interface Engineering, Designing for the Scent of Information.
	-	Peter Pirolli, Information Foraging Theory.
	-	Jakob Nielsen, Alertbox.

### 7.1: Design Studies You Can Learn From

-   Wrong: "Do you like my interface?"
-   Wrong: "How much do you like my interface?"
-   Please the experimenter bias, particularly with cultures with power differentials.
-   Developers are still valuable testers.
-   Need *specific measures* and *concrete questions*.
-   **Baserates**: How often does Y occur? Measure Y.
    -   What fraction of people click on this link?
-   **Correlations**: Do X and Y co-vary? Measure X and Y.
    -   e.g. if people click on first search result, does that mean it's a good search result or that they just click on the first thing they see?
    -   Might need to randomise search results to determine causality.
-   **Causes**: Does X cause Y?
    -   Measure X and Y, and manipulate X.
    -   Somehow also account for effects of other confounding variables.

-   **Independent variables**: what we manipulate. Don't depend on users. 
-   **Dependent variables**: what user does. e.g. task completion time, accuracy, recall, emotional response.
-   **Internal validity**: how reliable? If you ran experiment again would you see the same results?
-   **External validity**: generalizability. Do your results matter?

-   Is my cool new approach better than the industry standard?
-   Example - the user study of phone users, half physical QWERTY half physical numeric, using iPhones. They suck at it.
    -   Manipulation: input style.
    -   Measure: words per minute.
    -   Benefit of study: absolutely want to measure if new technology is viable.
    -   Drawback of study: beginners will always be poor at new technology. What happens when they become experts?
    -   External validity: not so much.
    -   A better version: actual users.
    -   No surprise, users get much better and is about the same speed as physical QWERTY. However, iPhone users make more errors.

-   Strategies for fairer comparisons
    -   Insert your new approach into the production setting.
        -   Even if no access to live server could use client-side scripting or proxy servers.
    -   Recreate the production approach in your new setting.
    -   Scale things down so you're just looking at a piece of a larger system.
    -   When expertise is relevant, trains people up.

-   Is interface X better than interface Y?
    -   It depends.
    -   More importantly, what does it depend on? What is measured, settings and context.

### 7.2: Assign Participants to Conditions

-   Should every participant use every alternative?

-   e.g. which is a better vacuum cleaner, automatic or traditional?
    -   What are the measures?
        -   Faster?
        -   Cleaner?
        -   Fatigue?
        -   ...
    -   Manipulation: vacuum type.
    -   Measures: speed and type (focus on two).
    -   **Between subjects design**: assign half subjects to two different measures.
        -   But with small smaple sizes difficult to tell if results are real.
    -   **Within subjects design**: everyone uses all measures, both interfaces.
        -   But what order to try them in? e.g. after trying manual vacuum cleaner you're tired.
    -   **Counter-balancing**: within subjects design with randomly assigned order.
        -   Hopefully ordering effects balance out.
        -   Also try to vary location for first and second trials, possible confounding variable.

-   How about individual differences? Shirt colour, shape of face?
    -   If you think it has an impact then control for it.
    -   Random assignment is usually best.

-   What about three or more alternatives?
    -   **Latin square**. Randomly assign such that in each trial each option is chosen once, and for each participant they try all three options.

-   The importance of random assignment.
    -   e.g. typing in morning rather than afternoon. Which is faster?
    -   If people come in as they wish and morning is faster that could just be because they're morning people.
    -   Hence randomly assign to time blocks.

-   **Hawthorne effect**
    -   Manipulation: lighting levels in factory.
    -   Measures: productivity.
    -   Every manipulation increased productivity.
    -   Conclusion: someone intervening skewed results.
    -   Results are disputed.

-   **Counterbalanced assignment**
    -   Identify potential confounding variables and assign participants accordingly.
    -   e.g. if typing speed could affect interface usage, use pre-test to identify typing speed, then assign evenly across conditions.
-   **Offline counterbalancing**
    -   Pre-test to measure confounding variable, e.g. typing speed.
    -   Sort by variable.
    -   For each pair in list flip a coin and choose one of the pair. Heads goes to first condition, tails to second.
-   **Online counterbalancing**
    -   If you can't pre-test pick some threshold that's likely to be in the middle.
-   Pre-test-like counter-balancing trying to make law of large numbers happen faster

-   Danger: regression to the mean.
    -   With small sample sizes law of large numbers doesn't have a chance to kick in.
    -   Results just oscillating around mean.
    -   Pre-test counterbalancing addresses this danger.

-   Three major strategies.
    -   **Within-subjects**. Everyone tries all options. Good when not worried about learning / practice / exposure issues.
    -   **Between-participants**: each person tries one. Needs more people, more attention to fair assignment. Benefit: each participant is uncorrupted.
        -   Most common.
    -   Use **counterbalancing** to minimize variation in a between-subjects design.

-   Further reading
    -   David Martin, "Doing Psychology Experiments"

### 7.3: In person experiments

-   Can talk to people, see what they're doing and their confusion.
-   Higher bandwidth.

-   **Make clear goals**. e.g. for online booking system.
    -  **Scope**: what we're focusing on.
    -   **Purpose**: what you hope to learn.
    -   **Hypothesis**: what you want to test, should be testable.
    -   **Schedule and location**: choose appropriately. Quiet room? Train station?
    -   **Participants**: what type of people and how many?
    -   **Scenarios**: what you want users to try to accomplish. They must be able to care about this.
    -   **Questions to ask**.
    -   **Data to be collected**. e.g. task completion, error rate.
    -   **Experimenter roles**. Try to have at least two people, one facilitator and one note taker.

-   e.g. for online booking system:
    -   Scope: make meeting room booking system.
    -   Purpose: create system that encourage people to book right sized room for right duration.
    -   Hypothesis: splitting up booking process encourages more thought and works better.

-   **Create concrete tasks**. Write them down.
-   **Ethical considerations**.
    -   Make consent voluntary. Don't pressure people to participate.
    -   Remind people: you're testing the site, not them.

-   **Experimental details**.
    -   *Order of tasks*? Easy to hard? Random?
    -   *Training*? Ticket machine vs. surgical machine.
    -   *Do not finish*? Upper bounds for time, small stumbling blocks => nudge them.
    -   *Pilot study*: work out kinks in experiment itself.
        -   Recommend having two pilot studies. One with colleague to figure out material needed and tasks. A second with one real user.

-   **Capturing results**.
    -   At least a notebook or a computer.
    -   Maybe record video.
    -   Screen recording.

-   **Think-aloud method**.
    -   Need to know what users are thinking, not just doing.
    -   Ask users while they're performing tasks:
        -   What they are thinking
        -   What they are trying to do.
        -   What questions they have.
        -   What they read.
    -   Prompt users to keep talking. "Tell me what you are thinking". Avoid specific questions.
    -   Only help on things you've pre-decided to, write this down.
    -   Record with watch and notes.

-   Study steps.
-   Greeting participants - explain study, show setup.
-   Collecting data.
    -   Process data, observations, more qualitative.
    -   Bottom-line data.
        -   Summaries of time, did they succeed, number of errors. Quantitative.
        -   i.e. dependent variables.
    -   Define errors or what success means ahead of time.
    -   Do not combine thinking aloud with bottom-line data.
        -   Talking can affect speed and accuracy.
-   Debrief them at the end.
    -   Get their holistic last impressions.
    -   Tell them what they've contributed to. 

### 7.4a: Running Web Experiments (1)

-    A/B testing.
    -    A: old version, B: potential new version
    -    Generalisable to multiple tests, randomly assign.
    -    Collect metrics on specific goals; click throughs, sales, etc.
-    Web makes it easy to prototype and rapidly try new designs.
-    Why did multiple columns reduce sales by half for National Alert Registry?
    -    For sales maybe want one clear funnel of activity, not ambiguous options.
-    Small differences make big changes, e.g.
    -    Position and color of primary call to action.
    -    Position of testimonials.
    -    White space?
    -    What is linked?
    -    Number of columns? Number of different elements?
-    Metrics for email shots, number of:
    -    Opened, click throughs, forwards.
-    Obama sign up campaign.
    -    Button text: "sign up", "learn more", "sign up now", "join us now".
    -    Videos worse than images.
    -    Small changes, big differences.
    -    Our expectations are often wrong.

### 7.4b: Running web experiments (2)

-    Be clear what you want from users, e.g. not "I'm on Twitter" but "You should follow me on Twitter here".
-    *Even small changes have detectable differences.*
-    Beware confounding factors; weekend, daylight savings switch.
-    Randomness helps imply causality rather than correlation.
-    "Expedia on how one extra data field can cost $12m".
    -    Just added their company name.
-    Microsoft feedback.
    -    After stars rating revealing text box 50% better than statically showing text box.
    -    Fewer buttons in feedback form increases feedback by factor of 3.5.
-    **Commitment escalation**: get user to do a little bit, and then a little bit more. Better than asking for a lot up front.
    -    Subtle, requires fine tuning.
    -    Hence needs iterative design and experimentation.

### 7.4c: Running web experiments (3)

-    Principles for Effective Online Experiments.
-    Equal probability in each variant *in the long run*.
    -    Ramp up. Start experiment at 0.1%. Helps prevent disasters.
    -    Automatically abort bad experiments.
-    Don't just run experiments on dependent variables that are easy to measure. Run experiments on variables that matter.
-    Run changes for long enough to allow them to become familiar with it.
-    Rules for random assignment:
    -    **Consistent**. Same person sees same variant on each login.
        -    Else users will perceive interface to be random.
    -    **Durable**. 
    -    **Independent**. Make sure assignment is truly random, not dependent on e.g. day of the week.
-    Even with data theorising why the changes matter is still hard.
-    Use multiple methods to help build theories, e.g. web experiements, in-person studies.
-    Design in the online age
    -    Rather than one great design, now many alternatives.
    -    Rapid experimentation, helps quickly test assumptions.

### 6.5: Comparing rates

-    Three questions
    1.    **What does my data look like?**
        -    Visualisations, summaries, plot.
    2.    **What are the overall numbers?*8
        -    Aggregate statistics, mean and standard deviations.
    3.    **Are the differences "real"?**
        -    Significace, p values.
        -    Likelihood results are due to chance.
-    **Test statistic**, how unexpected is result.
-    **Pearson's Chi-Squared Test**.
    -    Compare observed to expected, taking into account number of trials.
    
            chi^2 = (observed - expected) ^ 2 / expected
    
    -    "Normal" outcome variance, Gaussian.
        -    p = 0.05 means observation in the tails.
    -    **The Null Hypothesis**: opening assumption is "we don't think there is any difference between observation and expection".
    -    Critical Values for Chi-Squared. p value vs. chi^2 value. Bigger chi^2 => smaller p. 
-    **Degrees of freedom**: (number of choices - 1).
-    Statistical testing:
    -    Formalizes "we're pretty sure".
    -    Helps generalize (or not) from small samples.
-    Guiness beer.
    -    Student's t-test created for testing quality of stout. 
    -    If testing consumed all beer then not cost-effective!
    -    Also, statisticians find math more difficult after drinking a lot of beer.
-    For continuous data:
    -    T-tests (compare 2 conditions)
    -    ANOVA (compare >2 conditions).
    -    Both for normally distributed data.
-    Data often isn't "normal"
    -    Bi-modal, two maxima.
    -    Skewed, e.g. response rates as time.
-    Handling non-normal data:
    -    Knowing is half the battle.
    -    A/A test. Split users in half and expose to same variant. Any significance?
    -    Use randomized testing. Repeated simulations to model data. Not covered.
-    *Graph all the data!*
-    *Use statistical testing.*
-    Further reading
    -    Practical Statistics for HCI, Jacob Wobbrock, http://depts.washington.edu/aimgroup/proj/ps4hci
    -    Doing Psychology Experiments, David W. Martin
    -    Statistics as Principles Argument, Robert P. Abelson
    -    Learnign to use statistical tests in psychology, Judith Greene, Manuela D'Oliveira.

#### Readings:

-	3.1 and assignment 2, Prototyping

-	10 Heuristics for User Interface Design
	1.	**Visibility of system status**.
		-	Users informed of what is going in.
		-	Appropriate and prompt.
	2.	**Match between system and real world**.
		-	Speak user's language.
		-	Follow real-world conventions.
		-	Natural and logical.
	3.	**User control and freedom**.
		-	Mistakes happen, need undo and redo.
	4.	**Consistency and standards**.
		-	Follow conventions.
	5.	**Error prevention**.
		-	Eliminate error-prone conditions or check for them.
		-	Confirmation of action commits.
	6.	**Recognition rather than recall**.
		-	Instructions and options should be visible and easily retrievable.
	7.	**Flexibility and efficiency of use**.
		-	Experts need accelerators, novices can ignore them.
		-	Allow users to tailor frequent actions.
	8.	**Aesthetic and minimalist design**.
		-	Remove irrelevant or rarely needed information.
	9.	**Help users recognize, diagnose, and recover from errors**.
		-	Plain language.
		-	Explain problem and suggest solution.
	10.	**Help and documentation**.
		-	Better that system is usable without docs.
		-	But docs should exist, easy to search, focused on user's task, concrete.


	

## Assignment notes

## General notes
