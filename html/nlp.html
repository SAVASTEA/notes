<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="_pandoc.css" type="text/css" />
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <link href='http://fonts.googleapis.com/css?family=Noticia+Text:400,400italic,700,700italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic,700italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Inconsolata:400,700&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
  
</head>
<body>
<div id="TOC">
<ul>
<li><a href="#natural-language-processing">Natural Language Processing</a><ul>
<li><a href="#readings-policy">Readings policy</a></li>
<li><a href="#rendering">Rendering</a></li>
<li><a href="#week-1---introduction-to-natural-language-processing">Week 1 - Introduction to Natural Language Processing</a><ul>
<li><a href="#introduction-part-1">Introduction (Part 1)</a></li>
<li><a href="#introduction-part-2">Introduction (Part 2)</a></li>
</ul></li>
<li><a href="#week-1---the-language-modeling-problem">Week 1 - The Language Modeling Problem</a><ul>
<li><a href="#introduction-to-the-language-modeling-problem-part-1">Introduction to the Language Modeling Problem (Part 1)</a></li>
<li><a href="#introduction-to-the-language-modeling-problem-part-2">Introduction to the Language Modeling Problem (Part 2)</a></li>
<li><a href="#markov-processes-part-1">Markov Processes (Part 1)</a></li>
<li><a href="#markov-processes-part-2"> Markov Processes (Part 2)</a></li>
<li><a href="#trigram-language-models">Trigram Language Models</a></li>
<li><a href="#evaluating-language-models-perplexity">Evaluating Language Models: Perplexity</a></li>
</ul></li>
<li><a href="#week-1---parameter-estimation-in-language-models">Week 1 - Parameter Estimation in Language Models</a><ul>
<li><a href="#linear-interpolation-part-1">Linear Interpolation (Part 1)</a></li>
<li><a href="#linear-interpolation-part-2"> Linear Interpolation (Part 2)</a></li>
<li><a href="#discounting-methods-part-1">Discounting Methods (Part 1)</a></li>
<li><a href="#discounting-methods-part-2">Discounting Methods (Part 2)</a></li>
<li><a href="#summary"> Summary</a></li>
</ul></li>
<li><a href="#week-2---tagging-problems-and-hidden-markov-models"> Week 2 - Tagging Problems and Hidden Markov Models</a><ul>
<li><a href="#the-tagging-problem">The Tagging Problem</a></li>
<li><a href="#generative-models-for-supervised-learning">Generative Models for Supervised Learning</a></li>
<li><a href="#hidden-markov-models">Hidden Markov Models</a></li>
<li><a href="#parameter-estimation-in-hmms">Parameter Estimation in HMMs</a></li>
<li><a href="#the-viterbi-algorithm-for-hmms">The Viterbi Algorithm for HMMs</a></li>
<li><a href="#the-viterbi-algorithm">The Viterbi Algorithm</a></li>
<li><a href="#the-viterbi-algorithm-1">The Viterbi Algorithm</a></li>
<li><a href="#summary-1">Summary</a></li>
</ul></li>
<li><a href="#week-3---parsing-and-context-free-grammars">Week 3 - Parsing, and Context-Free Grammars</a><ul>
<li><a href="#introduction"> Introduction</a></li>
</ul></li>
<li><a href="#readings">Readings</a><ul>
<li><a href="#speech-and-language-processing-chapter-3-words-and-transducers">Speech and Language Processing, Chapter 3 (Words and Transducers)</a></li>
<li><a href="#speech-and-language-processing-chapter-4-n-gram-models">Speech and Language Processing, Chapter 4 (n-gram models)</a></li>
<li><a href="#arpa-language-model-lm-file-format">ARPA language model (LM) file format</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1 id="natural-language-processing"><a href="#natural-language-processing">Natural Language Processing</a></h1>
<p>Columbia University, via Coursera</p>
<p><a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br /><span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Natural Language Processing notes</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="http://www.asimihsan.com" property="cc:attributionName" rel="cc:attributionURL">Asim Ihsan</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.</p>
<iframe width="292" height="420" src="http://tools.flattr.net/widgets/thing.html?thing=1150095"></iframe>

<h2 id="readings-policy"><a href="#readings-policy">Readings policy</a></h2>
<p>There are excellent readings assigned to the class. They’re explicitly inlined into the respective lecture, to save typing stuff out twice.</p>
<p>Other readings (papers, textbooks, other courses) are explicitly inlined as well.</p>
<h2 id="rendering"><a href="#rendering">Rendering</a></h2>
<p>In order to use pandoc run (need to include custom LaTeX packages for some symbols):</p>
<pre><code>    pandoc \[course\]\ natural\ language\ processing.md
    -o pdf/nlp.pdf --include-in-header=latex.template</code></pre>
<p>or, for Markdown + LaTex to HTML + MathJax output:</p>
<pre><code>    pandoc \[course\]\ natural\ language\ processing.md
    -o html/nlp.html
    --include-in-header=html/_header.html
    --mathjax -s --toc --smart -c _pandoc.css</code></pre>
<p>and, for the ultimate experience, after <code>pip install watchdog</code>:</p>
<pre><code>    watchmedo shell-command --patterns=&quot;*.md&quot;
    --ignore-directories --recursive
    --command=&#39;&lt;command above&gt;&#39; .</code></pre>
<h2 id="week-1---introduction-to-natural-language-processing"><a href="#week-1---introduction-to-natural-language-processing">Week 1 - Introduction to Natural Language Processing</a></h2>
<h3 id="introduction-part-1"><a href="#introduction-part-1">Introduction (Part 1)</a></h3>
<ul>
<li>What is NLP?
<ul>
<li>Computers using natural language as input and/or output.</li>
<li>NLU: understanding, input</li>
<li>NLG: generation, output.</li>
</ul></li>
</ul>
<p>Tasks</p>
<ul>
<li>Oldest task: <strong>machine translation</strong>. Convert between two languages.</li>
<li><strong>Information extraction</strong>
<ul>
<li>Text as input, structure of key content as output.</li>
<li>e.g. job posting into industry, position, location, company, salary.</li>
<li>Complex searches (“jobs in Boston paying XXX”).</li>
<li>Statistical queries (“how has jobs changed in IT changed over time?”)</li>
</ul></li>
<li><strong>Text summarization</strong>
<ul>
<li>Condense one or many documents into a summary.</li>
<li><a href="http://newsblaster.cs.columbia.edu/"><em>Columbia Newsblaster</em></a> is an example.</li>
</ul></li>
<li><strong>Dialogue systems</strong>
<ul>
<li>Humans can interact with a computer to ask questions and achieve tasks.</li>
</ul></li>
</ul>
<p>Basic NLP problems</p>
<ul>
<li><strong>Tagging</strong>
<ul>
<li>Map strings to tagged sequences (each word is lexed and tagged with an appropriate label).</li>
<li><strong>Part-of-speech tagging</strong>: noun, verb, preposition, …
<ul>
<li>Profits (N) soared (V) at (P) Boeing (N)</li>
</ul></li>
<li><strong>Named Entity Recognition</strong>: companies, locations, people
<ul>
<li>Profits (NA) soared (NA) at (NA) Boeing (C)</li>
</ul></li>
</ul></li>
<li><strong>Parsing</strong>
<ul>
<li>e.g. “Boeing is located in Seattle” into a parse tree.</li>
</ul></li>
</ul>
<h3 id="introduction-part-2"><a href="#introduction-part-2">Introduction (Part 2)</a></h3>
<p>Why is NLP hard?</p>
<ul>
<li><strong>Ambiguity</strong>
<ul>
<li>“At last, a computer that understands you like your mother”; three intrepretations at the <em>syntactic</em> level.</li>
<li>But also occurs at an <em>acoustic</em> level: “like your” sounds like “lie cured”.
<ul>
<li>One is <em>more likely</em> than the other, but without this information difficult to tell.</li>
</ul></li>
<li>At <em>semantic</em> level, words often have more than one meaning. Need context to disambiguate.
<ul>
<li>“I saw her duck with a telescope”.</li>
</ul></li>
<li>At <em>discourse</em> (multi-clause) level.
<ul>
<li>“Alice says they’ve built a computer that understands you like your mother”</li>
<li>If you start a sentence saying “but she…”, who is she referring to?</li>
</ul></li>
</ul></li>
</ul>
<p>What will this course be about</p>
<ul>
<li><strong>NLP subproblems</strong>: tagging, parsing, disambiguation.</li>
<li><strong>Machine learning techniques</strong>: probabilistic CFGs, HMMs, EM algorithm, log-linear models.</li>
<li><strong>Applications</strong>: information extraction, machine translation, natural language interfaces.</li>
</ul>
<h4 id="syllabus"><a href="#syllabus">Syllabus</a></h4>
<ul>
<li>Language modelling, smoothed estimation</li>
<li>Tagging, hidden Markov models</li>
<li>Statistical parsing</li>
<li>Machine translation</li>
<li>Log-linear models, discriminative methods</li>
<li>Semi-supervised and unsupervised learning for NLP</li>
</ul>
<h2 id="week-1---the-language-modeling-problem"><a href="#week-1---the-language-modeling-problem">Week 1 - The Language Modeling Problem</a></h2>
<h3 id="introduction-to-the-language-modeling-problem-part-1"><a href="#introduction-to-the-language-modeling-problem-part-1">Introduction to the Language Modeling Problem (Part 1)</a></h3>
<ul>
<li>We have some finite vocabulary, i.e.</li>
</ul>
<p><span class="math">\[V = \{the, a, man telescope, Beckham, two, ...\}\]</span></p>
<ul>
<li>We have countably infinite set of strings, which are the set of possible sentences in the language:</li>
</ul>
<p><span class="math">\[V^+ = \{&quot;the\:STOP&quot;, &quot;a\:STOP&quot;, &quot;the\:fan\:STOP&quot;, ...\}\]</span></p>
<ul>
<li>STOP is a stop symbol at the end of a sentence. Convenient later on.</li>
<li>Sentences don’t have to make sense, just every sequence of words.</li>
<li><p>Also a sentence could just be {“STOP”}, empty.</p></li>
<li>We have a <em>training sample</em> of example sentences in English.
<ul>
<li>Sentences from the New York Times in the last 10 years.</li>
<li>Sentences from a large set of web pages.</li>
<li>In the 1990’s 20 million words common, by the end of the 90’s 1 billion words common.</li>
<li>Nowadays 100’s of billions of words.</li>
</ul></li>
<li><p>With this training sample we want to “learn” a probabiliy distribution p, i.e. p is a function that satisfies:</p></li>
</ul>
<p><span class="math">\[\sum_{x \in V^+} p(x) = 1, \quad p(x) \ge 0 \; \forall \; x \in V^+\]</span></p>
<ul>
<li>For any sentence x in language, p(x) &gt;= 0.</li>
<li>If we sum over all sentences x in language, p(x) sums to 1.</li>
<li>A good language model assigns high probabilities to likely sentences in English (the fan saw Beckham STOP), low probabilities to unlikely sentences in English (Beckham fan saw the STOP)</li>
</ul>
<h3 id="introduction-to-the-language-modeling-problem-part-2"><a href="#introduction-to-the-language-modeling-problem-part-2">Introduction to the Language Modeling Problem (Part 2)</a></h3>
<ul>
<li>But…why do we want to do this?!
<ul>
<li><strong>Speech recognition</strong> was original motivation; related problems are optical character recognition and handwriting recognition.</li>
<li>Input: sound wave time series.</li>
<li>Preprocess: split into relatively short time periods, e.g. 10ms.</li>
<li>For each frame do a Fourier transform, get energies of frequencies.</li>
<li>Problem is to output recognised speech, sequence of words.</li>
<li>Main course notes: it’s useful to have prior probabilities so that if we can choose between alternatives we can ask “which is most likely?”.
<ul>
<li>“recognise speech” vs “wreck a nice beach”</li>
</ul></li>
<li>The estimation techniques developed for this problem will be very useful for other problems in NLP.</li>
</ul></li>
<li>Naive method of language modelling
<ul>
<li>We have N training sentences.</li>
<li>For any sentence <span class="math">\(x_1, ..., x_n\)</span>, define <span class="math">\(c(x_1, ..., x_n)\)</span> as the number of times the sentences is seen in our training data.</li>
<li>Naive estimate:</li>
</ul></li>
</ul>
<p><span class="math">\[p(x_1, \ldots, x_n) = \frac{c(x_1, \ldots, x_n)}{N}\]</span></p>
<ul>
<li>This is a valid, well-formed language model (p(x) sums to 1, they’re all &gt;= 0).</li>
<li>However, they’ll assign a probabiliy of 0 to any unseen sentences; no ability to generalise to new sentences.</li>
<li>How can we build language models that generalise beyond the test sentences?</li>
</ul>
<h3 id="markov-processes-part-1"><a href="#markov-processes-part-1">Markov Processes (Part 1)</a></h3>
<ul>
<li>Markov Processes
<ul>
<li>Consider a sequence of random variables <span class="math">\(X_1, X_2, \ldots, X_n\)</span>.</li>
<li>Each random variable can take any value in a finite set V.</li>
<li>For now assume n is fixed, e.g. = 100. Every sequence is the same length.</li>
<li>Our goal: model the joint probability distribution of the values of these n variables:</li>
</ul></li>
</ul>
<p><span class="math">\[P(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n)\]</span></p>
<ul>
<li><p>This is huge: for vocabulary V, number of sequences of length n is <span class="math">\(|V|^n\)</span>.</p></li>
<li>First-Order Markov Processes</li>
<li>Going to use the chain rule of probabilities to decompose the expression into a product of expressions.</li>
<li><p>For two expressions, this rule is:</p></li>
</ul>
<p><span class="math">\[P(A,B) = P(A) \times P(B|A)\]</span> <span class="math">\[P(A,B,C) = P(A) \times P(B|A) \times P(C|A,B)\]</span></p>
<ul>
<li>Hence:</li>
</ul>
<p><span class="math">\[P(X_1 = x_1, X_2 = x_2) = P(X_1 = x_1) \times P(X_2 = x_2 | X_1 = x_1)\]</span> <span class="math">\[P(X_1 = x_1, X_2 = x_2, X_3 = x_3) = ... P(X_3 = x_3 | P(X_2 = x_2, X_1 = x_1)\]</span></p>
<ul>
<li>This kind of decomposition is <em>exact</em>: this is always true, and no assumptions are involved.</li>
<li>Hence the general decomposition:</li>
</ul>
<p><span class="math">\[P(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n)\]</span> <span class="math">\[=P(X_1 = x_1) \prod_{i=2}^{n} P(X_i = x_i\;|\;X_1 = x_1, \dots, X_{i-1} = x_{i-1})\]</span></p>
<ul>
<li>Continuing on, with first-order Markov assumption:</li>
</ul>
<p><span class="math">\[= P(X_1 = x_1) \prod_{i=2}^{n} P(X_i = x_i\;|\; X_{i-1} = x_{i-1})\]</span></p>
<ul>
<li>The first-order Markov assumption: for any <span class="math">\(i \in \{2, \dots, n\}\)</span>, for any <span class="math">\(x_1, \dots, x_n\)</span>:</li>
</ul>
<p><span class="math">\[P(X_i=x_i|X_1=x_1, \ldots, X_{i-1} = x_{i-1}) = P(X_i=x_i | X_{i-1} = x_{i-1})\]</span></p>
<ul>
<li>Random variable at position i depends on just the previous value, on the variable at position (i-1).
<ul>
<li><span class="math">\(X_i\)</span> is conditionally independent of all the other random variables once you condition on <span class="math">\(X_{i-1}\)</span>.</li>
</ul></li>
</ul>
<h3 id="markov-processes-part-2"><a href="#markov-processes-part-2"> Markov Processes (Part 2)</a></h3>
<ul>
<li>What about Second-Order Markov Processes?</li>
<li>Again, the problem is to model the joint distribution over <span class="math">\(n\)</span> random variables:</li>
</ul>
<p><span class="math">\[P(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n)\]</span> <span class="math">\[=P(X_1 = x_1) P(X_2 = x_2 | X_1 = x_1) \prod_{i=3}^{n} P(X_i = x_i | X_{i-2} = x_{i-2}, X_{i-1} = x_{i-1})\]</span></p>
<ul>
<li>For elements further along in the sequence the value for the i’th random variable depends on the previous <em>two</em> random variables.</li>
<li>This is a bit awkward, so for convenience we assume <span class="math">\(x_0 = x_{-1} = *\)</span>, where <span class="math">\(*\)</span> is a special “start” symbol.</li>
</ul>
<p><span class="math">\[= \prod_{i=1}^{n} P(X_i = x_i | X_{i-2} = x_{i-2}, X_{i-1} = x_{i-1})\]</span></p>
<ul>
<li>For example, <span class="math">\(x_{-1} = *,\;x_0 = *,\;x_1 = the,\;\ldots\)</span>,</li>
</ul>
<h4 id="modelling-variable-length-sequences"><a href="#modelling-variable-length-sequences">Modelling Variable Length Sequences</a></h4>
<ul>
<li>Want <span class="math">\(n\)</span> to also be a random variable.</li>
<li>Simple solution: always define <span class="math">\(X_n = STOP\)</span>, where <span class="math">\(STOP\)</span> is a special symbol.</li>
<li>Use a Markov process as before, but assume <span class="math">\(X_n = STOP\)</span>.</li>
</ul>
<h3 id="trigram-language-models"><a href="#trigram-language-models">Trigram Language Models</a></h3>
<ul>
<li>A trigram language model consists of:
<ol style="list-style-type: decimal">
<li>A finite set <span class="math">\(V\)</span> (the words, the vocabulary).</li>
<li>A parameter <span class="math">\(q(w|u,v)\)</span> for each trigram <span class="math">\(u,v,w\)</span> such that <span class="math">\(w \in V \bigcup \{STOP\}\)</span>, and <span class="math">\(u,v \in V \bigcup \{*\}\)</span>.
<ul>
<li>For each <em>trigram</em> <span class="math">\(u,v,w\)</span>, a sequence of three words, we have a parameter <span class="math">\(q(w|u,v)\)</span>.</li>
<li><span class="math">\(w\)</span> could be any element of V or STOP, and</li>
<li><span class="math">\(u,v\)</span> could be any element of V or START.</li>
</ul></li>
</ol></li>
<li>For any sentence <span class="math">\(x_1, \ldots, x_n\)</span> where <span class="math">\(x_i \in V\)</span> for <span class="math">\(i = 1 \ldots (n-1)\)</span>, and <span class="math">\(x_n = STOP\)</span>, the probability of the sentence under the trigram model is:</li>
</ul>
<p><span class="math">\[p(x_1, \dots, x_n) = \prod_{i=1}^{n}q(x_i\;|\;x_{i-2},x_{i-1})\]</span></p>
<ul>
<li>where we define <span class="math">\(x_0 = x_{-1} = *\)</span>.</li>
<li>i.e. for any sentence the probability of it is the product of second-order Markov probabilities of its constituent trigrams.</li>
</ul>
<p>An example. For the sentence</p>
<pre><code>    the dog barks STOP</code></pre>
<p>we could have</p>
<p><span class="math">\(p(\textrm{the dog barks STOP}) =\)</span><br /><span class="math">\(q(\textrm{the | *, *})\)</span><br /><span class="math">\(\times q(\textrm{dog | *, the})\)</span><br /><span class="math">\(\times q(\textrm{barks | the, dog})\)</span><br /><span class="math">\(\times q(\textrm{STOP | dog, barks})\)</span></p>
<ul>
<li>This is still a naive language model. It’s easy to find problems.</li>
<li>PCFGs, explored later, are much superior.</li>
<li>Having said that, trigram language models are extremely useful.
<ul>
<li>They are very hard to improve upon.</li>
<li>Considerable simplicity.</li>
</ul></li>
</ul>
<hr />
<ul>
<li>Quiz: say we have a language model with <span class="math">\(V = \{\textrm{the, dog, runs}\}\)</span>, and the following parameters:</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        q(\textrm{the | *, *}) &amp; = 1 \\
        q(\textrm{dog | *, the}) &amp; = 0.5 \\  
        q(\textrm{STOP | *, the}) &amp; = 0.5 \\
        q(\textrm{runs | the, dog}) &amp; = 0.5 \\ 
        q(\textrm{STOP | the, dog}) &amp; = 0.5 \\
        q(\textrm{STOP | dog, runs}) &amp; = 1
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>There are <strong>three</strong> sentences with non-zero probability under this model.
<ul>
<li>{*, *, the, STOP}</li>
<li>{*, *, the, dog, STOP}</li>
<li>{*, *, the, dog, runs, STOP}</li>
</ul></li>
<li>Draw out a graph, where nodes are words and edge labels denote probabilities, to see this:</li>
</ul>
<div class="figure">
<img src="nlp_01_trigram_graph.png" />
</div>
<ul>
<li>The above is a <a href="http://en.wikipedia.org/wiki/Markov_chain">Markov chain</a> drawn in the style of a <a href="http://en.wikipedia.org/wiki/Finite-state_machine">finite-state machine</a>. This is a lot of fancy talk for a very simple idea.</li>
<li>People call these sorts of diagrams all sorts of fancy names.
<ul>
<li>Mathematicians call this type of diagram a <strong>graph</strong>. (No, not a pie <em>chart</em>).
<ul>
<li>Since there are arrows in this diagram this is a <strong>directed graph</strong>.</li>
<li>If the digram just had plain lines, without arrow heads, it would be an <strong>undirected graph</strong>.</li>
</ul></li>
<li>Computer scientists call this type of diagram a <strong>finite-state machine</strong> (FSM).</li>
<li>Electrical engineers call diagrams that (kind of) look like this <strong>Markov chains</strong>.</li>
<li>In graph-speak, each circle is a <strong>node</strong> and each arrow is an <strong>edge</strong>.</li>
<li>In FSM-speak, each circle is a <strong>state</strong> and each arrow is a <strong>transition</strong>.</li>
<li>People will often use node/state and edge/transition interchangably, and others love pointing out that they’re using the wrong words.</li>
</ul></li>
<li>Every circle and double circle represents a <em>state</em>. A state is “somewhere you can be”, which’ll make sense soon.</li>
<li>Every line is the <em>likelihood of moving from one state to another</em>. If this number is 0 this transition is impossible, if it is 1 it is certain, and could be something in between.
<ul>
<li>This number can never be less than 0 or greater than 1. If it is then the probability distribution is no longer “well-formed”, i.e. doesn’t make sense.</li>
</ul></li>
<li>On the left-hand side an arrow appears to come out of nowhere to the first start symbol (*). This is the <em>start state</em>.</li>
<li>Every double circle is an <em>accepting state</em>. <strong>The only accepting state is the STOP symbol</strong>.</li>
<li>Take your finger and trace out every path from every start state to every accepting state. This is a possible sequence of words in this language model.
<ul>
<li>As the STOP symbol is the only accepting state, <strong>every sequence of words in this language model must end with the STOP symbol</strong>.</li>
</ul></li>
<li>The probability of a given sequence of words is the <strong>product</strong> of all the edge probabilities.</li>
<li>The sum of probabilities of all possible sequences of words must add up to one if the language model is “well-formed”.
<ul>
<li>One of the rules of a probability distributions is that all the “little probabilities”, or probabilities of individual events, must sum to 1 or else it just doesn’t make sense.</li>
</ul></li>
<li>You’ll notice something peculiar. If any edge has a value of 0 then all sequences of words that include that edge must be impossible, no matter how likely the other transitions in that sequence!
<ul>
<li>Hence when you’re tracing your finger looking for possible sequences you <strong>exclude any paths that include an edge with a probability of 0</strong>.</li>
</ul></li>
<li>You may be wondering how hard this gets with many nodes and many edges. Very hard. This is just a visual depiction of how utterly impossible brute force is when it comes to language models, and hence how important the upcoming Vitterbi algorithm is.</li>
</ul>
<hr />
<h4 id="the-trigram-estimation-problem"><a href="#the-trigram-estimation-problem">The Trigram Estimation Problem</a></h4>
<ul>
<li>But what are the values of parameters q?</li>
<li>This turns out to be a challenging problem.</li>
<li>A natural estimate: the <strong>maximum likelihood estimate (ML)</strong>.</li>
<li>Recall that we assume that we have a training set, some example sentences in our language, typically, as you recall, millions or billions of sentences.</li>
<li>From these sentences we can derive counts; how often do trigrams occur?</li>
</ul>
<p><span class="math">\[q(w_i\;|\;w_{i-2},w_{i-1}) = \frac{\textrm{Count}(w_{i-2},w_{i-1},w_{i})}{\textrm{Count}(w_{i-2},w_{i-1})}\]</span></p>
<ul>
<li>For example:</li>
</ul>
<p><span class="math">\[q(\textrm{laughs | the, dog}) = \frac{\textrm{Count(the, dog, laughs)}}{\textrm{Count(the, dog)}}\]</span></p>
<ul>
<li>This is intuitive. For instances of a particular bigram how often are they followed by the particular third word of our trigram?</li>
</ul>
<hr />
<ul>
<li>Quiz: consider the following corpus of sentences:
<ul>
<li>the dog walks STOP</li>
<li>walks the dog STOP</li>
<li>dog walks fast STOP</li>
</ul></li>
<li>Let <span class="math">\(q_{ML}\)</span> by the maximum-likelihood parameters of a trigram langauge model trained on this corpus. Which of the following parameters have a value that is both well-defined and non-zero?</li>
</ul>
<p>Correct:</p>
<p><span class="math">\(q_{ML}({\textrm{walks | *, dog}})\)</span><br /><span class="math">\(q_{ML}({\textrm{dog | walks, the}})\)</span><br /><span class="math">\(q_{ML}({\textrm{walks | the, dog}})\)</span></p>
<p>Incorrect:</p>
<p><span class="math">\(q_{ML}({\textrm{walks | dog, the}})\)</span><br /><span class="math">\(q_{ML}({\textrm{fast | dog, the}})\)</span><br /><span class="math">\(q_{ML}({\textrm{STOP | walks, dog}})\)</span></p>
<hr />
<ul>
<li>ML is a useful starting point, but has serious problems.</li>
</ul>
<p>Spare Data problems</p>
<ul>
<li>Say our vocabulary size is <span class="math">\(N = |V|\)</span>, then there are <span class="math">\(N^3\)</span> parameters in our model.</li>
<li>e.g. <span class="math">\(N = 20,000\;\implies\;20,000^3 = 8 \times 10^{12}\)</span> parameters.</li>
<li>Most parameters will be zero; most possible trigrams will not appear.</li>
<li>But does that mean all trigrams we haven’t seen are necessarily impossible to <em>ever</em> see? No.</li>
<li>Worse still, the bigram denominator may be zero, and the ML ratio is undefined.</li>
</ul>
<h3 id="evaluating-language-models-perplexity"><a href="#evaluating-language-models-perplexity">Evaluating Language Models: Perplexity</a></h3>
<ul>
<li>We have some test data, <span class="math">\(m\)</span> sentences, i.e. <span class="math">\(s_1, s_2, s_3, \ldots, s_m\)</span>. Each of these is a sentence in the language, e.g. {the dog laughs STOP}.</li>
<li>Additionally, assume that use some <em>development</em> data to determine the language model parameters, but hold out some additional <em>test data</em> to evaluate the language model.</li>
<li>Natural to look at the probability that our language model gives to sentences in the test data <span class="math">\(\prod_{i=1}^{m}p(s_i)\)</span>; it’s never seen it before.</li>
</ul>
<p><span class="math">\[\textrm{log}\;\prod_{i=1}^{m} p(s_i) = \sum_{i=1}^{m} \textrm{log}\;p(s_i)\]</span></p>
<ul>
<li>(the above is a basic rule of logarithms; log of product = sum of logs).</li>
<li>recall that e.g.:</li>
</ul>
<p><span class="math">\[p(s_i) = q(\textrm{the | *, *}) \times q(\textrm{dog | *, the}) \times \ldots\]</span></p>
<ul>
<li>Naturally we’d expect better languages models to assign higher probabilities to sentences in the test data.</li>
<li><p>And log is a monotonically increasing function, so expect the sum of logs to correspondingly be higher for better language models.</p></li>
<li><p>In fact, the usual evaluation measure is <strong>perplexity</strong>:</p></li>
</ul>
<p><span class="math">\[\textrm{Perplexity} = 2^{-l},\;\textrm{where}\]</span> <span class="math">\[l = \frac{1}{M} \sum_{i=1}^{m} \textrm{log}\;p(s_i)\]</span></p>
<ul>
<li>and M is the total number of <em>words</em> in the test data. In some sense with (1/M) the perplexity is now stable with respect to the size of the test data.</li>
<li>The <em>lower</em> the perplexity the <em>better the fit</em> of the language model to the test data.</li>
</ul>
<p>Some Intuition about Perplexity</p>
<ul>
<li>Say we have vocabulary <span class="math">\(V\)</span>, and <span class="math">\(N = |V| + 1\)</span>, and the dumbest possible model predicts:</li>
</ul>
<p><span class="math">\[q(w|u,v) = \frac{1}{N},\;\forall\;w \in V \cup \{\textrm{STOP}\},\;\forall\;u,v \in V \cup \{\textrm{*}\}\]</span>.</p>
<ul>
<li>This dumbest model assigns the uniform distribution over all possible words in each possible. Ignores previous words, doesn’t measure relative frequency.</li>
<li>Easy to calculate perplexity:</li>
</ul>
<p><span class="math">\[\textrm{Perplexity} = 2^{-l},\;\textrm{where}\;l=\textrm{log}\;\frac{1}{N}\]</span> <span class="math">\[\implies\; \textrm{Perplexity} = N\]</span></p>
<ul>
<li>!!AI implying all these calculations use log base 2.</li>
<li>Perplexity is a measure of effective “branching factor”.
<ul>
<li>The model is as confused on test data as if it had to choose uniformly and independently among P possibilities per word, where P is the perplexity. Source: <a href="http://en.wikipedia.org/wiki/Perplexity">Wikipedia:Perplexity</a>.</li>
</ul></li>
</ul>
<hr />
<p>Quiz: define a trigram language model with the following parameters:</p>
<ul>
<li>q(the | *, *) = 1</li>
<li>q(dog | *, the) = 0.5</li>
<li>q(cat | *, the) = 0.5</li>
<li>q(walks | the, cat) = 1</li>
<li>q(STOP | cat, walks) = 1</li>
<li>q(runs | the, dog) = 1</li>
<li>q(STOP | dog, runs) = 1</li>
</ul>
<p>Now consider a test corpus with the following sentences:</p>
<ul>
<li>the dog runs STOP</li>
<li>the cat walks STOP</li>
<li>the dog runs STOP</li>
</ul>
<p>Note that the number of words in this corpus, M, is 12.</p>
<p>What is the perplexity of the language model, to 3dp?</p>
<p><span class="math">\[P = 2^{-l}\]</span> <span class="math">\[l = \frac{1}{M} \sum \textrm{log}_2\{p(s_i)\}\]</span></p>
<p><span class="math">\(p(\textrm{the dog runs STOP}) = q(\textrm{the | *, *}) \times q(\textrm{dog | *, the}) \times q(\textrm{runs | the, dog}) \times q(\textrm{STOP | dog, runs})\)</span><br /><span class="math">\(= 1 \times 0.5 \times 1 \times 1 = 0.5\)</span></p>
<p><span class="math">\(p(\textrm{the cat walks STOP}) = q(\textrm{the | *, *}) \times q(\textrm{cat | *, the}) \times q(\textrm{walks | the, cat}) \times q(\textrm{STOP | cat walks})\)</span><br /><span class="math">\(= 1 \times 0.5 \times 1 \times 1 = 0.5\)</span></p>
<p><span class="math">\(l = \frac{1}{12} \{ 3 \times \textrm{log}_2(0.5) \}\)</span> <span class="math">\(=\frac{1}{12}(-3) = \frac{-1}{4}\)</span><br /><span class="math">\(p=2^{\frac{1}{4}} = \sqrt[4]{2} = 1.189\;\textrm{(3dp)}\)</span></p>
<hr />
<h4 id="typical-values-of-perplexity-goodman"><a href="#typical-values-of-perplexity-goodman"> Typical values of perplexity (Goodman)</a></h4>
<ul>
<li><span class="math">\(|V| = 50,000\)</span>.</li>
<li>Trigram model, second-order Markov process, <span class="math">\(p(x_1 \dots x_n) = \prod_{i=1}^{n} q(x_i|x_{i-2},x_{i-1})\)</span> gave perplexity = 74.</li>
<li>This is vastly smaller than the vocabulary size, so this is vastly superior to the uniform distribution.</li>
<li>Bigram model, a first-order Markov process, <span class="math">\(p(x_1 \ldots x_n) = \prod_{i=1}^{n}q(x_i|x_{i-1})\)</span> gave perplexity = 137.</li>
<li>Unigram model, <span class="math">\(p(x_1 \ldots x_n) = \prod_{i=1}^{n} q(x_i)\)</span>, gave perplexity = 955.
<ul>
<li>Predicting each word without using context of previous words.</li>
</ul></li>
</ul>
<h4 id="some-history"><a href="#some-history">Some history</a></h4>
<ul>
<li>Shannon conducted experiments on entropy of English. See “Prediction and entropy of printed English”, 1951.</li>
<li>Chomsky, in “Syntactic Structures”, 1957
<ul>
<li>“Colorless green ideas sleep furiously”</li>
<li>“Furiously sleep ideas green colorless”</li>
<li>Argues probability has little to offer for semantic sense and grammatical validity.</li>
<li>Very much against Shannon’s experiments with Markov processes and language.</li>
<li>Later in the course we’ll look at PCFGs that capture long-range dependencies.</li>
</ul></li>
</ul>
<h2 id="week-1---parameter-estimation-in-language-models"><a href="#week-1---parameter-estimation-in-language-models">Week 1 - Parameter Estimation in Language Models</a></h2>
<h3 id="linear-interpolation-part-1"><a href="#linear-interpolation-part-1">Linear Interpolation (Part 1)</a></h3>
<ul>
<li>Recall the “Sparse Data Problems” section before.</li>
</ul>
<h4 id="the-bias-variance-trade-off"><a href="#the-bias-variance-trade-off">The Bias-Variance Trade-Off</a></h4>
<ul>
<li>Trigram ML estimate</li>
</ul>
<p><span class="math">\[q_{ML}(w_i\;|\;w_{i-2},w_{i-1}) = \frac{\textrm{Count}(w_{i-2},w_{i-1},w_i)}{\textrm{Count}(w_{i-2},w_{i-1})}\]</span></p>
<ul>
<li>Bigram ML estimate</li>
</ul>
<p><span class="math">\[q_{ML}(w_i\;|\;w_{i-1}) = \frac{\textrm{Count}(w_{i-1},w_i)}{\textrm{Count}(w_{i-1})}\]</span></p>
<ul>
<li>Unigram ML estimate</li>
</ul>
<p><span class="math">\[q_{ML} = \frac{\textrm{Count}(w_i)}{\textrm{Count}()}\]</span></p>
<ul>
<li>The trigram MLE’s advantage is that it conditions on a lot of context, so given sufficient training data these counts will be high and it will converge to the “true value”.
<ul>
<li>This has <strong>relatively low bias</strong>. It is able to generalise from one particular training set to other unknown data.</li>
</ul></li>
<li>The unigram MLE completely ignores context, and so it will converge to a less-good estimator as the number of training samples increases.
<ul>
<li>This has <strong>relatively high bias</strong>.</li>
</ul></li>
<li>The trigram MLE’s disadvantage is that many counts will be equal to zero, so we need many samples to get a good estimate.
<ul>
<li>This has <strong>relatively high variance</strong>. It needs far more data to be able to generalise; if it has insufficient data it will not learn / generalise.</li>
</ul></li>
<li>The unigram MLE’s count will converge relatively quickly to their expected value, and so don’t need many samples.</li>
<li>The bigram MLE is in between the trigram MLE and unigram MLE.</li>
</ul>
<h3 id="linear-interpolation-part-2"><a href="#linear-interpolation-part-2"> Linear Interpolation (Part 2)</a></h3>
<h4 id="linear-interpolation"><a href="#linear-interpolation"> Linear Interpolation</a></h4>
<ul>
<li>Take our estimate <span class="math">\(q(w_i\;|\;w_{i-2},w_{i-1})\)</span> to be:</li>
</ul>
<p><span class="math">\(= \lambda_1 \times q_{ML}(w_i\;|\;w_{i-2},w_{i-1})\)</span><br /><span class="math">\(+ \lambda_2 \times q_{ML}(w_i\;|\;w_{i-1})\)</span><br /><span class="math">\(+ \lambda_3 \times q_{ML}(w_i)\)</span></p>
<ul>
<li>where <span class="math">\(\lambda_1 + \lambda_2 + \lambda_3 = 1\)</span> and <span class="math">\(\lambda_i \ge 0\;\forall\; i\)</span>.</li>
<li>New estimate is a weighted average of the three MLEs.</li>
<li>For example, assuming <span class="math">\(\lambda_1 = \lambda_2 = \lambda_3 = \frac{1}{3}\)</span></li>
</ul>
<p><span class="math">\(q(\textrm{laughs | the, dog})\)</span><br /><span class="math">\(= \frac{1}{3} \times q_{ML}(\textrm{laughs | the, dog})\)</span><br /><span class="math">\(+ \frac{1}{3} \times q_{ML}(\textrm{laughs | dog})\)</span><br /><span class="math">\(+ \frac{1}{3} \times q_{ML}(\textrm{laughs})\)</span></p>
<hr />
<p>Quiz: we are given the following corpus:</p>
<ul>
<li>the green book STOP</li>
<li>my blue book STOP</li>
<li>his green house STOP</li>
<li>book STOP</li>
</ul>
<p>Assume we compute a language model based on this corpus using linear interpolation with <span class="math">\(\lambda_i = \frac{1}{3}\;\forall\;i \in \{1,2,3\}\)</span>.</p>
<p>What is the value of the parameter <span class="math">\(q_{LI}(\textrm{book | the, green})\)</span> in this model to 3dp? (Note: please include STOP words in your unigram model).</p>
<p><span class="math">\(q_{LI}(\textrm{book | the, green})\)</span><br /><span class="math">\(= \frac{1}{3} \times q_{ML}(\textrm{book | the, green})\)</span><br /><span class="math">\(+ \frac{1}{3} \times q_{ML}(\textrm{book | green})\)</span><br /><span class="math">\(+ \frac{1}{3} \times q_{ML}(\textrm{book})\)</span></p>
<p><span class="math">\(= \frac{1}{3} \times \frac{\textrm{Count(the, green, book)}}{\textrm{Count(the, green)}}\)</span><br /><span class="math">\(+ \frac{1}{3} \times \frac{\textrm{Count(green, book)}}{\textrm{Count(green)}}\)</span><br /><span class="math">\(+ \frac{1}{3} \times \frac{\textrm{Count(book)}}{\textrm{Count()}}\)</span></p>
<p><span class="math">\(= \frac{ \frac{1}{3}(1) }{(1)} + \frac{ \frac{1}{3}(1) }{(2)} + \frac{ \frac{1}{3}(3) }{(14)}\)</span><br /><span class="math">\(= 0.571\;\textrm(3dp)\)</span></p>
<hr />
<p>Our estimate correctly defines a distribution. Define <span class="math">\(V^{&#39;} = V \cup \{STOP\}.\)</span></p>
<p><span class="math">\(\sum_{w \in V^{&#39;}} q(w|u,v)\)</span><br /><span class="math">\(=\sum_{w \in V^{&#39;}} [\lambda_1 \times q_{ML}(w|u,v) + \lambda_2 \times q_{ML}(w|v) + \lambda_3 \times q_{ML}(w)]\)</span></p>
<p>move out the constant lambdas:</p>
<p><span class="math">\(=\lambda_1 \sum_w q_{ML}(w|u,v) + \lambda_2 \sum_w q_{ML}(w|v) + \lambda_3 \sum_w q_{ML}(w)\)</span></p>
<p>By definition the maximum likelihood estimates in a given trigram, bigram, or unigram model sum to 1. Intuitively, the probability of each given trigram, bigram, or unigram probability in the model sums to 1.</p>
<p><span class="math">\(= \lambda_1 + \lambda_2 + \lambda_3 = 1\)</span></p>
<p>(Can also show that <span class="math">\(q(w|u,v) \ge 0\;\forall\;w \in V^{&#39;}\)</span>).</p>
<hr />
<p>Quiz: say we have <span class="math">\(\lambda_1 = -0.5, \lambda_2 = 0.5, \lambda_3 = 1.0\)</span>. Note that these satisfy the constraint <span class="math">\(\sum_i \lambda_i = 1\)</span>, but violate the constraint that <span class="math">\(\lambda_i \ge 0\)</span>.</p>
<p>(Credit to <a href="https://www.coursera.org/user/i/82f8725f5c57afaa5ef1cdded36d5f1d">Philip M. Hession</a> for the explanations).</p>
<p>Recalling our definition of <span class="math">\(q\)</span> above within: <span class="math">\(\sum_{w \in V^{&#39;}} q(w|u,v)\)</span>, it’s hence true that there might be a trigram <span class="math">\(u,v,w\)</span> such that <span class="math">\(q(w|u,v) \lt 0\)</span>:</p>
<p><span class="math">\[q(\text{barks}|\text{the,dog})=-\frac{1}{2}\frac{c(\text{the,dog,barks})}{c(\text{the,dog})}+\frac{1}{2}\frac{c(\text{dog,barks})}{c(\text{dog})}+1\cdot\frac{c(\text{barks})}{c()}\]</span></p>
<ul>
<li>if <span class="math">\(c() \gg c(\text{barks})\)</span></li>
<li>and if <span class="math">\(c(\text{dog}) \gg c(\text{dog,barks})\)</span></li>
<li>and if <span class="math">\(c(\text{the,dog}) \approx c(\text{the,dog,barks})\)</span></li>
</ul>
<p>then <span class="math">\(q(\text{barks}|\text{the,dog})=-\frac{1}{2}(\sim 1)+\frac{1}{2}(\ll 1)+1(\ll 1) \lt0\)</span></p>
<p>and there might be a trigram <span class="math">\(u,v,w\)</span> such that <span class="math">\(q(w|u,v) \gt 1\)</span>:</p>
<p><span class="math">\[q(\text{barks}|\text{the,dog})=-\frac{1}{2}\frac{c(\text{the,dog,barks})}{c(\text{the,dog})}+\frac{1}{2}\frac{c(\text{dog,barks})}{c(\text{dog})}+1\cdot\frac{c(\text{barks})}{c()}\]</span></p>
<ul>
<li>if <span class="math">\(c() \approx c(\text{barks})\)</span></li>
<li>and if <span class="math">\(c(\text{dog}) \approx c(\text{dog,barks})\)</span></li>
<li>and if <span class="math">\(c(\text{the,dog}) \gg c(\text{the,dog,barks})\)</span></li>
</ul>
<p>then <span class="math">\[q(\text{barks}|\text{the,dog})=-\frac{1}{2}(\ll 1)+\frac{1}{2}(\sim 1)+1(\sim 1) \gt 1\]</span></p>
<p>It is not true that we may have a bigram <span class="math">\(u,v\)</span> such that <span class="math">\(\sum_{w \in V} q(w|u,v) \neq 1\)</span>:</p>
<p><span class="math">\[\sum_{w}q(w|u,v) = -\frac{1}{2}\frac{\sum_w c(u,v,w)}{c(u,v)}+\frac{1}{2}\frac{\sum_w c(v,w)}{c(v)}+1\cdot\frac{\sum_w c(w)}{c()} = -\frac{1}{2}(1)+\frac{1}{2}(1)+1(1)=1\]</span></p>
<p>since <span class="math">\(\sum_w c(u,v,w)=c(u,v)\)</span>, <span class="math">\(\sum_w c(v,w)=c(v)\)</span>, and <span class="math">\(\sum_w c(w)=c()\)</span>.</p>
<hr />
<h4 id="how-to-estimate-the-lambda-values"><a href="#how-to-estimate-the-lambda-values">How to estimate the <span class="math">\(\lambda\)</span> values?</a></h4>
<ul>
<li>Hold out part of the training set as “validation” data.</li>
<li>Define <span class="math">\(c^{&#39;}(w_1,w_2,w_3)\)</span> to be the number of times the trigram <span class="math">\((w_1,w_2,w_3)\)</span> is seen in the validation set.</li>
<li>Take some small portion of all of our sentences, say 5%, as validation.</li>
<li>We train on the 95% bigger portion.</li>
<li>Define <span class="math">\(c^{&#39;}\)</span> as the number of times we see the training data in the smaller, other set.</li>
<li>Choose <span class="math">\(\lambda_1, \lambda_2, \lambda_3\)</span> to maximize:</li>
</ul>
<p><span class="math">\[L(\lambda_1,\lambda_2,\lambda_3) = \sum_{w_1,w_2,w_3} c^{&#39;}(w_1,w_2,w_3)\;\textrm{log}\;q(w_3|w_1,w_2)\]</span></p>
<p>such that <span class="math">\(\lambda_1 + \lambda_2 + \lambda_3 = 1\)</span> and <span class="math">\(\lambda_i \ge 0\;\forall\;i\)</span> and where:</p>
<p><span class="math">\(q(w_i|w_{i-2},w_{i-1}) =\)</span><br /><span class="math">\(\lambda_1  \times q_{ML}(w_i|w_{i-2},w_{i-1})\)</span><br /><span class="math">\(+\lambda_2 \times q_{ML}(w_i|w_{i-1})\)</span><br /><span class="math">\(+\lambda_3 \times q_{ML}(w_i)\)</span></p>
<ul>
<li>Many of the <span class="math">\(c^{&#39;}(w_1,w_2,w_3)\)</span> counts will of course be zero.</li>
<li>Optimization problem to maximize L, under the contraints that the lambdas are positive and sum to one.</li>
<li>If you maximize L it is easy to show that you minimize the perplexity of the language model with respect to the validation data.</li>
</ul>
<h4 id="allowing-the-lambdas-to-vary"><a href="#allowing-the-lambdas-to-vary">Allowing the <span class="math">\(\lambda\)</span>’s to vary</a></h4>
<ul>
<li>Take a function <span class="math">\(\Pi\)</span> that partitions histories, e.g. for some bigram:</li>
</ul>
<p><span class="math">\[
\begin{equation}
    \Pi(w_{i-2},w_{i-1}) = \begin{cases}
        1, &amp; \textrm{If Count}(w_{i-1},w_{i-2}) = 0\\
        2, &amp; \textrm{If 1} \le \textrm{Count}(w_{i-1},w_{i-2}) \le 2\\
        3, &amp; \textrm{If 3} \le \textrm{Count}(w_{i-1},w_{i-2}) \lt 5\\
        4, &amp; \textrm{Otherwise}
    \end{cases}
\end{equation}
\]</span></p>
<ul>
<li>Introduce a dependence of the <span class="math">\(\lambda\)</span>’s on the partition:</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        q(w_i\;|\;w_{i-2},w_{i-1}) &amp; = \lambda_1^{\Pi(w_{i-2},w_{i-1})} \times q_{ML}(w_i\;|\;w_{i-2},w_{i-1}) \\
        &amp;\; + \lambda_2^{\Pi(w_{i-2},w_{i-1})} \times q_{ML}(w_i\;|\;w_{i-1}) \\
        &amp;\; + \lambda_3^{\Pi(w_{i-2},w_{i-1})} \times q_{ML}(w_i)
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>where <span class="math">\(\lambda_1^{\Pi(w_{i-2},w_{i-1})} + \lambda_2^{\Pi(w_{i-2},w_{i-1})} + \lambda_3^{\Pi(w_{i-2},w_{i-1})} = 1\)</span>, and <span class="math">\(\lambda_i^{\Pi(w_{i-2},w_{i-1})} \ge 0\;\forall\;i\)</span>.</li>
<li>Instead of just 3 lambdas now we have 3 * 4 = 12 lambdas, one per MLE per partition, and we determine which parition to use based on the bigram count.
<ul>
<li>We condition on the bigram counts.</li>
<li><span class="math">\(\lambda_1^1, \lambda_2^1, \lambda_3^1\)</span>. These counts are used if the bigram count is 0.</li>
<li><span class="math">\(\lambda_1^2, \lambda_2^2, \lambda_3^2\)</span>. These counts are used if the bigram count is [1, 2].</li>
<li><span class="math">\(\lambda_1^3, \lambda_2^3, \lambda_3^3\)</span>. These counts are used if the bigram count is [3, 5).</li>
<li><span class="math">\(\lambda_1^4, \lambda_2^4, \lambda_3^4\)</span>. These counts are used if the bigram count is [5, <span class="math">\(\infty\)</span>).</li>
</ul></li>
<li>Partitions are generally chosen by hand, but this one is a typical definition.</li>
<li>These 12 lambdas are optimized according to L as before using validation data.</li>
<li>If this bigram count is 0 then parameter <span class="math">\(\lambda_1\)</span> will also be equal to 0, else it is undefined.
<ul>
<li>Recall that <span class="math">\(\lambda_1\)</span> is for the trigram MLE, and the bigram count is in the denominator.</li>
</ul></li>
</ul>
<h3 id="discounting-methods-part-1"><a href="#discounting-methods-part-1">Discounting Methods (Part 1)</a></h3>
<ul>
<li>Suppose we have a table of bigrams, their counts, and corresponding <span class="math">\(q_{ML}(w_i\;|\;w_{i-1})\)</span>.</li>
</ul>
<table>
<thead>
<tr class="header">
<th align="left">x</th>
<th align="left">Count(x)</th>
<th align="left"><span class="math">\(q_{ML}(w_i\;|\;w_{i-1})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">the</td>
<td align="left">48</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">the, dog</td>
<td align="left">15</td>
<td align="left"><span class="math">\(^{15}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, woman</td>
<td align="left">11</td>
<td align="left"><span class="math">\(^{11}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, man</td>
<td align="left">10</td>
<td align="left"><span class="math">\(^{10}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, park</td>
<td align="left">5</td>
<td align="left"><span class="math">\(^{5}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, job</td>
<td align="left">2</td>
<td align="left"><span class="math">\(^{2}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, telescope</td>
<td align="left">1</td>
<td align="left"><span class="math">\(^{1}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, manual</td>
<td align="left">1</td>
<td align="left"><span class="math">\(^{1}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, afternoon</td>
<td align="left">1</td>
<td align="left"><span class="math">\(^{1}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, country</td>
<td align="left">1</td>
<td align="left"><span class="math">\(^{1}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, street</td>
<td align="left">1</td>
<td align="left"><span class="math">\(^{1}/_{48}\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>The MLEs are systematically high, especially if we have a large vocabulary. This is particularly true for the low count items.</li>
<li><p>In a sense these words that follow “the” are just lucky; what about those poor words that don’t appear after “the” in this data set but, in the “true” language, actually can appear after “the”?</p></li>
<li><p>Now define “discounted” counts, <span class="math">\(\textrm{Count}^{*}(x) = \textrm{Count}(x) - 0.5\)</span></p></li>
</ul>
<table>
<thead>
<tr class="header">
<th align="left">x</th>
<th align="left">Count(x)</th>
<th align="left">Count*(x)</th>
<th align="left"><span class="math">\(\frac{\textrm{Count*(x)}}{\textrm{Count(the)}}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">the</td>
<td align="left">48</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">the, dog</td>
<td align="left">15</td>
<td align="left">14.5</td>
<td align="left"><span class="math">\(^{14.5}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, woman</td>
<td align="left">11</td>
<td align="left">10.5</td>
<td align="left"><span class="math">\(^{10.5}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, man</td>
<td align="left">10</td>
<td align="left">9.5</td>
<td align="left"><span class="math">\(^{9.5}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, park</td>
<td align="left">5</td>
<td align="left">4.5</td>
<td align="left"><span class="math">\(^{4.5}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, job</td>
<td align="left">2</td>
<td align="left">1.5</td>
<td align="left"><span class="math">\(^{1.5}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, telescope</td>
<td align="left">1</td>
<td align="left">0.5</td>
<td align="left"><span class="math">\(^{0.5}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, manual</td>
<td align="left">1</td>
<td align="left">0.5</td>
<td align="left"><span class="math">\(^{0.5}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, afternoon</td>
<td align="left">1</td>
<td align="left">0.5</td>
<td align="left"><span class="math">\(^{0.5}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, country</td>
<td align="left">1</td>
<td align="left">0.5</td>
<td align="left"><span class="math">\(^{0.5}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, street</td>
<td align="left">1</td>
<td align="left">0.5</td>
<td align="left"><span class="math">\(^{0.5}/_{48}\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>There is some missing or left over probability mass; if we sum the right-hand column you get <span class="math">\(\frac{43}{48} \lt 1\)</span>.</li>
<li>The left over probability mass, in this case, is <span class="math">\(\frac{5}{48}\)</span>.</li>
<li><p>The essence of discounting is to take this left over probability mass and distribute it back to the words that do not appear after “the” in this data set.</p></li>
<li><p>We’ll define for any word <span class="math">\(w_{i-1}\)</span> <span class="math">\(\alpha\)</span>, which is the left-over or missing probability mass:</p></li>
</ul>
<p><span class="math">\[\alpha(w_{i-1}) = 1 - \sum_{w} \frac{\textrm{Count}^{*}(w_{i-1},w)}{\textrm{Count}(w_{i-1})}\]</span></p>
<ul>
<li>e.g. in our example, <span class="math">\(\alpha(\textrm{the}) = 10 \times 0.5/48 = 5/48\)</span>.</li>
</ul>
<hr />
<p>Quiz: assume that we are given a corpus with the following properties:</p>
<ul>
<li>Count(the) = 70</li>
<li>|{w: c(the, w) &gt; 0}| = 15, i.e. there are 15 different words that follow “the”.</li>
</ul>
<p>Furthermore assume that the discounted counts are defined as <span class="math">\(c^{*}(\textrm{the,w}) = c(\textrm{the,w}) - 0.3\)</span>. Under this corpus, what is the missing probability mass <span class="math">\(\alpha(\textrm{the})\)</span> to 3dp?</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
    \alpha(\textrm{the}) &amp; = 1 - \sum_{w} \frac{\textrm{Count}^{*}(\textrm{the, w})}{\textrm{Count(the)}} \\
    &amp; = \frac{\textrm{Count(the)}}{\textrm{Count(the)}} - \frac{1}{\textrm{Count(the)}} \times \sum_{w} \textrm{Count}^{*}(\textrm{the,w}) \\
    &amp; = \frac{\textrm{Count(the)} - \sum_{w} \textrm{Count}^{*}\textrm{(the, w)}}{\textrm{Count(the)}} \\
    &amp; = \frac{\textrm{Count(the)} - \sum_{w} \left\{ \textrm{Count(the, w)} - 0.3\right\}}{\textrm{Count(the)}} \\
    &amp; = \frac{\textrm{Count(the)} + \sum_{w}(0.3) - \textrm{Count(the)}}{\textrm{Count(the)}} \\
    &amp; = \frac{0.3w}{\textrm{Count(the)}} \\
    &amp; = \frac{(0.3)(15)}{70} = 0.064\;\textrm{(3 dp)}
    \end{aligned}
\end{align}
\]</span></p>
<hr />
<h4 id="katz-back-off-models-bigrams"><a href="#katz-back-off-models-bigrams">Katz Back-Off Models (Bigrams)</a></h4>
<ul>
<li>For a bigram model, define two sets</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        A(w_{i-1}) &amp; = \left\{w : \textrm{Count}(w_{i-1},w) \gt 0\right\} \\
        B(w_{i-1}) &amp; = \left\{w : \textrm{Count}(w_{i-1},w) = 0\right\}
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>Assuming <span class="math">\(\alpha\)</span> such that:</li>
</ul>
<p><span class="math">\[\alpha(w_{i-1}) = 1 - \sum_{w \in A(w_{i-1})} \frac{\textrm{Count}^{*}(w_{i-1},w)}{\textrm{Count}(w_{i-1})}\]</span></p>
<ul>
<li>And <span class="math">\(\textrm{Count}^{*}\)</span> is such that:</li>
</ul>
<p><span class="math">\[\textrm{Count}^{*}(w_{i-1},w_i) = \textrm{Count}(w_{i-1},w_i) - \gamma\\ \textrm{where $\gamma$ is a constant}\]</span></p>
<ul>
<li>A bigram model</li>
</ul>
<p><span class="math">\[
\begin{equation}
    q_{BO}(w_i\;|\;w_{i-1}) = \begin{cases}
        \frac{\textrm{Count}^{*}(w_{i-1},w_i)}{\textrm{Count}(w_{i-1})}, &amp; \textrm{If } w_i \in A(w_{i-1})\\
        \alpha(w_{i-1})\frac{q_{ML}(w_i)}{\sum_{w \in B(w_{i-1})} q_{ML}(w)}, &amp; \textrm{If } w_i \in B(w_{i-1}) 
    \end{cases}
\end{equation}
\]</span></p>
<ul>
<li><span class="math">\(A(w_{i-1})\)</span> is the set of words whose bigram count is greater than 0, so they follow e.g. “the”.</li>
<li><span class="math">\(B(w_{i-1})\)</span> is the set of words whose bigram count is 0, so they’re never seen to follow e.g. “the”.</li>
<li><span class="math">\(\alpha(w_{i-1})\)</span> is the missing probability mass.</li>
<li><span class="math">\(\frac{\textrm{Count}^{*}(w_{i-1},w_i)}{\textrm{Count}(w_{i-1})}\)</span> is the discounted count for the words who are seen to follow e.g. “the”.</li>
<li>If the word is never seen after e.g. “the”, rather than set its <span class="math">\(q(w_i|w_{i-1})\)</span> parameter to 0 we assign it a portion of the missing probabiliy mass <span class="math">\(\alpha(w_{i-1})\)</span>, in proportion to its the unigram maximum-likelihood estimate <span class="math">\(q_{ML}(w_i)\)</span> divided by the sum of all the unigram MLEs for other such words <span class="math">\(\sum_{w \in B(w_{i-1})} q_{ML}(w)\)</span>.</li>
</ul>
<hr />
<p>Quiz: Let’s return to a smaller version of our corpus:</p>
<ul>
<li>the book STOP</li>
<li>his house STOP</li>
</ul>
<p>This time we computer a bigram language model using Katz back-off with <span class="math">\(c^{*}(v,w) = c(v,w) - 0.5\)</span>.</p>
<p>What is the value of <span class="math">\(q_{BO}(\textrm{book | his})\)</span> estimated from this corpus?</p>
<p><span class="math">\[w_i = \textrm{book}, w_{i-1} = \textrm{his}\]</span></p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        A(\textrm{his}) &amp; = \textrm{{house}} \\
        B(\textrm{his}) &amp; = \textrm{{his, the, book, STOP}}
    \end{aligned}
\end{align}
\]</span></p>
<p>Draw a table for <span class="math">\(w_{i-1}\)</span> and all words that follow it, in order to determine <span class="math">\(\alpha(w_{i-1})\)</span></p>
<table>
<thead>
<tr class="header">
<th align="left">x</th>
<th align="left">Count(x)</th>
<th align="left">Count*(x)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">his</td>
<td align="left">1</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">his, house</td>
<td align="left">1</td>
<td align="left">0.5</td>
</tr>
</tbody>
</table>
<p><span class="math">\[\alpha(\textrm{his}) = 1 - (0.5)/(1) = 0.5\]</span></p>
<p>Since <span class="math">\(\textrm{book} \in B(\textrm{his})\)</span>, i.e. since “book” never follows “his” in the corpus:</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        \sum_{w \in B(w_{i-1})} q_{ML}(w) &amp; = q_{ML}(\textrm{his}) + q_{ML}(\textrm{the}) + q_{ML}(\textrm{book}) + q_{ML}(\textrm{STOP}) \\
        &amp; = (1/6) + (1/6) + (1/6) + (2/6) \\
        &amp; = 5/6
    \end{aligned}
\end{align}
\]</span></p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        q_{BO}(\textrm{book | his}) &amp; = \alpha(w_{i-1})\frac{q_{ML}(w_i)}{\sum_{w \in B(w_{i-1})} q_{ML}(w)} \\
        &amp; = (0.5) \times \frac{(1/6)}{(5/6)} \\
        &amp; = 0.1
    \end{aligned}
\end{align}
\]</span></p>
<hr />
<h3 id="discounting-methods-part-2"><a href="#discounting-methods-part-2">Discounting Methods (Part 2)</a></h3>
<h4 id="katz-back-off-models-trigrams"><a href="#katz-back-off-models-trigrams">Katz Back-Off Models (Trigrams)</a></h4>
<ul>
<li>For a trigram model, first define two sets</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        A(w_{i-2},w_{i-1}) &amp; = \left\{w : \textrm{Count}(w_{i-2},w_{i-1},w) \gt 0\right\} \\
        B(w_{i-2},w_{i-1}) &amp; = \left\{w : \textrm{Count}(w_{i-2},w_{i-1},w) = 0\right\}
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>A trigram model is defined in terms of the bigram model:</li>
</ul>
<p><span class="math">\[
\begin{equation}
    q_{BO}(w_i\;|\;w_{i-2},w_{i-1}) = \begin{cases}
        \frac{\textrm{Count}^{*}(w_{i-2},w_{i-1},w_i)}{\textrm{Count}(w_{i-2},w_{i-1})}, &amp; \textrm{If } w_i \in A(w_{i-2},w_{i-1})\\
        \alpha(w_{i-2},w_{i-1})\frac{q_{BO}(w_i|w_{i-1})}{\sum_{w \in B(w_{i-2},w_{i-1})} q_{BO}(w|w_{i-1})}, &amp; \textrm{If } w_i \in B(w_{i-2},w_{i-1}) 
    \end{cases}
\end{equation}
\]</span></p>
<p>where</p>
<p><span class="math">\[\alpha(w_{i-2},w_{i-1}) = 1 - \sum_{w \in A(w_{i-2},w_{i-1})} \frac{\textrm{Count}^{*}(w_{i-2},w_{i-1},w)}{\textrm{Count}(w_{i-2},w_{i-1})}\]</span></p>
<ul>
<li>The one variable is the discount constant. It is typically between 0 and 1, and it can also be chosen via optimization on a validation data set.</li>
</ul>
<h3 id="summary"><a href="#summary"> Summary</a></h3>
<ul>
<li>Three steps in deriving the language model probabilities:
<ol style="list-style-type: decimal">
<li>Expand <span class="math">\(p(w_1, w_2, \ldots, w_n)\)</span> using <em>Chain Rule</em>.</li>
<li>Make <em>Markov Independence Assumptions</em>, i.e. <span class="math">\(p(w_i\;|\;w_1, w_2, \ldots, w_{i-2}, w_{i-1}) = p(w_i\;|\;w_{i-2},w_{i-1})\)</span></li>
<li><em>Smooth</em> the estimates using low order counts; linear interpolation and discounting.</li>
</ol></li>
<li>Other methods used to improve language models
<ul>
<li>“Topic” or “long-range” features.
<ul>
<li>Condition on the topic of the document within which sentences belong.</li>
<li>Condition on words outside of the two-word window under the second-order Markov assumption.</li>
</ul></li>
<li>Syntactic models
<ul>
<li>Grammatical information.</li>
</ul></li>
</ul></li>
<li>It’s generally hard to improve on trigram models though!</li>
</ul>
<h2 id="week-2---tagging-problems-and-hidden-markov-models"><a href="#week-2---tagging-problems-and-hidden-markov-models"> Week 2 - Tagging Problems and Hidden Markov Models</a></h2>
<h3 id="the-tagging-problem"><a href="#the-tagging-problem">The Tagging Problem</a></h3>
<h4 id="part-of-speech-tagging"><a href="#part-of-speech-tagging"> Part-of-Speech Tagging</a></h4>
<ul>
<li>We’d like to model <em>pairs of sequences</em>, rather than just one sequence.
<ul>
<li>The general problem is the <strong>sequence labelling problem</strong>, aka the <strong>tagging problem</strong>.</li>
</ul></li>
<li><p>Two instances of this problem - POS tagging and named entity recognition.</p></li>
<li><strong>Part-of-Speech Tagging</strong>: a fundamental problem.
<ul>
<li>Input: sentence.</li>
<li>Output: a tag sequence, aka a state sequence.</li>
</ul></li>
<li><p>Input, some sequence of words, a sentence:</p></li>
</ul>
<pre><code>Profits soared at Boeing Co., easily topping forecasts on Wall
Street, as their CEO Alan Nulally announced first quarter
results.</code></pre>
<ul>
<li>Tags:</li>
</ul>
<pre><code>N   =   Noun
V   =   Verb
P   =   Preposition
Adv =   Adverb
Adj =   Adjective
...</code></pre>
<ul>
<li>Output, a <em>tag sequence</em>:</li>
</ul>
<pre><code>Profits/N soared/V at/P Boeing/N Co./N ,/, easily/ADV
topping/V forecasts/N on/P Wall/N Street/N ,/, as/P
their/POSS CEO/N Alan/N Mulally/N announced/V first/ADJ
quarter/N results/N ./.</code></pre>
<ul>
<li>But context matters.
<ul>
<li><code>profits</code> isn’t always a noun, it can sometimes be a verb.</li>
<li><code>topping</code> is a verb, but can sometimes be a noun.</li>
<li>…</li>
</ul></li>
<li>Also, individual words, regardless of context, have a preference for their part of speech.
<ul>
<li><code>quarter</code> can be a noun or a verb, but in general is more likely to be a noun.</li>
</ul></li>
<li>Also some words are very rare, and may not show up in the training data.
<ul>
<li>Important to be able to deal with this.</li>
</ul></li>
</ul>
<h4 id="named-entity-recognition"><a href="#named-entity-recognition"> Named Entity Recognition</a></h4>
<ul>
<li><strong>Named Entity Recognition</strong>
<ul>
<li>Input: a sentence.</li>
<li>Output: identify names and their type (company, location, person, …)</li>
</ul></li>
<li>Input: same as above</li>
<li>Output:</li>
</ul>
<pre><code>Profits soared at [Company: Boeing Co.], easily ...
[Location: Wall Street], ..., [Person: Alan Mulally]</code></pre>
<ul>
<li>At first blush named entity recognition looks like segmentation, not part-of-speech tagging. But really they’re the same.</li>
</ul>
<h4 id="named-entity-extraction-as-tagging"><a href="#named-entity-extraction-as-tagging"> Named Entity Extraction as Tagging</a></h4>
<ul>
<li>Input: same as above</li>
<li>Tags:</li>
</ul>
<pre><code>NA  =   No entity
SC  =   Start Company
CC  =   Continue Company
SL  =   Start Location
CL  =   Continue Location
...</code></pre>
<ul>
<li>Output:</li>
</ul>
<pre><code>Profits/NA soared/NA at/NA Boeing/SC Co./CC ,/NA easily/NA
topping/NA ... Wall/SL Street/CL ,/NA ... CEO/NA Alan/SP
Mulally/CP ...</code></pre>
<ul>
<li>We are <em>encoding</em> the named entity boundaries as a tag sequence.</li>
</ul>
<hr />
<p>Quiz: given sentence: <code>Profits are topping all estimates</code>.</p>
<p>We also know:</p>
<ul>
<li><code>Profits</code> can be N or V.</li>
<li><code>are</code> is V</li>
<li><code>topping</code> can be N, ADJ, or V.</li>
<li><code>all</code> can be DT, ADV, or N.</li>
<li><code>estimates</code> can be N or V.</li>
</ul>
<p>How many tag sequences are possible?</p>
<p><span class="math">\[= 2 \times 1 \times 3 \times 3 \times 2 = 36\]</span></p>
<hr />
<ul>
<li>Objective: treating this like a supervised machine learning problem
<ul>
<li>Use a very common resource, called the “Wall Street Journal Treebank”.</li>
<li>Features: sentences (not individual words).</li>
<li>Training set: 38,219 sentences, each with tagged words.
<ul>
<li>Annotated by hand (!)</li>
</ul></li>
<li>Label: a sentence with each word tagged.</li>
<li>!!AI there are a lot of tags here. A reference list of tags is available in the <a href="http://bulba.sdsu.edu/jeanette/thesis/PennTags.html">Penn Treebank Tags</a>.</li>
<li>Output: a functon that maps sentences to tagged words.</li>
</ul></li>
<li>There are now many corpora available, across many languages.</li>
</ul>
<h4 id="two-types-of-contraints"><a href="#two-types-of-contraints">Two Types of Contraints</a></h4>
<pre><code>Influential/JJ members/NNS of/IN ... bailout/NN agency/NN
can/MD raise/VB capital/NN ./.</code></pre>
<ul>
<li>What will help us in this problem? Two constraints:
<ol style="list-style-type: decimal">
<li><strong>Local</strong>: e.g. <em>can</em> is more likely to be a modal verb (MD) than a noun (NN).
<ul>
<li>A <a href="http://en.wikipedia.org/wiki/Modal_verb">modal verb</a> (MD) is an auxillary verb used to indicate likelihood, ability, permission, and obligation.</li>
</ul></li>
<li><strong>Contextual</strong>: e.g. a noun (NN) is more likely than a verb (VB*) to follow a determiner (DT).
<ul>
<li>(e.g. <code>the can</code> is more likely to refer to a can of soup than talk about <code>the</code>’s ability to do something)</li>
<li>A <a href="http://en.wikipedia.org/wiki/Determiner">determiner</a> (DT) is a word, phrase, or affix that occurs together with a noun (NN).</li>
<li>DT can be indefinite articles (<code>the</code>, <code>a</code>, <code>an</code>), demonstratives (<code>this</code>, <code>that</code>), quantifiers (<code>many</code>, <code>few</code>, <code>several</code>).</li>
<li>Recall that an affix is a morpheme that attaches to word stems. Can be prefix, suffix, infix (in the middle of a word) or circumfix (on both sides of the word)</li>
</ul></li>
</ol></li>
<li>Sometimes the contraints are in conflict:</li>
</ul>
<pre><code>The trash can is in the garage.</code></pre>
<ul>
<li><code>can</code> has a <em>local</em> preference to be a modal verb (MD) because it follows a noun.</li>
<li>But clearly <code>can</code> belongs as a whole with <code>trash can</code>, so it depends on <em>context</em>.</li>
<li>We can build a model that balances these two contraints.</li>
</ul>
<h3 id="generative-models-for-supervised-learning"><a href="#generative-models-for-supervised-learning">Generative Models for Supervised Learning</a></h3>
<h4 id="supervised-learning-problems"><a href="#supervised-learning-problems">Supervised Learning Problems</a></h4>
<ul>
<li>We have training examples <span class="math">\(x^{(i)}, y^{(i)}\)</span> for <span class="math">\(i = 1 \ldots m\)</span>.</li>
<li>Each <span class="math">\(x^{(i)}\)</span> is an <strong>input</strong>, each <span class="math">\(y^{(i)}\)</span> is a <strong>label</strong>.</li>
<li>Objective: learn a function <span class="math">\(f\)</span> that maps inputs <span class="math">\(x\)</span> to labels <span class="math">\(f(x)\)</span>.</li>
<li>e.g.</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        &amp; x^{(1)} = \textrm{the dog laughs}, &amp; y^{(1)} = \textrm{DT NN VB} \\
        &amp; x^{(2)} = \textrm{the dog barks}, &amp; y^{(2)} = \textrm{DT NN VB} \\
        &amp; \ldots &amp; \ldots
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>The first model you may consider is a <strong>conditional model</strong>.
<ul>
<li>Learn a distribution <span class="math">\(p(y|x)\)</span> from training examples.</li>
<li>For any test input <span class="math">\(x\)</span>, define <span class="math">\(f(x) = \textrm{arg max}_{y}p(y|x)\)</span>.
<ul>
<li>The <span class="math">\(y\)</span> that maximizes this conditional probability.</li>
<li>Input <span class="math">\(x\)</span>, search through all possible <span class="math">\(y\)</span>’s, return most likely <span class="math">\(y\)</span>.</li>
</ul></li>
</ul></li>
<li>Alternative are generative models.</li>
</ul>
<h4 id="generative-models"><a href="#generative-models"> Generative Models</a></h4>
<ul>
<li>Same problem.</li>
<li>Learn a <em>joint distribution</em> <span class="math">\(p(x,y)\)</span> from training examples.
<ul>
<li>Before we had <span class="math">\(p(y|x)\)</span>.</li>
</ul></li>
<li>Often we have <span class="math">\(p(x,y)\)</span> = <span class="math">\(p(y)p(x|y)\)</span>.
<ul>
<li><strong>Bayes Rule</strong>.</li>
<li><span class="math">\(p(y)\)</span> is the <strong>prior</strong> probability; how likely is <span class="math">\(y\)</span> a-priori?</li>
<li><span class="math">\(p(x|y)\)</span> is the <strong>conditional</strong> probability. <em>Given</em> <span class="math">\(y\)</span> how likely is <span class="math">\(x\)</span>?</li>
</ul></li>
<li>Note: by the total probability variant of Bayes Rule we have:</li>
</ul>
<p><span class="math">\[p(y|x) = \frac{p(y)p(x|y)}{p(x)}\]</span></p>
<ul>
<li>where:</li>
</ul>
<p><span class="math">\[p(x) = \sum_y p(y)p(x|y)\]</span></p>
<ul>
<li>Estimating <span class="math">\(p(y|x)\)</span> <em>directly</em> is often referred to as a <strong>discriminative model</strong>.
<ul>
<li>We will see a lot of discriminative models later in the course.</li>
</ul></li>
<li>Estimating <span class="math">\(p(x,y)\)</span> is a <strong>generative model</strong>.</li>
<li>There are pros and cons to each, a lot of research, back and forth.</li>
<li>Still confused about generative vs. discriminative? <a href="http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm">http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm</a>
<ul>
<li>A generative algorithm takes into account some model about how the data was generated, and then classifies the input.
<ul>
<li>By calculating <span class="math">\(p(x,y)\)</span> we have enough to shove it into Bayes’ rule in order to generate <span class="math">\(p(y|x)\)</span>.</li>
<li>However, if we want, we can generate more <span class="math">\((x,y)\)</span> that fit the model.</li>
</ul></li>
<li>A discriminative model doesn’t care and just classifies.
<ul>
<li>Given some input it discerns what’s necessary to map it onto the most likely output.</li>
</ul></li>
</ul></li>
<li>How do we apply a generative model to a new test example?</li>
<li>Output from the model:</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        f(x) &amp; = \textrm{argmax}_{y}\;p(y|x) \\
             &amp; = \textrm{argmax}_{y}\;\frac{p(y)p(x|y)}{p(x)} \\
             &amp; = \textrm{argmax}_{y}\;p(y)p(x|y)
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>Second line: assuming we have a generative model, by Bayes Rule.</li>
<li>Third line: <span class="math">\(p(x)\)</span> does not vary with <span class="math">\(y\)</span>. <span class="math">\(\textrm{argmax}\)</span> implies we’re searching over <span class="math">\(y\)</span>, but denominator is constant and hence we can discard it.
<ul>
<li>This is computationally very useful, can be expensive to calculate.</li>
</ul></li>
<li>Models that decompose the joint probability <span class="math">\(p(x,y)\)</span> into <span class="math">\(p(y)\)</span> and <span class="math">\(p(x|y)\)</span> are called <strong>noisy-channel models</strong>.
<ul>
<li>Intuitively, the input <span class="math">\(x\)</span> is generated in two steps.
<ol style="list-style-type: decimal">
<li>Label <span class="math">\(y\)</span> is chosen with probability <span class="math">\(p(y)\)</span>.</li>
<li>Input <span class="math">\(x\)</span> is generated from the distribution <span class="math">\(p(x|y)\)</span>.</li>
</ol></li>
</ul></li>
</ul>
<h3 id="hidden-markov-models"><a href="#hidden-markov-models">Hidden Markov Models</a></h3>
<ul>
<li>We have an input sentence <span class="math">\(x = x_1, x_2, \ldots, x_n\)</span>. (<span class="math">\(x_i\)</span> is the <span class="math">\(i\)</span>’th word in the sentence).</li>
<li>We have a tag sequence <span class="math">\(y = y_1, y_2, \ldots, y_n\)</span>. (<span class="math">\(y_i\)</span> is the <span class="math">\(i\)</span>’th tag in the sentence).</li>
<li>We’ll use an HMM to define:</li>
</ul>
<p><span class="math">\[p(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_n)\]</span></p>
<ul>
<li>for any sentence <span class="math">\(x_1 \ldots x_n\)</span> and tag sequence <span class="math">\(y_1 \ldots y_n\)</span> of the same length.
<ul>
<li>Note this is <strong>generative</strong> (<span class="math">\(p(x,y)\)</span>), not <strong>discriminative</strong> (<span class="math">\(p(y|x)\)</span>).</li>
<li>Think of the <span class="math">\(x_i\)</span> as an input and the <span class="math">\(y_i\)</span> as a label.</li>
</ul></li>
<li>Then the most likely tag sequence for <span class="math">\(x\)</span> is:</li>
</ul>
<p><span class="math">\[\textrm{arg}\underset{y_1 \ldots y_n}{\textrm{max}} p(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_n)\]</span></p>
<ul>
<li>The number of total possible sequences is <span class="math">\(O(2^n)\)</span>, so brute force search is not feasible.</li>
</ul>
<h4 id="trigram-hidden-markov-models-triagram-hmms"><a href="#trigram-hidden-markov-models-triagram-hmms">Trigram Hidden Markov Models (Triagram HMMs)</a></h4>
<ul>
<li>For any sentence <span class="math">\(x_1, x_2, \ldots, x_n\)</span>, where <span class="math">\(x_i \in V\)</span> for <span class="math">\(i = 1, 2, \ldots, n\)</span>, and</li>
<li>For any tag sequence <span class="math">\(y_1, y_2, \ldots, y_{n+1}\)</span>, where <span class="math">\(y_i \in S\)</span> for <span class="math">\(i = 1, 2, \ldots, n\)</span> and <span class="math">\(y_{n+1} = \textrm{STOP}\)</span>.</li>
<li>The joint probability of the sentence and tag sequence is:</li>
</ul>
<p><span class="math">\[p(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_{n+1}) = \prod_{i=1}^{n+1} q(y_i|y_{i-2},y_{i-1}) \prod_{i=1}^{n} e(x_i|y_i)\]</span></p>
<ul>
<li>An example of the joint probability could be <span class="math">\(p(\textrm{the, dog barks, DT, NN, VB, STOP})\)</span>.</li>
<li>The first product is a trigram model applied to tag sequences! Very similar to before.
<ul>
<li>One <span class="math">\(q\)</span> term for each tag <em>including the STOP symbol</em>.</li>
</ul></li>
<li>The second product could have e.g. <span class="math">\(e(\textrm{the | DT})\)</span> is the probability of a tag emitting or generating a word.
<ul>
<li>One <span class="math">\(e\)</span> term for each (tagged) word.</li>
</ul></li>
<li>where we’ve assumed, as before in Markov Models, that <span class="math">\(y_0 = y_{-1} = {*}\)</span> (the start symbol).</li>
<li><span class="math">\(V\)</span> is the set of possible words in the language, e.g. <span class="math">\(\{\textrm{the, dog, book, ate, his}\}\)</span></li>
<li><span class="math">\(S\)</span> is the set of possible tags, e.g. <span class="math">\(\{\textrm{DT, NN, VB, P, ADV, ...}\}\)</span>.
<ul>
<li><span class="math">\(\simeq\)</span> hundreds of tags; the Wall Street Journal courpus has <span class="math">\(\simeq\)</span> 50 tags.</li>
</ul></li>
<li>Parameters of the model:
<ul>
<li><span class="math">\(q(s|u,v)\;\forall\;s \in S \cup \{\textrm{STOP}\},\;u,v \in S \cup \{\textrm{*}\}\)</span>
<ul>
<li><strong>Trigram parameters</strong> (but referred to in a quiz as <strong>transition parameters</strong>).</li>
</ul></li>
<li><span class="math">\(e(x|s)\;\forall\;s \in S, x \in V\)</span>
<ul>
<li><strong>Emission parameters</strong>.</li>
</ul></li>
</ul></li>
<li>This model has the same form as a noisy-channel model.
<ul>
<li>The first <span class="math">\(q\)</span> parameters are the prior probability of the tags, i.e. <span class="math">\(p(y)\)</span>.</li>
<li>The second <span class="math">\(e\)</span> parameters are the conditional probabilities, i.e. <span class="math">\(p(x|y)\)</span>.</li>
</ul></li>
<li>Notice that the <span class="math">\(e\)</span> parameters have an independence assumption.
<ul>
<li>Any value for random variable <span class="math">\(X_i = x_i\)</span> is only dependent on <span class="math">\(Y_i = y_i\)</span>.</li>
<li>More formally given the value of <span class="math">\(Y_i\)</span> the value for <span class="math">\(X_i\)</span> is conditionally independent of both previous observations <span class="math">\(X_1 \ldots X_{i-1}\)</span> and other state values <span class="math">\(Y_1 \ldots Y_{i-1}, Y_{i+1}, \ldots Y_{n+1}\)</span>.</li>
<li>See notes p12.</li>
</ul></li>
<li>Useful thinking exercise - how do I generate sequence pairs <span class="math">\(y_1, \ldots, y_{n+1}, x_1, \ldots, x_n\)</span>?
<ol style="list-style-type: decimal">
<li>Initialize <span class="math">\(i=1\)</span> and <span class="math">\(y_0 = y_{-1} = \textrm{*}\)</span>.</li>
<li>Generate <span class="math">\(y_i\)</span> from distribution <span class="math">\(q(y_i|y_{i-2},y_{i-1})\)</span>.</li>
<li>If <span class="math">\(y_i = \textrm{STOP}\)</span> then return <span class="math">\(y_1 \ldots y_i, x_1 \ldots x_{i-1}\)</span>. Else, generate <span class="math">\(x\)</span> from distribution <span class="math">\(e(x_i|y_i)\)</span>.</li>
<li>Set <span class="math">\(i=i+1\)</span>, return to step 2.</li>
</ol></li>
</ul>
<hr />
<p>Quiz: Given tagset <span class="math">\(S = \{\textrm{D, N}\}\)</span>, a vocabulary <span class="math">\(V = \{\textrm{the, dog}\}\)</span>, and a HMM with transition parameters:</p>
<ul>
<li><span class="math">\(q(\textrm{D | *, *}) = 1\)</span></li>
<li><span class="math">\(q(\textrm{N | *, D}) = 1\)</span></li>
<li><span class="math">\(q(\textrm{STOP | D, N}) = 1\)</span></li>
<li><span class="math">\(q(s|u,v) = 0\)</span> for all other <span class="math">\(q\)</span> params.</li>
</ul>
<p>and emission parameters:</p>
<ul>
<li><span class="math">\(e(\textrm{the | D}) = 0.9\)</span></li>
<li><span class="math">\(e(\textrm{dog | D}) = 0.1\)</span></li>
<li><span class="math">\(e(\textrm{dog | N}) = 1\)</span></li>
</ul>
<p>Under this model how many pairs of sequences <span class="math">\(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_{n+1}\)</span> satisfy <span class="math">\(p(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_{n+1}) \gt 0\)</span>?</p>
<p>First: how many non-zero-probability tag sequences are there? Enumerate them by drawing a graph of nodes and edges, where a node is a word and an edge is labelled with the transition probability to another word. Then follow all paths from any start symbol to any stop symbol whose product of probabilities is <span class="math">\(\gt\)</span> 0.</p>
<pre><code>D, N, STOP</code></pre>
<p>There’s only one! OK. Refer back to your taq sequence graph and copy it for each possible word that a given tag (i.e. node) that it may “generate”. If e.g. N could generate two words, not one, we would have <em>four</em> possible sentences.</p>
<pre><code>the dog
dog dog</code></pre>
<p>There’s only two! OK. Hence the answer itself is two, because we have just generated a sentence for each possible (tag, word) pair.</p>
<hr />
<h4 id="an-example"><a href="#an-example">An example</a></h4>
<p>If we have:</p>
<ul>
<li><span class="math">\(n = 3\)</span>,</li>
<li>The sentence <span class="math">\(\{x_1, x_2, x_3\} = \{\textrm{the, dog, laughs}\}\)</span>, and</li>
<li>The tag sequence <span class="math">\(\{y_1, y_2, y_3, y_4\} = \{\textrm{D, N, V, STOP}\}\)</span>.</li>
</ul>
<p>Then:</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        &amp; p(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_{n+1}) \\
      = &amp; q(\textrm{D | *, *}) \times q(\textrm{N | *, D}) \times q(\textrm{V | D, N}) \times q(\textrm{STOP | N, V}) \times \\
        &amp; e(\textrm{the | D}) \times e(\textrm{dog | N}) \times e(\textrm{laughs | V})
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>STOP is a special tag that terminates the sequence.</li>
<li>We take <span class="math">\(y_0 = y_{-1} = \textrm{*}\)</span>, where <span class="math">\(\textrm{*}\)</span> is a special “padding” symbol.</li>
<li>The <span class="math">\(e\)</span> parameters can be interpreted as the conditional probability <span class="math">\(p(\textrm{the dog laughs | D N V STOP})\)</span>.</li>
</ul>
<hr />
<p>Quiz: given set <span class="math">\(S = \{\textrm{D, N, V}\}\)</span>, and vocabulary <span class="math">\(V = \{\textrm{the, cat, drinks, milk, dog}\}\)</span>, and an HMM model:</p>
<ul>
<li>transition parameters <span class="math">\(q(s|u,v) = \frac{1}{4}\;\forall\;s, u, v\)</span></li>
<li>generative parameters <span class="math">\(e(x|s) = \frac{1}{5}\;\forall\; \textrm{tags}\;s\;\textrm{and words}\;x\)</span>.</li>
</ul>
<p>What is the value, under this model, of:</p>
<p><span class="math">\[p(\textrm{the, cat, drinks, milk, D, N, V, N, STOP})\]</span></p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        &amp; p(\textrm{the, cat, drinks, milk, STOP, D, N, V, N}) \\
      = &amp; \prod_{i=1}^{n+1} q(y_i|y_{i-2},y_{i-1}) \prod_{i=1}^{n} e(x_i|y_i) \\
      = &amp; \{ p(\textrm{the | *, *}) \times p(\textrm{cat | *, the}) \times p(\textrm{drinks | the, cat}) \times p(\textrm{milk | cat, drinks}) \times p(\textrm{STOP | drinks, milk}) \} \times \\
        &amp; e(\textrm{the | D}) \times e(\textrm{cat | N}) \times e(\textrm{drinks | V}) \times e(\textrm{milk | N}) \\
      = &amp; \left(\frac{1}{4}\right)^5 \times \left(\frac{1}{5}\right)^4
    \end{aligned}
\end{align}
\]</span></p>
<hr />
<h4 id="why-the-name"><a href="#why-the-name">Why the Name?</a></h4>
<ul>
<li>The first product is a <strong>second-order Markov Chain</strong>
<ul>
<li>Recall <span class="math">\(p(x,y) = p(y) \times p(x|y)\)</span></li>
<li>This product is solving for <span class="math">\(p(y)\)</span>.</li>
</ul></li>
<li>The second project is <span class="math">\(x_j\)</span>’s <strong>being observed</strong>.
<ul>
<li>Strong independence assumption that each word depends only on its underlying, generating tag.</li>
</ul></li>
<li>The generative process: we choose a sequence of tags, and then for each tag generate an associated word.
<ul>
<li>The <span class="math">\(y\)</span>’s are <em>not observed</em>.</li>
<li>The <span class="math">\(x\)</span>’s are <em>observed</em>.</li>
</ul></li>
<li>And so we will flip this: given an observation find the most likely underlying (<strong>hidden</strong>) tag sequence.</li>
</ul>
<hr />
<p>Quiz: for a bigram HMM:</p>
<p><span class="math">\[p(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_n) = \prod_{i=1}^{n+1} q(y_i|y_{i-1}) \prod_{i=1}^{n} e(x_i|y_i)\]</span></p>
<hr />
<h3 id="parameter-estimation-in-hmms"><a href="#parameter-estimation-in-hmms">Parameter Estimation in HMMs</a></h3>
<h4 id="smoothed-estimation"><a href="#smoothed-estimation">Smoothed Estimation</a></h4>
<p>e.g.</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        q(\textrm{Vt | DT, JJ}) &amp; = \lambda_1 \times \frac{\textrm{Count(Dt, JJ, Vt)}}{\textrm{Count(Dt, JJ)}} \\
                                &amp; + \lambda_2 \times \frac{\textrm{Count(JJ, Vt)}}{\textrm{Count(JJ)}} \\
                                &amp; + \lambda_3 \times \frac{\textrm{Count(Vt)}}{\textrm{Count()}}
    \end{aligned}
\end{align}
\]</span></p>
<p><span class="math">\[\lambda_1 + \lambda_2 + \lambda_3 = 1\]</span> <span class="math">\[\forall\;i, \lambda_i \ge 0\]</span></p>
<p><span class="math">\[e(\textrm{base | Vt}) = \frac{\textrm{Count(Vt, base)}}{\textrm{Count(Vt)}}\]</span></p>
<ul>
<li>For trigram / transition parameters:
<ul>
<li>We can of course induce counts of tag sequences directly from our corpus, and then determine <strong>maximum-likelihood estimates</strong>.
<ul>
<li><span class="math">\(\lambda_1\)</span> for <strong>trigram MLE</strong>.</li>
<li><span class="math">\(\lambda_2\)</span> for <strong>bigram MLE</strong>.</li>
<li><span class="math">\(\lambda_3\)</span> for <strong>unigram MLE</strong>.</li>
</ul></li>
<li>Linear interpolation is used, as seen before.</li>
</ul></li>
<li>For emission parameters:
<ul>
<li>Can use <strong>bigram MLEs</strong>.</li>
</ul></li>
<li>One problem.</li>
<li><span class="math">\(e(x|y) = 0\;\forall\;y\)</span> if <span class="math">\(x\)</span> is never seen in the training data.
<ul>
<li>!!AI sounds familiar! Will we do Laplacian smoothing, we we “add fudge” to everything, or back-off smoothing, where high mass gets re-distributed to zero mass, or something else?</li>
</ul></li>
</ul>
<hr />
<p>Quiz: Given the following corpus:</p>
<ul>
<li>the dog barks -&gt; D N V STOP</li>
<li>the cat sings -&gt; D N V STOP</li>
</ul>
<p>Assume we’ve calculated MLEs of a trigram HMM from this data. What is the value of the emission parameter <span class="math">\(e(\textrm{cat | N})\)</span> from this HMM?</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        e(\textrm{cat | N}) = &amp; \frac{\textrm{Count(N, cat)}}{\textrm{Count(N)}} \\
                            = &amp; \frac{(1)}{(2)}
    \end{aligned}
\end{align}
\]</span></p>
<p>Say we estimate the transition parameters for a trigram HMM using linear interpolation, such that <span class="math">\(\lambda_i = \frac{1}{3}\)</span> for <span class="math">\(i = \{1, 2, 3\}\)</span>. What is the value of the transition parameter <span class="math">\(q(\textrm{STOP | N, V})\)</span> under this model?</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        q(\textrm{STOP | N, V}) = &amp; \lambda_1 \times \frac{\textrm{Count(N, V, STOP)}}{\textrm{Count(N, V)}} \\
                                + &amp; \lambda_2 \times \frac{\textrm{Count(V, STOP)}}{\textrm{Count(V)}} \\
                                + &amp; \lambda_3 \times \frac{\textrm{Count(STOP)}}{\textrm{Count()}} \\
                                = &amp; \left(\frac{1}{3} \times \frac{(2)}{(2)}\right) \\
                                + &amp; \left(\frac{1}{3} \times \frac{(2)}{(2)}\right) \\
                                + &amp; \left(\frac{1}{3} \times \frac{(2)}{(8)}\right) \\
                                = &amp; 0.75
    \end{aligned}
\end{align}
\]</span></p>
<hr />
<h4 id="dealing-with-low-frequency-words-an-example"><a href="#dealing-with-low-frequency-words-an-example">Dealing with Low-Frequency Words: An Example</a></h4>
<ul>
<li>Test sentence</li>
</ul>
<pre><code>Profits soared at Boeing Co., easily topping ...
CEO Alan Mulally.</code></pre>
<ul>
<li><code>topping</code> and <code>Mulally</code> are likely to be infrequent.</li>
<li>Long tail: you will frequently encounter words in test data that you have never encountered in training data.</li>
<li>And hence: <span class="math">\(e(\textrm{Mulally | y}) = 0\)</span> for all tags <span class="math">\(y\)</span>.</li>
<li>And it can be verified that the joint probability <span class="math">\(p(x_1, \ldots, x_n, y_1, \ldots, y_{n+1}) = 0\)</span> for all tag sequences <span class="math">\(y_1, \ldots, y_{n+1}\)</span>.</li>
<li>This is because all tag sequences will involve this emission parameter.</li>
<li><p>And hence all tag sequences are equally likely; applying argmax to an expression that <em>always</em> evaluates to zero implies that <span class="math">\(y\)</span> is equally maximum everywhere!</p></li>
<li>A common way of dealing with this:
<ol style="list-style-type: decimal">
<li><strong>Split the vocabulary into two sets</strong>.
<ul>
<li><em>Frequent words</em>: words occurring <span class="math">\(\ge\)</span> 5 times in training (or some threshold).</li>
<li><em>Low frequency words</em>: all other words.</li>
</ul></li>
<li><strong>Map</strong> low frequency words into a small, finite set, depending on affixes.</li>
</ol></li>
<li>The set of low frequency words is very large.</li>
<li><p>Map each low frequency word to a small set of e.g. 20 new words.</p></li>
<li><p>from [Bikel et. al 1999] for named-entity recognition.</p></li>
</ul>
<table>
<thead>
<tr class="header">
<th align="left">Word class</th>
<th align="left">Example</th>
<th align="left">Intuition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">twoDigitNum</td>
<td align="left">90</td>
<td align="left">Two digit year</td>
</tr>
<tr class="even">
<td align="left">fourDigitNum</td>
<td align="left">1990</td>
<td align="left">Four digit year</td>
</tr>
<tr class="odd">
<td align="left">containsDigitAndAlpha</td>
<td align="left">A8956-67</td>
<td align="left">Product code</td>
</tr>
<tr class="even">
<td align="left">containsDigitAndDash</td>
<td align="left">09-96</td>
<td align="left">Date</td>
</tr>
<tr class="odd">
<td align="left">containsDigitAndSlash</td>
<td align="left">11/9/89</td>
<td align="left">Date</td>
</tr>
<tr class="even">
<td align="left">containsDigitAndComma</td>
<td align="left">23,000.00</td>
<td align="left">Monetary amount</td>
</tr>
<tr class="odd">
<td align="left">containsDigitAndPeriod</td>
<td align="left">1.00</td>
<td align="left">Monetary, financial</td>
</tr>
<tr class="even">
<td align="left">othernum</td>
<td align="left">456789</td>
<td align="left">Other</td>
</tr>
<tr class="odd">
<td align="left">allCaps</td>
<td align="left">BBN</td>
<td align="left">Organization</td>
</tr>
<tr class="even">
<td align="left">capsPeriod</td>
<td align="left">M.</td>
<td align="left">Initial</td>
</tr>
<tr class="odd">
<td align="left">firstWord</td>
<td align="left">first</td>
<td align="left">no useful capitalisation infomation</td>
</tr>
<tr class="even">
<td align="left">initCap</td>
<td align="left">Sally</td>
<td align="left">Capitalized word</td>
</tr>
<tr class="odd">
<td align="left">lowercase</td>
<td align="left">can</td>
<td align="left">Uncapitalized word</td>
</tr>
<tr class="even">
<td align="left">other</td>
<td align="left">,</td>
<td align="left">Punctuation, other words</td>
</tr>
</tbody>
</table>
<ul>
<li>These were chosen by hand with intuition.</li>
<li>We want to preserve some useful information for the specific task at hand, i.e. named entity recognition.</li>
<li>e.g. <code>firstWord</code> will be capitalized in the corpus, but we lowercase it because the capitalization does not give us useful information, because all words at the start of a sentence are capitalized.</li>
<li>We’re mapping low-frequency words to classes that preserve spelling features.</li>
</ul>
<p>Return to an old example. Before transformation:</p>
<pre><code>Profits/NA soared/NA at/NA Boeing/SC Co./CC easily/NA
topping/NA forecasts/NA on/NA Wall/SL Street/CL ,/NA their/NA
CEO/NA Alan/SP Mulally/CP announced/NA first/NA quarter/NA
results/NA ./NA</code></pre>
<p>After transformation:</p>
<pre><code>firstword/NA soared/NA at/NA initCap/SC Co./CC ,/NA easily/NA
lowercase/NA forecasts/NA on/NA initCap/SL Street/CL ,/NA as/NA
their/NA CEO/NA Alan/SP initCap/CP announced/NA first/NA
quarter/NA results/NA ./NA</code></pre>
<ul>
<li>Resolving low-frequency words in a way that preserves their spelling is useful for the named-entity recognition problem.</li>
<li>Build our HMM on this transformed data.
<ul>
<li><span class="math">\(e(\textrm{firstword | NA})\)</span></li>
<li><span class="math">\(e(\textrm{initCap | SC})\)</span></li>
</ul></li>
<li>We’re <strong>closing</strong> the vocabulary.</li>
<li>This is a simple method, but requires human heuristics.</li>
</ul>
<h3 id="the-viterbi-algorithm-for-hmms"><a href="#the-viterbi-algorithm-for-hmms">The Viterbi Algorithm for HMMs</a></h3>
<ul>
<li>How to apply HMMs to new test sentences?</li>
</ul>
<h4 id="problem"><a href="#problem">Problem</a></h4>
<ul>
<li>For a <em>new</em> test input sentence <span class="math">\(x_1, \ldots, x_n\)</span>, map it onto the most likely set of tags, i.e. find:</li>
</ul>
<p><span class="math">\[\textrm{arg}\underset{y_1 \dots y_{n+1}}{\textrm{max}} p(x_1 \ldots x_n, y_1 \ldots y_{n+1})\]</span></p>
<ul>
<li><p>where the arg max is taken over all sequences <span class="math">\(y_1 \ldots y_{n+1}\)</span> such that <span class="math">\(y_i \in S\)</span> for <span class="math">\(i = 1, \ldots, n\)</span> and <span class="math">\(y_{n+1} = \textrm{STOP}\)</span>.</p></li>
<li><p>We assume that <span class="math">\(p\)</span> again takes the form:</p></li>
</ul>
<p><span class="math">\[p(x_1 \ldots x_n, y_1 \ldots y_{n+1}) = \prod_{i=1}^{n+1} q(y_i | y_{i-2}, y_{i-1}) \prod_{i=1}^{n} e(x_i | y_i)\]</span></p>
<ul>
<li><p>Recall the assumptions that <span class="math">\(y_0 = y_{-1} = \textrm{*}\)</span> and <span class="math">\(y_{n+1} = \textrm{STOP}\)</span>.</p></li>
<li>!!AI from a practical perspective the product of many small numbers will rapidly become unrepresentable on a machine.</li>
<li>Logarithms come up again and again in machine learning because it is a <strong>monotonic increasing</strong> function with some <strong>very useful rules</strong>.
<ul>
<li><a href="http://en.wikipedia.org/wiki/Monotonic_function">http://en.wikipedia.org/wiki/Monotonic_function</a></li>
<li>A monotically increasing function <span class="math">\(f(x)\)</span> is such that for all <span class="math">\(x, y\)</span> such that <span class="math">\(x \le y\)</span> the following is always true: <span class="math">\(f(x) \le f(y)\)</span>.</li>
<li>Colloquially, <span class="math">\(f\)</span> <em>preserves order</em>.</li>
</ul></li>
<li>Also recall that <span class="math">\(\textrm{log}(a \times b) = \textrm{log}(a) + \textrm{log}(b)\)</span>.</li>
<li>Hence if we apply logarithms to everything in <span class="math">\(p\)</span> we can not only <strong>add instead of multiply</strong> but also <strong>retain</strong> the ability to calculate argmax over the y’s, i.e. determine the most likely tag sequence.</li>
<li><p>To clarify what I mean by <strong>retain</strong>:</p></li>
</ul>
<p><span class="math">\[\textrm{arg}\underset{y_1 \dots y_{n+1}}{\textrm{max}} p(x_1 \ldots x_n, y_1 \ldots y_{n+1}) = \textrm{arg}\underset{y_1 \dots y_{n+1}}{\textrm{max}} \textrm{log} \left\{ p(x_1 \ldots x_n, y_1 \ldots y_{n+1}) \right\}\]</span></p>
<ul>
<li>And to clarify what I mean by <strong>add instead of multiply</strong>:</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        p(x_1 \ldots x_n, y_1 \ldots y_{n+1}) &amp; = \prod_{i=1}^{n+1} q(y_i | y_{i-2}, y_{i-1}) \prod_{i=1}^{n} e(x_i | y_i) \\
        \textrm{log} \left\{ p(x_1 \ldots x_n, y_1 \ldots y_{n+1}) \right\} &amp; = \textrm{log} \left\{ \prod_{i=1}^{n+1} q(y_i | y_{i-2}, y_{i-1}) \prod_{i=1}^{n} e(x_i | y_i) \right\} \\
        &amp; = \textrm{log} \left\{ \prod_{i=1}^{n+1} q(y_i | y_{i-2}, y_{i-1}) \right\} + \textrm{log} \left\{ \prod_{i=1}^{n} e(x_i | y_i) \right\}
    \end{aligned}
\end{align}
\]</span></p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        &amp; = \textrm{log} \left\{ q(y_1|y_{-1},y_0) \times q(y_2|y_0,y_1) \times \ldots \times q(y_{n+1}|y_{n-1},y_{n}) \right\} \\
        &amp; + \textrm{log} \left\{ e(x_0|y_0) \times e(x_1|y_1) \times \ldots \times e(x_n|y_n) \right\} \\
        &amp; = \textrm{log} \left\{ q(y_1|y_{-1},y_0) \right\} + \textrm{log} \left\{ q(y_2|y_0,y_1) \right\} + \ldots + \textrm{log} \left\{ q(y_{n+1}|y_{n-1},y_{n}) \right\} \\
        &amp; + \textrm{log} \left\{ e(x_0|y_0) \right\} + \textrm{log} \left\{ e(x_1|y_1) \right\} + \ldots + \textrm{log} \left\{ e(x_n|y_n) \right\}
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>To be absolutely clear: it is irrelevant what base you use in the logarithm.
<ul>
<li>In this course I think we’re expected to use base 2.</li>
<li>In the ARPA file format of backoff language models base 10 is used.</li>
<li>But use whatever you want! <strong>Just don’t forget</strong> which one you used ;).</li>
</ul></li>
</ul>
<h4 id="brute-force-search-is-hopelessly-inefficient"><a href="#brute-force-search-is-hopelessly-inefficient">Brute Force Search is Hopelessly Inefficient</a></h4>
<ul>
<li>For example
<ul>
<li><span class="math">\(x_1 \ldots x_n = \{\textrm{the, dog, laughs}\}\)</span>.</li>
<li><span class="math">\(y_1 \ldots y_n = \{\textrm{D, N, V}\}\)</span> (the correct answer).</li>
<li><span class="math">\(S = \{\textrm{D, N, V}\}\)</span> (assume that the set of all possible tags is just this).</li>
</ul></li>
<li>So <span class="math">\(|S| = 3\)</span>, and all possible tag sequences are all combinations (<em>not</em> permutations):
<ul>
<li>D D D STOP</li>
<li>D D N STOP</li>
<li>D D U STOP</li>
<li>D U D STOP</li>
<li>…</li>
</ul></li>
<li>Only <span class="math">\(3^3 = 27\)</span> possible tag sequences.</li>
<li>Use the transmission and emissions parameters of the HMM model to assign probabilities to each particular tag sequnce, then choose the most likely tag sequence.</li>
<li><p>However, in the general case <span class="math">\(|S|^n\)</span>, where <span class="math">\(n\)</span> is sentence length, is the number of possible sequences.</p></li>
<li>The transmission parameters only depend on sequences of length three for trigram HMMs.
<ul>
<li>This structure allows a more efficient solution.</li>
</ul></li>
</ul>
<h3 id="the-viterbi-algorithm"><a href="#the-viterbi-algorithm">The Viterbi Algorithm</a></h3>
<ul>
<li>Define <span class="math">\(n\)</span> to be length of sentence.</li>
<li>Define <span class="math">\(S_k\)</span> for <span class="math">\(k = -1, 0, \ldots, n\)</span>, to be set of possible tags at position <span class="math">\(k\)</span>:</li>
</ul>
<p><span class="math">\[S_{-1} = S_0 = \{\textrm{*}\}\]</span> <span class="math">\[S_k = S\;\textrm{for}\;k \in \{1, 2, \ldots n\}\]</span></p>
<ul>
<li>Define:</li>
</ul>
<p><span class="math">\[r(y_{-1}, y_0, y_1, \ldots, y_k) = \prod_{i=1}^{k} q(y_i | y_{i-2}, y_{i-1}) \prod_{i=1}^{k} e(x_i|y_i)\]</span></p>
<ul>
<li>Note that, always, <span class="math">\(y_{-1} = y_0 = \{\textrm{*}\}\)</span>.</li>
<li>This is a truncated <span class="math">\(q\)</span>, as it only goes <span class="math">\(i=1\)</span> to <span class="math">\(k\)</span>.</li>
<li>Define a dynamic programming table</li>
</ul>
<p><span class="math">\[\pi(k,u,v) = \textrm{maximum probability of a tag sequence ending in tags}\;u, v\;\textrm{at position}\;k\]</span></p>
<p>i.e.</p>
<p><span class="math">\[\pi(k,u,v) = max_{(y_{-1},y_0,y_{1},\ldots,y_k):y_{k-1}=u,\;y_k=v} r(y_{-1},y_0,y_1,\ldots,y_k)\]</span></p>
<ul>
<li><span class="math">\(k\)</span> takes any value <span class="math">\(\{\textrm{1,2,...,n}\}\)</span>.</li>
<li><span class="math">\(u \in S_{k-1}\)</span>.</li>
<li><p><span class="math">\(v \in S_k\)</span>.</p></li>
<li>What do the <span class="math">\(S\)</span> and <span class="math">\(k\)</span> expressions at the begining imply:
<ul>
<li>For example, (the, dog, laughs, D, N, V) implies <span class="math">\(k = 3\)</span>.</li>
<li>Each tag in <span class="math">\(S\)</span> could be responsible for generating a word in <span class="math">\(x\)</span>.
<ul>
<li>If <span class="math">\(S = \{\textrm{D, N, V, P}\}\)</span>, then <span class="math">\(x_1\)</span> could be one of D, N, V, P, as is <span class="math">\(x_2\)</span>, etc.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="an-example-1"><a href="#an-example-1">An Example</a></h4>
<p><span class="math">\[\underset{-1}{\textrm{*}}\;\underset{0}{\textrm{*}}\;\underset{1}{\textrm{The}}\;\underset{2}{\textrm{man}}\;\underset{3}{\textrm{saw}}\;\underset{4}{\textrm{the}}\;\underset{5}{\textrm{dog}}\;\underset{6}{\textrm{with}}\;\underset{7}{\textrm{the}}\;\underset{8}{\textrm{telescope}}\;\]</span></p>
<ul>
<li>Assume <span class="math">\(S = \{\textrm{D, N, V, P}\}\)</span></li>
<li>What does <span class="math">\(\pi(7, \textrm{P}, \textrm{D})\)</span> mean, intuitively?
<ul>
<li>The probability of the most likely tag sequence ending at the word in position 7 such that the last two tags are (P, D).</li>
<li>Fix ‘with’ (6) to P.</li>
<li>Fix ‘the’ (7) with D.</li>
<li>Each preceding word has four possible tags.
<ul>
<li>‘dog’ (5) could be D, N, V, P.</li>
<li>‘the’ (4) could be D, N, V, P.</li>
<li>‘saw’ (3) could be D, N, V, P.</li>
<li>‘man’ (2) could be D, N, V, P.</li>
<li>‘The’ (1) could be D, N, V, P.</li>
</ul></li>
</ul></li>
</ul>
<hr />
<p>Quiz: We have a trigram HMM model with the following transition parameters:</p>
<ul>
<li><span class="math">\(q(\textrm{D | *, *}) = 1\)</span></li>
<li><span class="math">\(q(\textrm{N | *, D}) = 1\)</span></li>
<li><span class="math">\(q(\textrm{V | D, N}) = 1\)</span></li>
<li><span class="math">\(q(\textrm{STOP | N, V}) = 1\)</span></li>
</ul>
<p>and emission parameters:</p>
<ul>
<li><span class="math">\(e(\textrm{the | D}) = 0.8\)</span></li>
<li><span class="math">\(e(\textrm{dog | D}) = 0.2\)</span></li>
<li><span class="math">\(e(\textrm{dog | N}) = 0.8\)</span></li>
<li><span class="math">\(e(\textrm{the | N}) = 0.2\)</span></li>
<li><span class="math">\(e(\textrm{barks | V}) = 1.0\)</span></li>
</ul>
<p>Say we have the sentence:</p>
<pre><code>the dog barks</code></pre>
<p>What is the value of <span class="math">\(\pi(3, \textrm{N}, \textrm{V})\)</span>?</p>
<ul>
<li>Intuitively, this reads as ‘what is the probability of the most likely tag sequence that ends at position 3 such that the last two tags are N and V?’</li>
<li>First, expand and label your test sentence, omitting the STOP symbol:</li>
</ul>
<p><span class="math">\[\underset{-1}{\textrm{*}}\;\underset{0}{\textrm{*}}\;\underset{1}{\textrm{the}}\;\underset{2}{\textrm{dog}}\;\underset{3}{\textrm{barks}}\]</span></p>
<ul>
<li>Draw a Markov Chain graph of your transmission parameters, covering every single possible path.
<ul>
<li>Think of every tag as a node (including the start symbols), and an edge as moving from one tag to another with a certain probability.</li>
<li>In our case this is very easy; there is only one path, i.e. <span class="math">\(\textrm{* -&gt; * -&gt; D -&gt; N -&gt; V -&gt; STOP}\)</span>, with probabilities of <span class="math">\(1\)</span> for each edge.</li>
</ul></li>
<li>Eliminate all paths from the Markov Chain graph that do not meet the constraints of <span class="math">\(\pi(3,\textrm{N},\textrm{V})\)</span>. Also eliminate any paths that contain an edge with zero probability.</li>
<li>For us, we only have one path, and this path meets the contraints of this function.</li>
<li><p>Prove this to yourself by putting one finger on the start of the test sentence, and one finger on the start of the Markon Chain graph, and counting until <span class="math">\(k=3\)</span>.</p></li>
<li>Your Markov Chain graph now covers every possible combination of tags that <em>could</em> match this test sentence. For each path calculate the product of probabilities from a start symbol to <span class="math">\(k=3\)</span>. Determine which path gives you the highest probability.</li>
<li>In our case there is only one path, so the <strong>most likely tag sequence</strong> is (D, N, V).</li>
<li>This gives us the <span class="math">\(q\)</span> part of the <span class="math">\(r\)</span> expression.</li>
<li>For this tag sequence use the emission parameters to “generate” the appropriate word in order to calculate the <span class="math">\(e\)</span> parameters.</li>
<li><p>Mathematically:</p></li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        r(y_{-1},y_0,y_1,\ldots,y_n) &amp; = \prod_{i=1}^{k} q(y_i|y_{i-2},y_{i-1}) \prod_{i=1}^{k} e(x_i|y_i) \\
        r(\textrm{*, *, D, N, V}) &amp; = \left\{ q(\textrm{D | *, *}) \times q(\textrm{N | *, D}) \times q(\textrm{V | D, N}) \right\} \times \\
        &amp; \left\{ e(\textrm{the | D}) \times e(\textrm{dog | N}) \times e(\textrm{barks | V}) \right\} \\
        &amp; = \left\{ 1 \times 1 \times 1\right\} \times \left\{0.8 \times 0.8 \times 1.0 \right\} \\
        &amp; = 0.64
    \end{aligned}
\end{align}
\]</span></p>
<hr />
<h4 id="a-recursive-definition"><a href="#a-recursive-definition">A Recursive Definition</a></h4>
<ul>
<li>Base case: <span class="math">\(\pi(0, \textrm{*}, \textrm{*}) = 1\)</span>
<ul>
<li>Every tag sequence starts with <span class="math">\(\textrm{* *}\)</span>.</li>
</ul></li>
<li><strong>Recursive definition</strong>: <span class="math">\(\forall\; k \in \{1 \ldots n\},\;\forall\; u \in S_{k-1}\;\textrm{and}\;v \in S_k:\)</span></li>
</ul>
<p><span class="math">\[\pi(k,u,v) = \underset{w \in S_{k-2}}{\textrm{max}} (\pi(k-1),w,u) \times q(v|w,u) \times e(x_k|v))\]</span></p>
<ul>
<li><span class="math">\(u\)</span> can take any tag in <span class="math">\(S_{k-1}\)</span>, <span class="math">\(v\)</span> can take any tag in <span class="math">\(S_k\)</span>.</li>
<li>Notice how we’re working backwards in the sentence back to the base case, the start.</li>
</ul>
<h4 id="justification-for-the-recursive-definition"><a href="#justification-for-the-recursive-definition">Justification for the Recursive Definition</a></h4>
<p>(part 2)</p>
<p><span class="math">\[\underset{-1}{\textrm{*}}\;\underset{0}{\textrm{*}}\;\underset{1}{\textrm{The}}\;\underset{2}{\textrm{man}}\;\underset{3}{\textrm{saw}}\;\underset{4}{\textrm{the}}\;\underset{5}{\textrm{dog}}\;\underset{6}{\textrm{with}}\;\underset{7}{\textrm{the}}\;\underset{8}{\textrm{telescope}}\;\]</span></p>
<p>What is <span class="math">\(\pi(7, P, D)\)</span>?</p>
<ul>
<li>Recall this puts ‘with’ (6) = P, ‘the’ (7) = D.</li>
<li><span class="math">\(u = \textrm{P}, v = \textrm{D}\)</span></li>
<li>Note that <span class="math">\(S_5 = S_4 = \ldots = S = \{\textrm{D, N, V, P}\}\)</span>.</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        \pi(7, \textrm{P}, \textrm{D}) = &amp; \underset{w \in \{\textrm{D,N,V,P}\}}{\textrm{max}} \left\{ \pi(6, w, \textrm{P}) \times q(\textrm{D} | w, \textrm{P}) \times e(\textrm{the} | \textrm{D}) \right\}
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>Any tag sequence ending in (P, D) must have included one previous tag in (D, N, V, P). The ‘max’ explicitly searches over these.</li>
</ul>
<hr />
<p>Quiz: assume <span class="math">\(S = \{\textrm{D, N, V, P}\}\)</span> and a trigram HMM with parameters:</p>
<ul>
<li><span class="math">\(q(\textrm{D | N, P}) = 0.4\)</span></li>
<li><span class="math">\(q(\textrm{D | w, P}) = 0\)</span> for <span class="math">\(w \neq N\)</span>.</li>
<li><span class="math">\(e(\textrm{the | D}) = 0.6\)</span></li>
</ul>
<p>We are also given the sentence:</p>
<pre><code>Ella walks to the red house</code></pre>
<p>Say the dynamic programming table for this sentence has the following entries:</p>
<ul>
<li><span class="math">\(\pi(\textrm{3, D, P}) = 0.1\)</span></li>
<li><span class="math">\(\pi(\textrm{3, N, P}) = 0.2\)</span></li>
<li><span class="math">\(\pi(\textrm{3, V, P}) = 0.01\)</span></li>
<li><span class="math">\(\pi(\textrm{3, P, P}) = 0.5\)</span></li>
</ul>
<p>What is the value of <span class="math">\(\pi(\textrm{4, P, D})\)</span>?</p>
<ul>
<li><span class="math">\(u = \textrm{P}\)</span></li>
<li><span class="math">\(v = \textrm{D}\)</span></li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        \pi(k,u,v) = &amp; \underset{w \in S_{k-2}}{\textrm{max}} (\pi(k-1),w,u) \times q(v|w,u) \times e(x_k|v)) \\
        \pi(4, \textrm{P}, \textrm{D}) = &amp; \underset{w \in \{\textrm{D, N, V, P}\}}{\textrm{max}} \left\{ \pi(3, w, \textrm{P}) \times q(\textrm{D} | w, \textrm{P}) \times e(\textrm{the | D}) \right\} \\
        = &amp; \textrm{max} \left\{ 0.1 \times 0 \times 0.6, 0.2 \times 0.4 \times 0.6, 0.01 \times 0 \times 0.6, 0.5 \times 0 \times 0.6 \right\} \\
        = &amp; 0.048
    \end{aligned}
\end{align}
\]</span></p>
<hr />
<h3 id="the-viterbi-algorithm-1"><a href="#the-viterbi-algorithm-1">The Viterbi Algorithm</a></h3>
<ul>
<li><strong>Inputs</strong>:
<ul>
<li>a sentence <span class="math">\(x_1 \ldots x_n\)</span>, a sequence of words</li>
<li>transmisson parameters <span class="math">\(q(s|u,v)\)</span>,</li>
<li>emission parameters <span class="math">\(e(x|s)\)</span>.</li>
</ul></li>
<li><strong>Output</strong>:
<ul>
<li><span class="math">\(\underset{y_1 \ldots y_{n+1}}{\textrm{max}} p(x_1 \ldots x_n, y_1 \ldots y_{n+1})\)</span></li>
<li>Notice this is <em>not argmax</em>; just returns max probability. A simple change later will fix this.</li>
</ul></li>
<li><strong>Initializtion</strong>:
<ul>
<li>Set <span class="math">\(\pi(0,\textrm{*},\textrm{*}) = 1\)</span>.
<ul>
<li>Base case of the recursion.</li>
</ul></li>
</ul></li>
<li><strong>Definition</strong>:
<ul>
<li><span class="math">\(S_{-1} = S_0 = \{\textrm{*}\}\)</span>
<ul>
<li>Can only have the star symbols at positions -1 and 0.</li>
</ul></li>
<li><span class="math">\(S_k = S\;\forall\;k \in \{1 \ldots n\}\)</span>
<ul>
<li>Recall e.g. {D, N, V, P}</li>
</ul></li>
</ul></li>
<li><strong>Algorithm</strong>
<ul>
<li>For <span class="math">\(k = 1 \ldots n\)</span>:
<ul>
<li>For <span class="math">\(u \in S_{k-1}\)</span>, <span class="math">\(v \in S_k\)</span>:
<ul>
<li><span class="math">\(\pi(k,u,v) = \underset{w \in S_{k-2}}{\textrm{max}} (\pi(k-1,w,u) \times q(v|w,u) \times e(x_k|v))\)</span></li>
</ul></li>
</ul></li>
<li><strong>Return</strong>: <span class="math">\(\textrm{max}_{u \in S_{n-1},v \in S_n} (\pi(n,u,v) \times q(\textrm{STOP}|u,v))\)</span></li>
</ul></li>
</ul>
<h4 id="the-viterbi-algorithm-with-backpointers"><a href="#the-viterbi-algorithm-with-backpointers">The Viterbi Algorithm with Backpointers</a></h4>
<p>We want ‘argmax’, not ‘max’, i.e. the actual most-likely tag sequence.</p>
<ul>
<li><strong>Inputs</strong>:
<ul>
<li>a sentence <span class="math">\(x_1 \ldots x_n\)</span>, a sequence of words</li>
<li>transmisson parameters <span class="math">\(q(s|u,v)\)</span>,</li>
<li>emission parameters <span class="math">\(e(x|s)\)</span>.</li>
</ul></li>
<li><strong>Output</strong>:
<ul>
<li><span class="math">\(\textrm{arg}\underset{y_1 \ldots y_{n+1}}{\textrm{max}} p(x_1 \ldots x_n, y_1 \ldots y_{n+1})\)</span></li>
</ul></li>
<li><strong>Initialization</strong>:
<ul>
<li>Set <span class="math">\(\pi(0,\textrm{*},\textrm{*}) = 1\)</span>.</li>
</ul></li>
<li><strong>Definition</strong>:
<ul>
<li><span class="math">\(S_{-1} = S_0 = \{\textrm{*}\}\)</span></li>
<li><span class="math">\(S_k = S\;\forall\;k \in \{1 \ldots n\}\)</span><br /></li>
</ul></li>
<li><strong>Algorithm</strong>
<ul>
<li>For <span class="math">\(k = 1 \ldots n\)</span>:
<ul>
<li>For <span class="math">\(u \in S_{k-1}\)</span>, <span class="math">\(v \in S_k\)</span>:
<ul>
<li><span class="math">\(\pi(k,u,v) = \underset{w \in S_{k-2}}{\textrm{max}} (\pi(k-1,w,u) \times q(v|w,u) \times e(x_k|v))\)</span></li>
<li><span class="math">\(bp(k,u,v) = arg \underset{w \in S_{k-2}}{max} (\pi(k-1,w,u) \times q(v|w,u) \times e(x_k|v))\)</span></li>
</ul></li>
</ul></li>
<li>Set <span class="math">\((y_{n-1},y_n) = \textrm{argmax}_{(u,v)} (\pi(n,u,v) \times q(\textrm{STOP}|u,v))\)</span></li>
<li>For <span class="math">\(k = (n-2) \ldots 1\)</span>, <span class="math">\(y_k = bp(k+1, y_{k+1}, y_{k+2})\)</span></li>
<li><strong>Return</strong> the tag sequence <span class="math">\(y_1 \ldots y_n\)</span>.</li>
</ul></li>
<li>What is different?
<ul>
<li>Don’t just record <span class="math">\(\pi\)</span> at each point but also a backpointer <span class="math">\(bp\)</span>; which tag achieved this max. Which tag is most likely at <span class="math">\(k\)</span> given <span class="math">\(u,v\)</span>.</li>
<li>We then have <span class="math">\(\pi\)</span> and <span class="math">\(bp\)</span> values.</li>
<li>At the end we go backwards in the sequence.</li>
</ul></li>
<li>Run-time complexity is <span class="math">\(O(n \times |S|^3)\)</span>.
<ul>
<li>We enter the <span class="math">\(u,v\)</span> loop <span class="math">\(n \times |S|^2\)</span> times.</li>
<li>Each time we enter we need to search over <span class="math">\(|S|\)</span> possible tags.</li>
<li>It is <strong>linear</strong> with respect to sentence length.</li>
<li>Much better than brute force, which was <span class="math">\(O(|S|^n)\)</span>.</li>
</ul></li>
</ul>
<h3 id="summary-1"><a href="#summary-1">Summary</a></h3>
<ul>
<li>HMM taggers are <strong>very simple to train</strong>.
<ul>
<li>Just compile counts from training corpus, calculate MLEs.</li>
</ul></li>
<li>Perform relatively well, over 90% on named entity recognition.</li>
<li>Main difficulty is modelling: e(word|tag), especially if words are low-frequency.
<ul>
<li>One approach is to group low-frequency words into classes, but very clumsy and heuristic.</li>
<li>When words are complex even worse.</li>
<li>Later in the course we develop more complex methods.</li>
</ul></li>
</ul>
<h2 id="week-3---parsing-and-context-free-grammars"><a href="#week-3---parsing-and-context-free-grammars">Week 3 - Parsing, and Context-Free Grammars</a></h2>
<h3 id="introduction"><a href="#introduction"> Introduction</a></h3>
<p>-</p>
<h2 id="readings"><a href="#readings">Readings</a></h2>
<h3 id="speech-and-language-processing-chapter-3-words-and-transducers"><a href="#speech-and-language-processing-chapter-3-words-and-transducers">Speech and Language Processing, Chapter 3 (Words and Transducers)</a></h3>
<h4 id="word-and-sentence-tokenization"><a href="#word-and-sentence-tokenization">3.9: Word and Sentence Tokenization</a></h4>
<ul>
<li>p75: <strong>Tokenization</strong>: segmenting running text into words and sentences.</li>
<li><p>Consider:</p>
<pre><code>Mr.  Sherwood said reaction to Sea
Containers&#39; proposal has been &quot;very
positive.&quot; In New York Stock Exchange
composite tradying yesterday, Sea Containers
closed at $62.625, up 62.5 cents.</code></pre></li>
<li>Notice that:
<ul>
<li>There could be double-spaces, which are just typos and can be considered a word delimeter.</li>
<li>With quotation marks the end of sentence period is <em>within</em> the quotation marks. The word <em>is not</em> <code>positive.&quot;</code>.</li>
<li>There may be numbers in a sentence.</li>
</ul></li>
<li>You might be tempted to treat punctuation as a word boundary.
<ul>
<li>But what about <code>m.p.h.</code>, <code>Ph.D</code>, <code>AT&amp;T</code>, <code>cap'n</code>, <code>01/02/06</code>, <code>google.com</code>.</li>
</ul></li>
<li>Also want to expand clitic contractions.
<ul>
<li><code>what're</code> becomes <code>what are</code>.</li>
<li>But apostrophes aren’t always clitic contractions, e.g. <code>her books' covers</code>.</li>
<li>Segmenting and expanding clitics can be done using <strong>morpological parsing</strong> presented in this chapter.</li>
</ul></li>
<li>Depending on your application you may want to parse multiple words as single tokens, for example <code>New York</code> or <code>rock 'n' roll</code>.
<ul>
<li>This requires a multiword expression dictionary of some sort.</li>
<li>Tokenization is hence very closely reliant on <strong>named entity detection</strong>.</li>
</ul></li>
<li>This is all just word segmentation.</li>
<li><strong>Sentence segmentation</strong> is also important.
<ul>
<li><code>?</code> and <code>!</code> are relatively unambiguous markers of sentence endings.</li>
<li><code>.</code> is more ambiguous.
<ul>
<li><code>Mr.</code>, <code>Inc.</code>, <code>he said &quot;howdy.&quot;</code></li>
<li>Sentence tokenization and word tokenization hence tend to be addressed together,</li>
</ul></li>
</ul></li>
<li>Sentence tokenization methods build a <em>binary classifier</em>, either using rules or machine learning, to decide if a period is part of a word or a sentence boundary marker.
<ul>
<li>Abbreviation dictionaries help to deal with abbreviations.</li>
<li>State of the art methods use machine learning, but a sequence of regular expressions is still useful.</li>
</ul></li>
<li>p77: Perl script based on Grefenstette, 1999.</li>
<li>p78: this is so simple that this suggests Finite State Transducers (FSTs) may also be easily implemented.
<ul>
<li>This is the case. Karttunen et. al 1996 and Beesley and Karttunen 2003 give descriptions.</li>
</ul></li>
</ul>
<h3 id="speech-and-language-processing-chapter-4-n-gram-models"><a href="#speech-and-language-processing-chapter-4-n-gram-models">Speech and Language Processing, Chapter 4 (n-gram models)</a></h3>
<ul>
<li>p96: a <strong>word</strong> is the full inflected or derived form of a word.
<ul>
<li>In English n-gram models are based on wordforms, not the <strong>lemmas</strong>, i.e. root.</li>
<li>e.g. cat is the lemma, cats is the inflected wordform.</li>
</ul></li>
<li>p96: n-gram models, and counting words in general, requires tokenization or text normalization; separating out punctuation, dealing with abbreviations, normalizing spelling, etc.
<ul>
<li>Covered in Chapter 3.</li>
</ul></li>
<li>p96: a <strong>type</strong> is a distinct word in a corpus.</li>
<li>p96: a <strong>token</strong> is any instance of a word in the corpus.</li>
<li>p102: typically divide our data ito 80% training, 10% development, and 10% test.</li>
<li>p104: quadrigram sentences based on Shakespeare are actually real Shakespeare.
<ul>
<li>The n-gram probability matrices are very sparse.</li>
</ul></li>
<li>p104: be sure to choose similar training and test copurses. Don’t choose from different genres.</li>
<li>p105: <strong>closed vocabulary</strong> assumes we know all the words in the vocabulary.
<ul>
<li>This can’t possibly be exactly true.</li>
<li>There will be <strong>out of vocabulary (OOV)</strong> words.</li>
<li>The percentive of OOV words in the test set is called the <strong>OOV rate</strong>.</li>
<li>An <strong>open vocabulary</strong> is one where we model OOV words by adding a pseudo-word called <code>&lt;UNK&gt;</code>. We train these probabilities as follows:
<ol style="list-style-type: decimal">
<li><em>Choose a fixed vocabulary</em> in advance.</li>
<li><em>Convert</em> in the training set any OOV word to the unknown word token <code>&lt;UNK&gt;</code> in a text normalization step.</li>
<li><em>Estimate</em> the probabilities for <code>&lt;UNK&gt;</code> from its counts just like any other regular word in the training set.</li>
</ol></li>
</ul></li>
<li>p105: <strong>extrinsic evaluation</strong> of language models is best; apply them to your problem and see which is best.</li>
<li>difficult in practice, so use <strong>intrinsic evaluation</strong> instead, which measures quality independent of any application.</li>
<li><strong>perplexity</strong> is the most common intrinsic evaluation metric.
<ul>
<li>Perplexity is a <strong>weighted average branching factor</strong> of a language. The number of possible next words that can follow any word.</li>
<li>p107: It is closely related to the information theoretic notion of entropy.</li>
</ul></li>
<li>p108: <strong>smoothing</strong> is modifications made to address poor estimates that are due to variability in small data sets.
<ul>
<li>pull in probabiliy mass from higher counts, pile it on to zero counts.</li>
</ul></li>
<li>p108: Laplacian smoothing.</li>
</ul>
<hr />
<p>p111: Good-Turing Discounting</p>
<ul>
<li>Use count of things you’ve seen <em>once</em> (<strong>singletons</strong> or <strong>hapax legomenons</strong>) to re-estimate the frequency of zero-count things.</li>
<li>The <strong>frequency of frequency c</strong> is the number of n-grams that occur c times.</li>
<li>More formally:</li>
</ul>
<p><span class="math">\[N_c = \sum_{x\;:\;\textrm{Count(x)} = c} 1\]</span></p>
<ul>
<li>The MLE count for <span class="math">\(N_c\)</span> is <span class="math">\(c\)</span>. The Good-Turing estimate replaces this with a smoothed count <span class="math">\(c^*\)</span>, as a function of <span class="math">\(N_{c+1}\)</span>:</li>
</ul>
<p><span class="math">\[c^* = (c+1)\frac{N_{c+1}}{N_c}\]</span></p>
<ul>
<li>We can use the equation above to replace the MLE counts for all the bins <span class="math">\(N_1, N_2, \ldots\)</span>.</li>
<li>However, instead of using this equation directly to re-estimate the smoothed count <span class="math">\(c^*\)</span> for <span class="math">\(N_0\)</span>, use the following which we can call the <strong>missing mass</strong>:</li>
</ul>
<p><span class="math">\[P_{GT}^{*}\;\textrm{(things with frequency zero in training)} = \frac{N_1}{N}\]</span></p>
<ul>
<li>Here <span class="math">\(N_1\)</span> is the count of items in bin 1, i.e. seen once in the training set, and <span class="math">\(N\)</span> is the total number of items we have seen in training.</li>
<li>p113: some advanced issues in Good-Turing estimation</li>
<li>p114: Good-Turing discounting is not used by itself; it’s only used in combination with backoff and interpolation, discused later.</li>
</ul>
<hr />
<ul>
<li>We can use an n-gram “hierarchy”, i.e. trigrams, bigrams, and unigrams.</li>
<li>In <strong>backoff</strong> if there is evidence of a higher order N-gram we use it exclusively.</li>
<li><p>In <strong>interpolation</strong> we always mix the probability esitmates of all N-gram estimators.</p></li>
<li>p115: interpolation.</li>
<li>p116: backoff
<ul>
<li>is better than interpolation</li>
<li>takes into account Good-Turing discounting.</li>
</ul></li>
</ul>
<hr />
<ul>
<li>p118: practical issues: toolkits and data formats</li>
<li>Since probabilities by definition are less than 1, the more probabilities we multiply together tha smaller they become.</li>
<li>Hence we use log probabilities rather than raw probabilities, and add in log space rather than multiply in linear space.</li>
<li>In order to report probabilities just take the “exp” of the logprob:</li>
</ul>
<p><span class="math">\[p_1 \times p_2 \times p_3 \times p_4 = exp(log p_1 + log p_2 + log p_3 + log p_4)\]</span></p>
<ul>
<li>Backoff N-gram language models are generally stored in <strong>ARPA format</strong>
<ul>
<li>Small header.</li>
<li>List of all non-zero N-gram probabilities (all unigrams, followed by bigrams, followed by trigrams, etc).</li>
<li>Each N-gram entry is stored with its discounted log probabiliy (in <span class="math">\(\textrm{log}_{10}\)</span> format) and its backoff weight <span class="math">\(\alpha\)</span>.</li>
<li>Backoff weights only necessary if the N-gram forms a prefix of a longer N-gram.</li>
<li>Thus, for trigram grammar, the format of each N-gram is:</li>
</ul></li>
</ul>
<p><TODO></p>
<ul>
<li><p>p119: e.g.</p>
<pre><code>\data\ 
ngram 1=1000
ngram 2=10000
ngram 3=5000

\1-grams:
-0.4405     &lt;/s&gt;
-99         &lt;s&gt;
-4.34443    the         -1.43973
-4.5325     dog         -4.3438
&lt;snip&gt;

\2-grams:
-3.43535    &lt;s&gt;     i     -5.353535
-4.43333    i       went  0.0430843
...

\3-grams:
-3.3245     &lt;s&gt;     i     prefer     3.434
...</code></pre></li>
<li>In training mode each toolkit takes a raw text file, one sentence per line, words separated by white-space.</li>
<li>It also takes parameters such as order <span class="math">\(N\)</span>, thresholds, type of discounting.</li>
<li><p>It outputs a language model in ARPA format.</p></li>
<li><p>In perplexity or decoding mode the toolkit take a language model in ARPA format, a sentence or corpus, and produces the probability and perplexity of the sentence or corpus.</p></li>
</ul>
<hr />
<p><all TODO></p>
<ul>
<li>p119: Advanced smoothing methods: Kneser-Ney Smoothing</li>
<li>p121: it turns out that any interpolation model can be represented as a backoff model, hence stored in ARPA backoff format.</li>
<li>p121: class-based N-grams.</li>
<li>p122: language model adaptation and using the web</li>
<li>use web search hits to estimate trigram language model parameters.</li>
<li><p>works well in practice, even though only getting page counts and not word counts back.</p></li>
<li>p122: using longer distance information: a brief summary</li>
<li>state of the art systems use 4-grams and 5-grams.</li>
<li>After 6-grams up to 20-grams, Goodman found that no useful improvement.</li>
<li><strong>cache</strong> model: use the preceding part of a test corpus and mix it into your trained language model when making predictions.
<ul>
<li>words are often repeated.</li>
<li>only works well in domains where you have perfect knowledge of words.</li>
</ul></li>
<li><strong>topic-based</strong>: train different language models for different kinds of words.</li>
<li><strong>latent-semantic indexing</strong>: measure probability based on the word’s similarity to preceding words, mix it in.</li>
<li><strong>trigger</strong>: a word that is not adjacent but highly related, so we mix it in.</li>
<li><strong>skip N-grams</strong>: we skip over an intermediate word.</li>
<li><p><strong>variable-length N-grams</strong>: adjust context size.</p></li>
<li><p>pruning by removing low-probability events is important, and essential on low-power platforms like cellphones.</p></li>
</ul>
<h3 id="arpa-language-model-lm-file-format"><a href="#arpa-language-model-lm-file-format">ARPA language model (LM) file format</a></h3>
<p>Example:</p>
<pre><code>\data\
ngram 1=19979
ngram 2=4987955
ngram 3=6136155

\1-grams:
-1.6682  A      -2.2371
-5.5975  A&#39;S    -0.2818
-2.8755  A.     -1.1409
-4.3297  A.&#39;S   -0.5886
-5.1432  A.S    -0.4862
...

\2-grams:
-3.4627  A  BABY    -0.2884
-4.8091  A  BABY&#39;S  -0.1659
-5.4763  A  BACH    -0.4722
-3.6622  A  BACK    -0.8814
...

\3-grams:
-4.3813  !SENT_START    A       CAMBRIDGE
-4.4782  !SENT_START    A       CAMEL
-4.0196  !SENT_START    A       CAMERA
-4.9004  !SENT_START    A       CAMP
-3.4319  !SENT_START    A       CAMPAIGN
...
\end\</code></pre>
<ul>
<li>Official reference: <a href="http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html">http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html</a>
<ul>
<li>If you need to represent 0 probability (noting that log 0 is minus infinity) just put “-99”, and when parsing interpret this as 0.</li>
<li>You’re allowed to put in other <code>key=value</code> pairs in the <code>\data\</code> section at the top (e.g. lambda’s for linear interpolation models, bucket sizes, etc.).</li>
</ul></li>
<li>Concise blog interpretation: <a href="http://kered.org/blog/2008-08-12/arpa-language-model-file-format/">http://kered.org/blog/2008-08-12/arpa-language-model-file-format/</a></li>
<li>Grammar-like interpretation: <a href="http://www.ee.ucla.edu/~weichu/htkbook/node243_ct.html">http://www.ee.ucla.edu/~weichu/htkbook/node243_ct.html</a></li>
<li>!!AI I think “start sentence” is <code>&lt;s&gt;</code> and “end sentence” is <code>&lt;/s&gt;</code>.</li>
<li>Note that for unigrams you have <code>log_prob word1 log_alpha</code>.</li>
<li>Note that for bigrams you have <code>log_prob word1 word2 log_alpha</code>.</li>
<li>Note that for trigrams there is no backoff parameter; it is useless because, if you note the definition of Katz-backoff, it’s never used.</li>
</ul>
<hr />
</body>
</html>
