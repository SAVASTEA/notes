<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="_pandoc.css" type="text/css" />
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <link href='http://fonts.googleapis.com/css?family=Noticia+Text:400,400italic,700,700italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic,700italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Inconsolata:400,700&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
  
</head>
<body>
<div id="TOC">
<ul>
<li><a href="#natural-language-processing">Natural Language Processing</a><ul>
<li><a href="#readings-policy">Readings policy</a></li>
<li><a href="#rendering">Rendering</a></li>
<li><a href="#week-1---introduction-to-natural-language-processing">Week 1 - Introduction to Natural Language Processing</a><ul>
<li><a href="#introduction-part-1">Introduction (Part 1)</a></li>
<li><a href="#introduction-part-2">Introduction (Part 2)</a></li>
</ul></li>
<li><a href="#week-1---the-language-modeling-problem">Week 1 - The Language Modeling Problem</a><ul>
<li><a href="#introduction-to-the-language-modeling-problem-part-1">Introduction to the Language Modeling Problem (Part 1)</a></li>
<li><a href="#introduction-to-the-language-modeling-problem-part-2">Introduction to the Language Modeling Problem (Part 2)</a></li>
<li><a href="#markov-processes-part-1">Markov Processes (Part 1)</a></li>
<li><a href="#markov-processes-part-2"> Markov Processes (Part 2)</a></li>
<li><a href="#trigram-language-models">Trigram Language Models</a></li>
<li><a href="#evaluating-language-models-perplexity">Evaluating Language Models: Perplexity</a></li>
</ul></li>
<li><a href="#week-1---parameter-estimation-in-language-models">Week 1 - Parameter Estimation in Language Models</a><ul>
<li><a href="#linear-interpolation-part-1">Linear Interpolation (Part 1)</a></li>
<li><a href="#linear-interpolation-part-2"> Linear Interpolation (Part 2)</a></li>
<li><a href="#discounting-methods-part-1">Discounting Methods (Part 1)</a></li>
<li><a href="#discounting-methods-part-2">Discounting Methods (Part 2)</a></li>
<li><a href="#summary"> Summary</a></li>
</ul></li>
<li><a href="#week-2---tagging-problems-and-hidden-markov-models"> Week 2 - Tagging Problems and Hidden Markov Models</a><ul>
<li><a href="#the-tagging-problem">The Tagging Problem</a></li>
<li><a href="#generative-models-for-supervised-learning">Generative Models for Supervised Learning</a></li>
<li><a href="#hidden-markov-models">Hidden Markov Models</a></li>
<li><a href="#parameter-estimation-in-hmms">Parameter Estimation in HMMs</a></li>
<li><a href="#the-viterbi-algorithm-for-hmms">The Viterbi Algorithm for HMMs</a></li>
<li><a href="#the-viterbi-algorithm">The Viterbi Algorithm</a></li>
<li><a href="#the-viterbi-algorithm-1">The Viterbi Algorithm</a></li>
<li><a href="#summary-1">Summary</a></li>
</ul></li>
<li><a href="#week-3---parsing-and-context-free-grammars-cfgs">Week 3 - Parsing, and Context-Free Grammars (CFGs)</a><ul>
<li><a href="#parsing-syntatic-structure"> Parsing (Syntatic Structure)</a></li>
<li><a href="#syntactic-formalisms">Syntactic Formalisms</a></li>
<li><a href="#the-information-conveyed-by-parse-trees">The Information Conveyed By Parse Trees</a></li>
<li><a href="#an-example-application-machine-translation">An Example Application: Machine Translation</a></li>
<li><a href="#context-free-grammars">Context-Free Grammars</a></li>
<li><a href="#example-cfg-for-english">Example CFG for English</a></li>
<li><a href="#left-most-derivations">Left-Most Derivations</a></li>
<li><a href="#an-example-2"> An Example</a></li>
<li><a href="#properties-of-cfgs"> Properties of CFGs</a></li>
<li><a href="#the-problem-with-parsing-ambiguity">The Problem with Parsing: Ambiguity</a></li>
<li><a href="#a-brief-sketch-of-the-syntax-of-english">A brief sketch of the syntax of English</a></li>
<li><a href="#a-fragment-of-a-noun-phrase-grammar"> A Fragment of a Noun Phrase Grammar</a></li>
<li><a href="#prepositions-and-prepositional-phrases">Prepositions and Prepositional Phrases</a></li>
<li><a href="#verbs-verb-phrases-and-sentences">Verbs, Verb Phrases, and Sentences</a></li>
<li><a href="#pps-modifying-verb-phrases">PPs Modifying Verb Phrases</a></li>
<li><a href="#complementizers-and-sbars">Complementizers, and SBARs</a></li>
<li><a href="#more-verbs">More Verbs</a></li>
<li><a href="#coordination">Coordination</a></li>
<li><a href="#weve-only-scratched-the-surface"> We’ve only scratched the surface…</a></li>
<li><a href="#sources-of-ambiguity">Sources of Ambiguity</a></li>
</ul></li>
<li><a href="#week-3---probabilistic-context-free-grammars-pcfgs">Week 3 - Probabilistic Context-Free Grammars (PCFGs)</a><ul>
<li><a href="#a-probabilistic-context-free-grammar-pcfg">A Probabilistic Context-Free Grammar (PCFG)</a></li>
<li><a href="#properties-of-cfgs-1">Properties of CFGs</a></li>
<li><a href="#data-for-parsing-experiments-treebanks">Data for Parsing Experiments: Treebanks</a></li>
<li><a href="#deriving-a-pcfg-from-a-treebank">Deriving a PCFG from a Treebank</a></li>
<li><a href="#pcfgs">PCFGs</a></li>
<li><a href="#parsing-with-a-pcfg">Parsing with a PCFG</a></li>
<li><a href="#chomsky-normal-form">Chomsky Normal Form</a></li>
<li><a href="#a-dynamic-programming-algorithm">A Dynamic Programming Algorithm</a></li>
<li><a href="#an-example-3"> An Example</a></li>
<li><a href="#a-dynamic-programming-algorithm-1"> A Dynamic Programming Algorithm</a></li>
<li><a href="#an-example-4">An Example</a></li>
<li><a href="#justification">Justification</a></li>
<li><a href="#the-full-dynamic-programming-algorithm"> The Full Dynamic Programming Algorithm</a></li>
<li><a href="#summary-2"> Summary</a></li>
</ul></li>
<li><a href="#week-4---weaknesses-of-pcfgs"> Week 4 - Weaknesses of PCFGs</a><ul>
<li><a href="#lack-of-sensitivity-to-lexical-information">Lack of sensitivity to lexical information</a></li>
<li><a href="#lack-of-sensitivity-to-structural-frequencies">Lack of sensitivity to structural frequencies</a></li>
</ul></li>
<li><a href="#week-4---lexicalized-pcfgs">Week 4 - Lexicalized PCFGs</a><ul>
<li><a href="#heads-in-context-free-rules">Heads in Context-Free Rules</a></li>
<li><a href="#more-about-heads">More about Heads</a></li>
<li><a href="#rules-which-recover-heads-an-example-for-nps">Rules which Recover Heads: An Example for NPs</a></li>
<li><a href="#rules-which-recover-heads-an-example-for-vps">Rules which Recover Heads: An Example for VPs</a></li>
<li><a href="#adding-headwords-to-trees">Adding Headwords to Trees</a></li>
<li><a href="#adding-headwords-to-trees-continued">Adding Headwords to Trees (Continued)</a></li>
<li><a href="#chomsky-normal-form-1">Chomsky Normal Form</a></li>
<li><a href="#lexicalized-context-free-grammars-in-chomsky-normal-form"> Lexicalized Context-Free Grammars in Chomsky Normal Form</a></li>
<li><a href="#an-example-5"> An Example</a></li>
<li><a href="#parameters-in-a-lexicalized-pcfg">Parameters in a Lexicalized PCFG</a></li>
<li><a href="#parsing-with-lexicalized-cfgs">Parsing with Lexicalized CFGs</a></li>
<li><a href="#a-model-from-charniak-1997">A Model from Charniak (1997)</a></li>
<li><a href="#other-important-details">Other Important Details</a></li>
<li><a href="#evaluation-representing-trees-as-constituents">Evaluation: Representing Trees as Constituents</a></li>
<li><a href="#precision-and-recall"> Precision and Recall</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#evaluation-dependencies">Evaluation: Dependencies</a></li>
<li><a href="#strengths-and-weaknesses-of-modern-parsers"> Strengths and Weaknesses of Modern Parsers</a></li>
<li><a href="#summary-3">Summary</a></li>
<li><a href="#dependency-accuracies">Dependency Accuracies</a></li>
</ul></li>
<li><a href="#readings">Readings</a><ul>
<li><a href="#speech-and-language-processing-chapter-3-words-and-transducers">Speech and Language Processing, Chapter 3 (Words and Transducers)</a></li>
<li><a href="#speech-and-language-processing-chapter-4-n-gram-models">Speech and Language Processing, Chapter 4 (n-gram models)</a></li>
<li><a href="#arpa-language-model-lm-file-format">ARPA language model (LM) file format</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1 id="natural-language-processing"><a href="#natural-language-processing">Natural Language Processing</a></h1>
<p>Columbia University, via Coursera</p>
<p><a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a><br /><span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Natural Language Processing notes</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="http://www.asimihsan.com" property="cc:attributionName" rel="cc:attributionURL">Asim Ihsan</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.</p>
<iframe width="292" height="420" src="http://tools.flattr.net/widgets/thing.html?thing=1150095"></iframe>

<h2 id="readings-policy"><a href="#readings-policy">Readings policy</a></h2>
<p>There are excellent readings assigned to the class. They’re explicitly inlined into the respective lecture, to save typing stuff out twice.</p>
<p>Other readings (papers, textbooks, other courses) are explicitly inlined as well.</p>
<h2 id="rendering"><a href="#rendering">Rendering</a></h2>
<p>In order to use pandoc run (need to include custom LaTeX packages for some symbols):</p>
<pre><code>    pandoc \[course\]\ natural\ language\ processing.md
    -o pdf/nlp.pdf --include-in-header=latex.template</code></pre>
<p>or, for Markdown + LaTex to HTML + MathJax output:</p>
<pre><code>    pandoc \[course\]\ natural\ language\ processing.md
    -o html/nlp.html
    --include-in-header=html/_header.html
    --mathjax -s --toc --smart -c _pandoc.css</code></pre>
<p>and, for the ultimate experience, after <code>pip install watchdog</code>:</p>
<pre><code>    watchmedo shell-command --patterns=&quot;*.md&quot;
    --ignore-directories --recursive
    --command=&#39;&lt;command above&gt;&#39; .</code></pre>
<h2 id="week-1---introduction-to-natural-language-processing"><a href="#week-1---introduction-to-natural-language-processing">Week 1 - Introduction to Natural Language Processing</a></h2>
<h3 id="introduction-part-1"><a href="#introduction-part-1">Introduction (Part 1)</a></h3>
<ul>
<li>What is NLP?
<ul>
<li>Computers using natural language as input and/or output.</li>
<li>NLU: understanding, input</li>
<li>NLG: generation, output.</li>
</ul></li>
</ul>
<p>Tasks</p>
<ul>
<li>Oldest task: <strong>machine translation</strong>. Convert between two languages.</li>
<li><strong>Information extraction</strong>
<ul>
<li>Text as input, structure of key content as output.</li>
<li>e.g. job posting into industry, position, location, company, salary.</li>
<li>Complex searches (“jobs in Boston paying XXX”).</li>
<li>Statistical queries (“how has jobs changed in IT changed over time?”)</li>
</ul></li>
<li><strong>Text summarization</strong>
<ul>
<li>Condense one or many documents into a summary.</li>
<li><a href="http://newsblaster.cs.columbia.edu/"><em>Columbia Newsblaster</em></a> is an example.</li>
</ul></li>
<li><strong>Dialogue systems</strong>
<ul>
<li>Humans can interact with a computer to ask questions and achieve tasks.</li>
</ul></li>
</ul>
<p>Basic NLP problems</p>
<ul>
<li><strong>Tagging</strong>
<ul>
<li>Map strings to tagged sequences (each word is lexed and tagged with an appropriate label).</li>
<li><strong>Part-of-speech tagging</strong>: noun, verb, preposition, …
<ul>
<li>Profits (N) soared (V) at (P) Boeing (N)</li>
</ul></li>
<li><strong>Named Entity Recognition</strong>: companies, locations, people
<ul>
<li>Profits (NA) soared (NA) at (NA) Boeing (C)</li>
</ul></li>
</ul></li>
<li><strong>Parsing</strong>
<ul>
<li>e.g. “Boeing is located in Seattle” into a parse tree.</li>
</ul></li>
</ul>
<h3 id="introduction-part-2"><a href="#introduction-part-2">Introduction (Part 2)</a></h3>
<p>Why is NLP hard?</p>
<ul>
<li><strong>Ambiguity</strong>
<ul>
<li>“At last, a computer that understands you like your mother”; three intrepretations at the <em>syntactic</em> level.</li>
<li>But also occurs at an <em>acoustic</em> level: “like your” sounds like “lie cured”.
<ul>
<li>One is <em>more likely</em> than the other, but without this information difficult to tell.</li>
</ul></li>
<li>At <em>semantic</em> level, words often have more than one meaning. Need context to disambiguate.
<ul>
<li>“I saw her duck with a telescope”.</li>
</ul></li>
<li>At <em>discourse</em> (multi-clause) level.
<ul>
<li>“Alice says they’ve built a computer that understands you like your mother”</li>
<li>If you start a sentence saying “but she…”, who is she referring to?</li>
</ul></li>
</ul></li>
</ul>
<p>What will this course be about</p>
<ul>
<li><strong>NLP subproblems</strong>: tagging, parsing, disambiguation.</li>
<li><strong>Machine learning techniques</strong>: probabilistic CFGs, HMMs, EM algorithm, log-linear models.</li>
<li><strong>Applications</strong>: information extraction, machine translation, natural language interfaces.</li>
</ul>
<h4 id="syllabus"><a href="#syllabus">Syllabus</a></h4>
<ul>
<li>Language modelling, smoothed estimation</li>
<li>Tagging, hidden Markov models</li>
<li>Statistical parsing</li>
<li>Machine translation</li>
<li>Log-linear models, discriminative methods</li>
<li>Semi-supervised and unsupervised learning for NLP</li>
</ul>
<h2 id="week-1---the-language-modeling-problem"><a href="#week-1---the-language-modeling-problem">Week 1 - The Language Modeling Problem</a></h2>
<h3 id="introduction-to-the-language-modeling-problem-part-1"><a href="#introduction-to-the-language-modeling-problem-part-1">Introduction to the Language Modeling Problem (Part 1)</a></h3>
<ul>
<li>We have some finite vocabulary, i.e.</li>
</ul>
<p><span class="math">\[V = \{the, a, man telescope, Beckham, two, ...\}\]</span></p>
<ul>
<li>We have countably infinite set of strings, which are the set of possible sentences in the language:</li>
</ul>
<p><span class="math">\[V^+ = \{&quot;the\:STOP&quot;, &quot;a\:STOP&quot;, &quot;the\:fan\:STOP&quot;, ...\}\]</span></p>
<ul>
<li>STOP is a stop symbol at the end of a sentence. Convenient later on.</li>
<li>Sentences don’t have to make sense, just every sequence of words.</li>
<li><p>Also a sentence could just be {“STOP”}, empty.</p></li>
<li>We have a <em>training sample</em> of example sentences in English.
<ul>
<li>Sentences from the New York Times in the last 10 years.</li>
<li>Sentences from a large set of web pages.</li>
<li>In the 1990’s 20 million words common, by the end of the 90’s 1 billion words common.</li>
<li>Nowadays 100’s of billions of words.</li>
</ul></li>
<li><p>With this training sample we want to “learn” a probabiliy distribution p, i.e. p is a function that satisfies:</p></li>
</ul>
<p><span class="math">\[\sum_{x \in V^+} p(x) = 1, \quad p(x) \ge 0 \; \forall \; x \in V^+\]</span></p>
<ul>
<li>For any sentence x in language, p(x) &gt;= 0.</li>
<li>If we sum over all sentences x in language, p(x) sums to 1.</li>
<li>A good language model assigns high probabilities to likely sentences in English (the fan saw Beckham STOP), low probabilities to unlikely sentences in English (Beckham fan saw the STOP)</li>
</ul>
<h3 id="introduction-to-the-language-modeling-problem-part-2"><a href="#introduction-to-the-language-modeling-problem-part-2">Introduction to the Language Modeling Problem (Part 2)</a></h3>
<ul>
<li>But…why do we want to do this?!
<ul>
<li><strong>Speech recognition</strong> was original motivation; related problems are optical character recognition and handwriting recognition.</li>
<li>Input: sound wave time series.</li>
<li>Preprocess: split into relatively short time periods, e.g. 10ms.</li>
<li>For each frame do a Fourier transform, get energies of frequencies.</li>
<li>Problem is to output recognised speech, sequence of words.</li>
<li>Main course notes: it’s useful to have prior probabilities so that if we can choose between alternatives we can ask “which is most likely?”.
<ul>
<li>“recognise speech” vs “wreck a nice beach”</li>
</ul></li>
<li>The estimation techniques developed for this problem will be very useful for other problems in NLP.</li>
</ul></li>
<li>Naive method of language modelling
<ul>
<li>We have N training sentences.</li>
<li>For any sentence <span class="math">\(x_1, ..., x_n\)</span>, define <span class="math">\(c(x_1, ..., x_n)\)</span> as the number of times the sentences is seen in our training data.</li>
<li>Naive estimate:</li>
</ul></li>
</ul>
<p><span class="math">\[p(x_1, \ldots, x_n) = \frac{c(x_1, \ldots, x_n)}{N}\]</span></p>
<ul>
<li>This is a valid, well-formed language model (p(x) sums to 1, they’re all &gt;= 0).</li>
<li>However, they’ll assign a probabiliy of 0 to any unseen sentences; no ability to generalise to new sentences.</li>
<li>How can we build language models that generalise beyond the test sentences?</li>
</ul>
<h3 id="markov-processes-part-1"><a href="#markov-processes-part-1">Markov Processes (Part 1)</a></h3>
<ul>
<li>Markov Processes
<ul>
<li>Consider a sequence of random variables <span class="math">\(X_1, X_2, \ldots, X_n\)</span>.</li>
<li>Each random variable can take any value in a finite set V.</li>
<li>For now assume n is fixed, e.g. = 100. Every sequence is the same length.</li>
<li>Our goal: model the joint probability distribution of the values of these n variables:</li>
</ul></li>
</ul>
<p><span class="math">\[P(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n)\]</span></p>
<ul>
<li><p>This is huge: for vocabulary V, number of sequences of length n is <span class="math">\(|V|^n\)</span>.</p></li>
<li>First-Order Markov Processes</li>
<li>Going to use the chain rule of probabilities to decompose the expression into a product of expressions.</li>
<li><p>For two expressions, this rule is:</p></li>
</ul>
<p><span class="math">\[P(A,B) = P(A) \times P(B|A)\]</span> <span class="math">\[P(A,B,C) = P(A) \times P(B|A) \times P(C|A,B)\]</span></p>
<ul>
<li>Hence:</li>
</ul>
<p><span class="math">\[P(X_1 = x_1, X_2 = x_2) = P(X_1 = x_1) \times P(X_2 = x_2 | X_1 = x_1)\]</span> <span class="math">\[P(X_1 = x_1, X_2 = x_2, X_3 = x_3) = ... P(X_3 = x_3 | P(X_2 = x_2, X_1 = x_1)\]</span></p>
<ul>
<li>This kind of decomposition is <em>exact</em>: this is always true, and no assumptions are involved.</li>
<li>Hence the general decomposition:</li>
</ul>
<p><span class="math">\[P(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n)\]</span> <span class="math">\[=P(X_1 = x_1) \prod_{i=2}^{n} P(X_i = x_i\;|\;X_1 = x_1, \dots, X_{i-1} = x_{i-1})\]</span></p>
<ul>
<li>Continuing on, with first-order Markov assumption:</li>
</ul>
<p><span class="math">\[= P(X_1 = x_1) \prod_{i=2}^{n} P(X_i = x_i\;|\; X_{i-1} = x_{i-1})\]</span></p>
<ul>
<li>The first-order Markov assumption: for any <span class="math">\(i \in \{2, \dots, n\}\)</span>, for any <span class="math">\(x_1, \dots, x_n\)</span>:</li>
</ul>
<p><span class="math">\[P(X_i=x_i|X_1=x_1, \ldots, X_{i-1} = x_{i-1}) = P(X_i=x_i | X_{i-1} = x_{i-1})\]</span></p>
<ul>
<li>Random variable at position i depends on just the previous value, on the variable at position (i-1).
<ul>
<li><span class="math">\(X_i\)</span> is conditionally independent of all the other random variables once you condition on <span class="math">\(X_{i-1}\)</span>.</li>
</ul></li>
</ul>
<h3 id="markov-processes-part-2"><a href="#markov-processes-part-2"> Markov Processes (Part 2)</a></h3>
<ul>
<li>What about Second-Order Markov Processes?</li>
<li>Again, the problem is to model the joint distribution over <span class="math">\(n\)</span> random variables:</li>
</ul>
<p><span class="math">\[P(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n)\]</span> <span class="math">\[=P(X_1 = x_1) P(X_2 = x_2 | X_1 = x_1) \prod_{i=3}^{n} P(X_i = x_i | X_{i-2} = x_{i-2}, X_{i-1} = x_{i-1})\]</span></p>
<ul>
<li>For elements further along in the sequence the value for the i’th random variable depends on the previous <em>two</em> random variables.</li>
<li>This is a bit awkward, so for convenience we assume <span class="math">\(x_0 = x_{-1} = *\)</span>, where <span class="math">\(*\)</span> is a special “start” symbol.</li>
</ul>
<p><span class="math">\[= \prod_{i=1}^{n} P(X_i = x_i | X_{i-2} = x_{i-2}, X_{i-1} = x_{i-1})\]</span></p>
<ul>
<li>For example, <span class="math">\(x_{-1} = *,\;x_0 = *,\;x_1 = the,\;\ldots\)</span>,</li>
</ul>
<h4 id="modelling-variable-length-sequences"><a href="#modelling-variable-length-sequences">Modelling Variable Length Sequences</a></h4>
<ul>
<li>Want <span class="math">\(n\)</span> to also be a random variable.</li>
<li>Simple solution: always define <span class="math">\(X_n = STOP\)</span>, where <span class="math">\(STOP\)</span> is a special symbol.</li>
<li>Use a Markov process as before, but assume <span class="math">\(X_n = STOP\)</span>.</li>
</ul>
<h3 id="trigram-language-models"><a href="#trigram-language-models">Trigram Language Models</a></h3>
<ul>
<li>A trigram language model consists of:
<ol style="list-style-type: decimal">
<li>A finite set <span class="math">\(V\)</span> (the words, the vocabulary).</li>
<li>A parameter <span class="math">\(q(w|u,v)\)</span> for each trigram <span class="math">\(u,v,w\)</span> such that <span class="math">\(w \in V \bigcup \{STOP\}\)</span>, and <span class="math">\(u,v \in V \bigcup \{*\}\)</span>.
<ul>
<li>For each <em>trigram</em> <span class="math">\(u,v,w\)</span>, a sequence of three words, we have a parameter <span class="math">\(q(w|u,v)\)</span>.</li>
<li><span class="math">\(w\)</span> could be any element of V or STOP, and</li>
<li><span class="math">\(u,v\)</span> could be any element of V or START.</li>
</ul></li>
</ol></li>
<li>For any sentence <span class="math">\(x_1, \ldots, x_n\)</span> where <span class="math">\(x_i \in V\)</span> for <span class="math">\(i = 1 \ldots (n-1)\)</span>, and <span class="math">\(x_n = STOP\)</span>, the probability of the sentence under the trigram model is:</li>
</ul>
<p><span class="math">\[p(x_1, \dots, x_n) = \prod_{i=1}^{n}q(x_i\;|\;x_{i-2},x_{i-1})\]</span></p>
<ul>
<li>where we define <span class="math">\(x_0 = x_{-1} = *\)</span>.</li>
<li>i.e. for any sentence the probability of it is the product of second-order Markov probabilities of its constituent trigrams.</li>
</ul>
<p>An example. For the sentence</p>
<pre><code>    the dog barks STOP</code></pre>
<p>we could have</p>
<p><span class="math">\(p(\textrm{the dog barks STOP}) =\)</span><br /><span class="math">\(q(\textrm{the | *, *})\)</span><br /><span class="math">\(\times q(\textrm{dog | *, the})\)</span><br /><span class="math">\(\times q(\textrm{barks | the, dog})\)</span><br /><span class="math">\(\times q(\textrm{STOP | dog, barks})\)</span></p>
<ul>
<li>This is still a naive language model. It’s easy to find problems.</li>
<li>PCFGs, explored later, are much superior.</li>
<li>Having said that, trigram language models are extremely useful.
<ul>
<li>They are very hard to improve upon.</li>
<li>Considerable simplicity.</li>
</ul></li>
</ul>
<hr />
<ul>
<li>Quiz: say we have a language model with <span class="math">\(V = \{\textrm{the, dog, runs}\}\)</span>, and the following parameters:</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        q(\textrm{the | *, *}) &amp; = 1 \\
        q(\textrm{dog | *, the}) &amp; = 0.5 \\  
        q(\textrm{STOP | *, the}) &amp; = 0.5 \\
        q(\textrm{runs | the, dog}) &amp; = 0.5 \\ 
        q(\textrm{STOP | the, dog}) &amp; = 0.5 \\
        q(\textrm{STOP | dog, runs}) &amp; = 1
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>There are <strong>three</strong> sentences with non-zero probability under this model.
<ul>
<li>{*, *, the, STOP}</li>
<li>{*, *, the, dog, STOP}</li>
<li>{*, *, the, dog, runs, STOP}</li>
</ul></li>
<li>Draw out a graph, where nodes are words and edge labels denote probabilities, to see this:</li>
</ul>
<div class="figure">
<img src="nlp_01_trigram_graph.png" />
</div>
<ul>
<li>The above is a <a href="http://en.wikipedia.org/wiki/Markov_chain">Markov chain</a> drawn in the style of a <a href="http://en.wikipedia.org/wiki/Finite-state_machine">finite-state machine</a>. This is a lot of fancy talk for a very simple idea.</li>
<li>People call these sorts of diagrams all sorts of fancy names.
<ul>
<li>Mathematicians call this type of diagram a <strong>graph</strong>. (No, not a pie <em>chart</em>).
<ul>
<li>Since there are arrows in this diagram this is a <strong>directed graph</strong>.</li>
<li>If the digram just had plain lines, without arrow heads, it would be an <strong>undirected graph</strong>.</li>
</ul></li>
<li>Computer scientists call this type of diagram a <strong>finite-state machine</strong> (FSM).</li>
<li>Electrical engineers call diagrams that (kind of) look like this <strong>Markov chains</strong>.</li>
<li>In graph-speak, each circle is a <strong>node</strong> and each arrow is an <strong>edge</strong>.</li>
<li>In FSM-speak, each circle is a <strong>state</strong> and each arrow is a <strong>transition</strong>.</li>
<li>People will often use node/state and edge/transition interchangably, and others love pointing out that they’re using the wrong words.</li>
</ul></li>
<li>Every circle and double circle represents a <em>state</em>. A state is “somewhere you can be”, which’ll make sense soon.</li>
<li>Every line is the <em>likelihood of moving from one state to another</em>. If this number is 0 this transition is impossible, if it is 1 it is certain, and could be something in between.
<ul>
<li>This number can never be less than 0 or greater than 1. If it is then the probability distribution is no longer “well-formed”, i.e. doesn’t make sense.</li>
</ul></li>
<li>On the left-hand side an arrow appears to come out of nowhere to the first start symbol (*). This is the <em>start state</em>.</li>
<li>Every double circle is an <em>accepting state</em>. <strong>The only accepting state is the STOP symbol</strong>.</li>
<li>Take your finger and trace out every path from every start state to every accepting state. This is a possible sequence of words in this language model.
<ul>
<li>As the STOP symbol is the only accepting state, <strong>every sequence of words in this language model must end with the STOP symbol</strong>.</li>
</ul></li>
<li>The probability of a given sequence of words is the <strong>product</strong> of all the edge probabilities.</li>
<li>The sum of probabilities of all possible sequences of words must add up to one if the language model is “well-formed”.
<ul>
<li>One of the rules of a probability distributions is that all the “little probabilities”, or probabilities of individual events, must sum to 1 or else it just doesn’t make sense.</li>
</ul></li>
<li>You’ll notice something peculiar. If any edge has a value of 0 then all sequences of words that include that edge must be impossible, no matter how likely the other transitions in that sequence!
<ul>
<li>Hence when you’re tracing your finger looking for possible sequences you <strong>exclude any paths that include an edge with a probability of 0</strong>.</li>
</ul></li>
<li>You may be wondering how hard this gets with many nodes and many edges. Very hard. This is just a visual depiction of how utterly impossible brute force is when it comes to language models, and hence how important the upcoming Vitterbi algorithm is.</li>
</ul>
<hr />
<h4 id="the-trigram-estimation-problem"><a href="#the-trigram-estimation-problem">The Trigram Estimation Problem</a></h4>
<ul>
<li>But what are the values of parameters q?</li>
<li>This turns out to be a challenging problem.</li>
<li>A natural estimate: the <strong>maximum likelihood estimate (ML)</strong>.</li>
<li>Recall that we assume that we have a training set, some example sentences in our language, typically, as you recall, millions or billions of sentences.</li>
<li>From these sentences we can derive counts; how often do trigrams occur?</li>
</ul>
<p><span class="math">\[q(w_i\;|\;w_{i-2},w_{i-1}) = \frac{\textrm{Count}(w_{i-2},w_{i-1},w_{i})}{\textrm{Count}(w_{i-2},w_{i-1})}\]</span></p>
<ul>
<li>For example:</li>
</ul>
<p><span class="math">\[q(\textrm{laughs | the, dog}) = \frac{\textrm{Count(the, dog, laughs)}}{\textrm{Count(the, dog)}}\]</span></p>
<ul>
<li>This is intuitive. For instances of a particular bigram how often are they followed by the particular third word of our trigram?</li>
</ul>
<hr />
<ul>
<li>Quiz: consider the following corpus of sentences:
<ul>
<li>the dog walks STOP</li>
<li>walks the dog STOP</li>
<li>dog walks fast STOP</li>
</ul></li>
<li>Let <span class="math">\(q_{ML}\)</span> by the maximum-likelihood parameters of a trigram langauge model trained on this corpus. Which of the following parameters have a value that is both well-defined and non-zero?</li>
</ul>
<p>Correct:</p>
<p><span class="math">\(q_{ML}({\textrm{walks | *, dog}})\)</span><br /><span class="math">\(q_{ML}({\textrm{dog | walks, the}})\)</span><br /><span class="math">\(q_{ML}({\textrm{walks | the, dog}})\)</span></p>
<p>Incorrect:</p>
<p><span class="math">\(q_{ML}({\textrm{walks | dog, the}})\)</span><br /><span class="math">\(q_{ML}({\textrm{fast | dog, the}})\)</span><br /><span class="math">\(q_{ML}({\textrm{STOP | walks, dog}})\)</span></p>
<hr />
<ul>
<li>ML is a useful starting point, but has serious problems.</li>
</ul>
<p>Spare Data problems</p>
<ul>
<li>Say our vocabulary size is <span class="math">\(N = |V|\)</span>, then there are <span class="math">\(N^3\)</span> parameters in our model.</li>
<li>e.g. <span class="math">\(N = 20,000\;\implies\;20,000^3 = 8 \times 10^{12}\)</span> parameters.</li>
<li>Most parameters will be zero; most possible trigrams will not appear.</li>
<li>But does that mean all trigrams we haven’t seen are necessarily impossible to <em>ever</em> see? No.</li>
<li>Worse still, the bigram denominator may be zero, and the ML ratio is undefined.</li>
</ul>
<h3 id="evaluating-language-models-perplexity"><a href="#evaluating-language-models-perplexity">Evaluating Language Models: Perplexity</a></h3>
<ul>
<li>We have some test data, <span class="math">\(m\)</span> sentences, i.e. <span class="math">\(s_1, s_2, s_3, \ldots, s_m\)</span>. Each of these is a sentence in the language, e.g. {the dog laughs STOP}.</li>
<li>Additionally, assume that use some <em>development</em> data to determine the language model parameters, but hold out some additional <em>test data</em> to evaluate the language model.</li>
<li>Natural to look at the probability that our language model gives to sentences in the test data <span class="math">\(\prod_{i=1}^{m}p(s_i)\)</span>; it’s never seen it before.</li>
</ul>
<p><span class="math">\[\textrm{log}\;\prod_{i=1}^{m} p(s_i) = \sum_{i=1}^{m} \textrm{log}\;p(s_i)\]</span></p>
<ul>
<li>(the above is a basic rule of logarithms; log of product = sum of logs).</li>
<li>recall that e.g.:</li>
</ul>
<p><span class="math">\[p(s_i) = q(\textrm{the | *, *}) \times q(\textrm{dog | *, the}) \times \ldots\]</span></p>
<ul>
<li>Naturally we’d expect better languages models to assign higher probabilities to sentences in the test data.</li>
<li><p>And log is a monotonically increasing function, so expect the sum of logs to correspondingly be higher for better language models.</p></li>
<li><p>In fact, the usual evaluation measure is <strong>perplexity</strong>:</p></li>
</ul>
<p><span class="math">\[\textrm{Perplexity} = 2^{-l},\;\textrm{where}\]</span> <span class="math">\[l = \frac{1}{M} \sum_{i=1}^{m} \textrm{log}\;p(s_i)\]</span></p>
<ul>
<li>and M is the total number of <em>words</em> in the test data. In some sense with (1/M) the perplexity is now stable with respect to the size of the test data.</li>
<li>The <em>lower</em> the perplexity the <em>better the fit</em> of the language model to the test data.</li>
</ul>
<p>Some Intuition about Perplexity</p>
<ul>
<li>Say we have vocabulary <span class="math">\(V\)</span>, and <span class="math">\(N = |V| + 1\)</span>, and the dumbest possible model predicts:</li>
</ul>
<p><span class="math">\[q(w|u,v) = \frac{1}{N},\;\forall\;w \in V \cup \{\textrm{STOP}\},\;\forall\;u,v \in V \cup \{\textrm{*}\}\]</span>.</p>
<ul>
<li>This dumbest model assigns the uniform distribution over all possible words in each possible. Ignores previous words, doesn’t measure relative frequency.</li>
<li>Easy to calculate perplexity:</li>
</ul>
<p><span class="math">\[\textrm{Perplexity} = 2^{-l},\;\textrm{where}\;l=\textrm{log}\;\frac{1}{N}\]</span> <span class="math">\[\implies\; \textrm{Perplexity} = N\]</span></p>
<ul>
<li>!!AI implying all these calculations use log base 2.</li>
<li>Perplexity is a measure of effective “branching factor”.
<ul>
<li>The model is as confused on test data as if it had to choose uniformly and independently among P possibilities per word, where P is the perplexity. Source: <a href="http://en.wikipedia.org/wiki/Perplexity">Wikipedia:Perplexity</a>.</li>
</ul></li>
</ul>
<hr />
<p>Quiz: define a trigram language model with the following parameters:</p>
<ul>
<li>q(the | *, *) = 1</li>
<li>q(dog | *, the) = 0.5</li>
<li>q(cat | *, the) = 0.5</li>
<li>q(walks | the, cat) = 1</li>
<li>q(STOP | cat, walks) = 1</li>
<li>q(runs | the, dog) = 1</li>
<li>q(STOP | dog, runs) = 1</li>
</ul>
<p>Now consider a test corpus with the following sentences:</p>
<ul>
<li>the dog runs STOP</li>
<li>the cat walks STOP</li>
<li>the dog runs STOP</li>
</ul>
<p>Note that the number of words in this corpus, M, is 12.</p>
<p>What is the perplexity of the language model, to 3dp?</p>
<p><span class="math">\[P = 2^{-l}\]</span> <span class="math">\[l = \frac{1}{M} \sum \textrm{log}_2\{p(s_i)\}\]</span></p>
<p><span class="math">\(p(\textrm{the dog runs STOP}) = q(\textrm{the | *, *}) \times q(\textrm{dog | *, the}) \times q(\textrm{runs | the, dog}) \times q(\textrm{STOP | dog, runs})\)</span><br /><span class="math">\(= 1 \times 0.5 \times 1 \times 1 = 0.5\)</span></p>
<p><span class="math">\(p(\textrm{the cat walks STOP}) = q(\textrm{the | *, *}) \times q(\textrm{cat | *, the}) \times q(\textrm{walks | the, cat}) \times q(\textrm{STOP | cat walks})\)</span><br /><span class="math">\(= 1 \times 0.5 \times 1 \times 1 = 0.5\)</span></p>
<p><span class="math">\(l = \frac{1}{12} \{ 3 \times \textrm{log}_2(0.5) \}\)</span> <span class="math">\(=\frac{1}{12}(-3) = \frac{-1}{4}\)</span><br /><span class="math">\(p=2^{\frac{1}{4}} = \sqrt[4]{2} = 1.189\;\textrm{(3dp)}\)</span></p>
<hr />
<h4 id="typical-values-of-perplexity-goodman"><a href="#typical-values-of-perplexity-goodman"> Typical values of perplexity (Goodman)</a></h4>
<ul>
<li><span class="math">\(|V| = 50,000\)</span>.</li>
<li>Trigram model, second-order Markov process, <span class="math">\(p(x_1 \dots x_n) = \prod_{i=1}^{n} q(x_i|x_{i-2},x_{i-1})\)</span> gave perplexity = 74.</li>
<li>This is vastly smaller than the vocabulary size, so this is vastly superior to the uniform distribution.</li>
<li>Bigram model, a first-order Markov process, <span class="math">\(p(x_1 \ldots x_n) = \prod_{i=1}^{n}q(x_i|x_{i-1})\)</span> gave perplexity = 137.</li>
<li>Unigram model, <span class="math">\(p(x_1 \ldots x_n) = \prod_{i=1}^{n} q(x_i)\)</span>, gave perplexity = 955.
<ul>
<li>Predicting each word without using context of previous words.</li>
</ul></li>
</ul>
<h4 id="some-history"><a href="#some-history">Some history</a></h4>
<ul>
<li>Shannon conducted experiments on entropy of English. See “Prediction and entropy of printed English”, 1951.</li>
<li>Chomsky, in “Syntactic Structures”, 1957
<ul>
<li>“Colorless green ideas sleep furiously”</li>
<li>“Furiously sleep ideas green colorless”</li>
<li>Argues probability has little to offer for semantic sense and grammatical validity.</li>
<li>Very much against Shannon’s experiments with Markov processes and language.</li>
<li>Later in the course we’ll look at PCFGs that capture long-range dependencies.</li>
</ul></li>
</ul>
<h2 id="week-1---parameter-estimation-in-language-models"><a href="#week-1---parameter-estimation-in-language-models">Week 1 - Parameter Estimation in Language Models</a></h2>
<h3 id="linear-interpolation-part-1"><a href="#linear-interpolation-part-1">Linear Interpolation (Part 1)</a></h3>
<ul>
<li>Recall the “Sparse Data Problems” section before.</li>
</ul>
<h4 id="the-bias-variance-trade-off"><a href="#the-bias-variance-trade-off">The Bias-Variance Trade-Off</a></h4>
<ul>
<li>Trigram ML estimate</li>
</ul>
<p><span class="math">\[q_{ML}(w_i\;|\;w_{i-2},w_{i-1}) = \frac{\textrm{Count}(w_{i-2},w_{i-1},w_i)}{\textrm{Count}(w_{i-2},w_{i-1})}\]</span></p>
<ul>
<li>Bigram ML estimate</li>
</ul>
<p><span class="math">\[q_{ML}(w_i\;|\;w_{i-1}) = \frac{\textrm{Count}(w_{i-1},w_i)}{\textrm{Count}(w_{i-1})}\]</span></p>
<ul>
<li>Unigram ML estimate</li>
</ul>
<p><span class="math">\[q_{ML} = \frac{\textrm{Count}(w_i)}{\textrm{Count}()}\]</span></p>
<ul>
<li>The trigram MLE’s advantage is that it conditions on a lot of context, so given sufficient training data these counts will be high and it will converge to the “true value”.
<ul>
<li>This has <strong>relatively low bias</strong>. It is able to generalise from one particular training set to other unknown data.</li>
</ul></li>
<li>The unigram MLE completely ignores context, and so it will converge to a less-good estimator as the number of training samples increases.
<ul>
<li>This has <strong>relatively high bias</strong>.</li>
</ul></li>
<li>The trigram MLE’s disadvantage is that many counts will be equal to zero, so we need many samples to get a good estimate.
<ul>
<li>This has <strong>relatively high variance</strong>. It needs far more data to be able to generalise; if it has insufficient data it will not learn / generalise.</li>
</ul></li>
<li>The unigram MLE’s count will converge relatively quickly to their expected value, and so don’t need many samples.</li>
<li>The bigram MLE is in between the trigram MLE and unigram MLE.</li>
</ul>
<h3 id="linear-interpolation-part-2"><a href="#linear-interpolation-part-2"> Linear Interpolation (Part 2)</a></h3>
<h4 id="linear-interpolation"><a href="#linear-interpolation"> Linear Interpolation</a></h4>
<ul>
<li>Take our estimate <span class="math">\(q(w_i\;|\;w_{i-2},w_{i-1})\)</span> to be:</li>
</ul>
<p><span class="math">\(= \lambda_1 \times q_{ML}(w_i\;|\;w_{i-2},w_{i-1})\)</span><br /><span class="math">\(+ \lambda_2 \times q_{ML}(w_i\;|\;w_{i-1})\)</span><br /><span class="math">\(+ \lambda_3 \times q_{ML}(w_i)\)</span></p>
<ul>
<li>where <span class="math">\(\lambda_1 + \lambda_2 + \lambda_3 = 1\)</span> and <span class="math">\(\lambda_i \ge 0\;\forall\; i\)</span>.</li>
<li>New estimate is a weighted average of the three MLEs.</li>
<li>For example, assuming <span class="math">\(\lambda_1 = \lambda_2 = \lambda_3 = \frac{1}{3}\)</span></li>
</ul>
<p><span class="math">\(q(\textrm{laughs | the, dog})\)</span><br /><span class="math">\(= \frac{1}{3} \times q_{ML}(\textrm{laughs | the, dog})\)</span><br /><span class="math">\(+ \frac{1}{3} \times q_{ML}(\textrm{laughs | dog})\)</span><br /><span class="math">\(+ \frac{1}{3} \times q_{ML}(\textrm{laughs})\)</span></p>
<hr />
<p>Quiz: we are given the following corpus:</p>
<ul>
<li>the green book STOP</li>
<li>my blue book STOP</li>
<li>his green house STOP</li>
<li>book STOP</li>
</ul>
<p>Assume we compute a language model based on this corpus using linear interpolation with <span class="math">\(\lambda_i = \frac{1}{3}\;\forall\;i \in \{1,2,3\}\)</span>.</p>
<p>What is the value of the parameter <span class="math">\(q_{LI}(\textrm{book | the, green})\)</span> in this model to 3dp? (Note: please include STOP words in your unigram model).</p>
<p><span class="math">\(q_{LI}(\textrm{book | the, green})\)</span><br /><span class="math">\(= \frac{1}{3} \times q_{ML}(\textrm{book | the, green})\)</span><br /><span class="math">\(+ \frac{1}{3} \times q_{ML}(\textrm{book | green})\)</span><br /><span class="math">\(+ \frac{1}{3} \times q_{ML}(\textrm{book})\)</span></p>
<p><span class="math">\(= \frac{1}{3} \times \frac{\textrm{Count(the, green, book)}}{\textrm{Count(the, green)}}\)</span><br /><span class="math">\(+ \frac{1}{3} \times \frac{\textrm{Count(green, book)}}{\textrm{Count(green)}}\)</span><br /><span class="math">\(+ \frac{1}{3} \times \frac{\textrm{Count(book)}}{\textrm{Count()}}\)</span></p>
<p><span class="math">\(= \frac{ \frac{1}{3}(1) }{(1)} + \frac{ \frac{1}{3}(1) }{(2)} + \frac{ \frac{1}{3}(3) }{(14)}\)</span><br /><span class="math">\(= 0.571\;\textrm(3dp)\)</span></p>
<hr />
<p>Our estimate correctly defines a distribution. Define <span class="math">\(V^{&#39;} = V \cup \{STOP\}.\)</span></p>
<p><span class="math">\(\sum_{w \in V^{&#39;}} q(w|u,v)\)</span><br /><span class="math">\(=\sum_{w \in V^{&#39;}} [\lambda_1 \times q_{ML}(w|u,v) + \lambda_2 \times q_{ML}(w|v) + \lambda_3 \times q_{ML}(w)]\)</span></p>
<p>move out the constant lambdas:</p>
<p><span class="math">\(=\lambda_1 \sum_w q_{ML}(w|u,v) + \lambda_2 \sum_w q_{ML}(w|v) + \lambda_3 \sum_w q_{ML}(w)\)</span></p>
<p>By definition the maximum likelihood estimates in a given trigram, bigram, or unigram model sum to 1. Intuitively, the probability of each given trigram, bigram, or unigram probability in the model sums to 1.</p>
<p><span class="math">\(= \lambda_1 + \lambda_2 + \lambda_3 = 1\)</span></p>
<p>(Can also show that <span class="math">\(q(w|u,v) \ge 0\;\forall\;w \in V^{&#39;}\)</span>).</p>
<hr />
<p>Quiz: say we have <span class="math">\(\lambda_1 = -0.5, \lambda_2 = 0.5, \lambda_3 = 1.0\)</span>. Note that these satisfy the constraint <span class="math">\(\sum_i \lambda_i = 1\)</span>, but violate the constraint that <span class="math">\(\lambda_i \ge 0\)</span>.</p>
<p>(Credit to <a href="https://www.coursera.org/user/i/82f8725f5c57afaa5ef1cdded36d5f1d">Philip M. Hession</a> for the explanations).</p>
<p>Recalling our definition of <span class="math">\(q\)</span> above within: <span class="math">\(\sum_{w \in V^{&#39;}} q(w|u,v)\)</span>, it’s hence true that there might be a trigram <span class="math">\(u,v,w\)</span> such that <span class="math">\(q(w|u,v) \lt 0\)</span>:</p>
<p><span class="math">\[q(\text{barks}|\text{the,dog})=-\frac{1}{2}\frac{c(\text{the,dog,barks})}{c(\text{the,dog})}+\frac{1}{2}\frac{c(\text{dog,barks})}{c(\text{dog})}+1\cdot\frac{c(\text{barks})}{c()}\]</span></p>
<ul>
<li>if <span class="math">\(c() \gg c(\text{barks})\)</span></li>
<li>and if <span class="math">\(c(\text{dog}) \gg c(\text{dog,barks})\)</span></li>
<li>and if <span class="math">\(c(\text{the,dog}) \approx c(\text{the,dog,barks})\)</span></li>
</ul>
<p>then <span class="math">\(q(\text{barks}|\text{the,dog})=-\frac{1}{2}(\sim 1)+\frac{1}{2}(\ll 1)+1(\ll 1) \lt0\)</span></p>
<p>and there might be a trigram <span class="math">\(u,v,w\)</span> such that <span class="math">\(q(w|u,v) \gt 1\)</span>:</p>
<p><span class="math">\[q(\text{barks}|\text{the,dog})=-\frac{1}{2}\frac{c(\text{the,dog,barks})}{c(\text{the,dog})}+\frac{1}{2}\frac{c(\text{dog,barks})}{c(\text{dog})}+1\cdot\frac{c(\text{barks})}{c()}\]</span></p>
<ul>
<li>if <span class="math">\(c() \approx c(\text{barks})\)</span></li>
<li>and if <span class="math">\(c(\text{dog}) \approx c(\text{dog,barks})\)</span></li>
<li>and if <span class="math">\(c(\text{the,dog}) \gg c(\text{the,dog,barks})\)</span></li>
</ul>
<p>then <span class="math">\[q(\text{barks}|\text{the,dog})=-\frac{1}{2}(\ll 1)+\frac{1}{2}(\sim 1)+1(\sim 1) \gt 1\]</span></p>
<p>It is not true that we may have a bigram <span class="math">\(u,v\)</span> such that <span class="math">\(\sum_{w \in V} q(w|u,v) \neq 1\)</span>:</p>
<p><span class="math">\[\sum_{w}q(w|u,v) = -\frac{1}{2}\frac{\sum_w c(u,v,w)}{c(u,v)}+\frac{1}{2}\frac{\sum_w c(v,w)}{c(v)}+1\cdot\frac{\sum_w c(w)}{c()} = -\frac{1}{2}(1)+\frac{1}{2}(1)+1(1)=1\]</span></p>
<p>since <span class="math">\(\sum_w c(u,v,w)=c(u,v)\)</span>, <span class="math">\(\sum_w c(v,w)=c(v)\)</span>, and <span class="math">\(\sum_w c(w)=c()\)</span>.</p>
<hr />
<h4 id="how-to-estimate-the-lambda-values"><a href="#how-to-estimate-the-lambda-values">How to estimate the <span class="math">\(\lambda\)</span> values?</a></h4>
<ul>
<li>Hold out part of the training set as “validation” data.</li>
<li>Define <span class="math">\(c^{&#39;}(w_1,w_2,w_3)\)</span> to be the number of times the trigram <span class="math">\((w_1,w_2,w_3)\)</span> is seen in the validation set.</li>
<li>Take some small portion of all of our sentences, say 5%, as validation.</li>
<li>We train on the 95% bigger portion.</li>
<li>Define <span class="math">\(c^{&#39;}\)</span> as the number of times we see the training data in the smaller, other set.</li>
<li>Choose <span class="math">\(\lambda_1, \lambda_2, \lambda_3\)</span> to maximize:</li>
</ul>
<p><span class="math">\[L(\lambda_1,\lambda_2,\lambda_3) = \sum_{w_1,w_2,w_3} c^{&#39;}(w_1,w_2,w_3)\;\textrm{log}\;q(w_3|w_1,w_2)\]</span></p>
<p>such that <span class="math">\(\lambda_1 + \lambda_2 + \lambda_3 = 1\)</span> and <span class="math">\(\lambda_i \ge 0\;\forall\;i\)</span> and where:</p>
<p><span class="math">\(q(w_i|w_{i-2},w_{i-1}) =\)</span><br /><span class="math">\(\lambda_1  \times q_{ML}(w_i|w_{i-2},w_{i-1})\)</span><br /><span class="math">\(+\lambda_2 \times q_{ML}(w_i|w_{i-1})\)</span><br /><span class="math">\(+\lambda_3 \times q_{ML}(w_i)\)</span></p>
<ul>
<li>Many of the <span class="math">\(c^{&#39;}(w_1,w_2,w_3)\)</span> counts will of course be zero.</li>
<li>Optimization problem to maximize L, under the contraints that the lambdas are positive and sum to one.</li>
<li>If you maximize L it is easy to show that you minimize the perplexity of the language model with respect to the validation data.</li>
</ul>
<h4 id="allowing-the-lambdas-to-vary"><a href="#allowing-the-lambdas-to-vary">Allowing the <span class="math">\(\lambda\)</span>’s to vary</a></h4>
<ul>
<li>Take a function <span class="math">\(\Pi\)</span> that partitions histories, e.g. for some bigram:</li>
</ul>
<p><span class="math">\[
\begin{equation}
    \Pi(w_{i-2},w_{i-1}) = \begin{cases}
        1, &amp; \textrm{If Count}(w_{i-1},w_{i-2}) = 0\\
        2, &amp; \textrm{If 1} \le \textrm{Count}(w_{i-1},w_{i-2}) \le 2\\
        3, &amp; \textrm{If 3} \le \textrm{Count}(w_{i-1},w_{i-2}) \lt 5\\
        4, &amp; \textrm{Otherwise}
    \end{cases}
\end{equation}
\]</span></p>
<ul>
<li>Introduce a dependence of the <span class="math">\(\lambda\)</span>’s on the partition:</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        q(w_i\;|\;w_{i-2},w_{i-1}) &amp; = \lambda_1^{\Pi(w_{i-2},w_{i-1})} \times q_{ML}(w_i\;|\;w_{i-2},w_{i-1}) \\
        &amp;\; + \lambda_2^{\Pi(w_{i-2},w_{i-1})} \times q_{ML}(w_i\;|\;w_{i-1}) \\
        &amp;\; + \lambda_3^{\Pi(w_{i-2},w_{i-1})} \times q_{ML}(w_i)
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>where <span class="math">\(\lambda_1^{\Pi(w_{i-2},w_{i-1})} + \lambda_2^{\Pi(w_{i-2},w_{i-1})} + \lambda_3^{\Pi(w_{i-2},w_{i-1})} = 1\)</span>, and <span class="math">\(\lambda_i^{\Pi(w_{i-2},w_{i-1})} \ge 0\;\forall\;i\)</span>.</li>
<li>Instead of just 3 lambdas now we have 3 * 4 = 12 lambdas, one per MLE per partition, and we determine which parition to use based on the bigram count.
<ul>
<li>We condition on the bigram counts.</li>
<li><span class="math">\(\lambda_1^1, \lambda_2^1, \lambda_3^1\)</span>. These counts are used if the bigram count is 0.</li>
<li><span class="math">\(\lambda_1^2, \lambda_2^2, \lambda_3^2\)</span>. These counts are used if the bigram count is [1, 2].</li>
<li><span class="math">\(\lambda_1^3, \lambda_2^3, \lambda_3^3\)</span>. These counts are used if the bigram count is [3, 5).</li>
<li><span class="math">\(\lambda_1^4, \lambda_2^4, \lambda_3^4\)</span>. These counts are used if the bigram count is [5, <span class="math">\(\infty\)</span>).</li>
</ul></li>
<li>Partitions are generally chosen by hand, but this one is a typical definition.</li>
<li>These 12 lambdas are optimized according to L as before using validation data.</li>
<li>If this bigram count is 0 then parameter <span class="math">\(\lambda_1\)</span> will also be equal to 0, else it is undefined.
<ul>
<li>Recall that <span class="math">\(\lambda_1\)</span> is for the trigram MLE, and the bigram count is in the denominator.</li>
</ul></li>
</ul>
<h3 id="discounting-methods-part-1"><a href="#discounting-methods-part-1">Discounting Methods (Part 1)</a></h3>
<ul>
<li>Suppose we have a table of bigrams, their counts, and corresponding <span class="math">\(q_{ML}(w_i\;|\;w_{i-1})\)</span>.</li>
</ul>
<table>
<thead>
<tr class="header">
<th align="left">x</th>
<th align="left">Count(x)</th>
<th align="left"><span class="math">\(q_{ML}(w_i\;|\;w_{i-1})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">the</td>
<td align="left">48</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">the, dog</td>
<td align="left">15</td>
<td align="left"><span class="math">\(^{15}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, woman</td>
<td align="left">11</td>
<td align="left"><span class="math">\(^{11}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, man</td>
<td align="left">10</td>
<td align="left"><span class="math">\(^{10}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, park</td>
<td align="left">5</td>
<td align="left"><span class="math">\(^{5}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, job</td>
<td align="left">2</td>
<td align="left"><span class="math">\(^{2}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, telescope</td>
<td align="left">1</td>
<td align="left"><span class="math">\(^{1}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, manual</td>
<td align="left">1</td>
<td align="left"><span class="math">\(^{1}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, afternoon</td>
<td align="left">1</td>
<td align="left"><span class="math">\(^{1}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, country</td>
<td align="left">1</td>
<td align="left"><span class="math">\(^{1}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, street</td>
<td align="left">1</td>
<td align="left"><span class="math">\(^{1}/_{48}\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>The MLEs are systematically high, especially if we have a large vocabulary. This is particularly true for the low count items.</li>
<li><p>In a sense these words that follow “the” are just lucky; what about those poor words that don’t appear after “the” in this data set but, in the “true” language, actually can appear after “the”?</p></li>
<li><p>Now define “discounted” counts, <span class="math">\(\textrm{Count}^{*}(x) = \textrm{Count}(x) - 0.5\)</span></p></li>
</ul>
<table>
<thead>
<tr class="header">
<th align="left">x</th>
<th align="left">Count(x)</th>
<th align="left">Count*(x)</th>
<th align="left"><span class="math">\(\frac{\textrm{Count*(x)}}{\textrm{Count(the)}}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">the</td>
<td align="left">48</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">the, dog</td>
<td align="left">15</td>
<td align="left">14.5</td>
<td align="left"><span class="math">\(^{14.5}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, woman</td>
<td align="left">11</td>
<td align="left">10.5</td>
<td align="left"><span class="math">\(^{10.5}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, man</td>
<td align="left">10</td>
<td align="left">9.5</td>
<td align="left"><span class="math">\(^{9.5}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, park</td>
<td align="left">5</td>
<td align="left">4.5</td>
<td align="left"><span class="math">\(^{4.5}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, job</td>
<td align="left">2</td>
<td align="left">1.5</td>
<td align="left"><span class="math">\(^{1.5}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, telescope</td>
<td align="left">1</td>
<td align="left">0.5</td>
<td align="left"><span class="math">\(^{0.5}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, manual</td>
<td align="left">1</td>
<td align="left">0.5</td>
<td align="left"><span class="math">\(^{0.5}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, afternoon</td>
<td align="left">1</td>
<td align="left">0.5</td>
<td align="left"><span class="math">\(^{0.5}/_{48}\)</span></td>
</tr>
<tr class="even">
<td align="left">the, country</td>
<td align="left">1</td>
<td align="left">0.5</td>
<td align="left"><span class="math">\(^{0.5}/_{48}\)</span></td>
</tr>
<tr class="odd">
<td align="left">the, street</td>
<td align="left">1</td>
<td align="left">0.5</td>
<td align="left"><span class="math">\(^{0.5}/_{48}\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li>There is some missing or left over probability mass; if we sum the right-hand column you get <span class="math">\(\frac{43}{48} \lt 1\)</span>.</li>
<li>The left over probability mass, in this case, is <span class="math">\(\frac{5}{48}\)</span>.</li>
<li><p>The essence of discounting is to take this left over probability mass and distribute it back to the words that do not appear after “the” in this data set.</p></li>
<li><p>We’ll define for any word <span class="math">\(w_{i-1}\)</span> <span class="math">\(\alpha\)</span>, which is the left-over or missing probability mass:</p></li>
</ul>
<p><span class="math">\[\alpha(w_{i-1}) = 1 - \sum_{w} \frac{\textrm{Count}^{*}(w_{i-1},w)}{\textrm{Count}(w_{i-1})}\]</span></p>
<ul>
<li>e.g. in our example, <span class="math">\(\alpha(\textrm{the}) = 10 \times 0.5/48 = 5/48\)</span>.</li>
</ul>
<hr />
<p>Quiz: assume that we are given a corpus with the following properties:</p>
<ul>
<li>Count(the) = 70</li>
<li>|{w: c(the, w) &gt; 0}| = 15, i.e. there are 15 different words that follow “the”.</li>
</ul>
<p>Furthermore assume that the discounted counts are defined as <span class="math">\(c^{*}(\textrm{the,w}) = c(\textrm{the,w}) - 0.3\)</span>. Under this corpus, what is the missing probability mass <span class="math">\(\alpha(\textrm{the})\)</span> to 3dp?</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
    \alpha(\textrm{the}) &amp; = 1 - \sum_{w} \frac{\textrm{Count}^{*}(\textrm{the, w})}{\textrm{Count(the)}} \\
    &amp; = \frac{\textrm{Count(the)}}{\textrm{Count(the)}} - \frac{1}{\textrm{Count(the)}} \times \sum_{w} \textrm{Count}^{*}(\textrm{the,w}) \\
    &amp; = \frac{\textrm{Count(the)} - \sum_{w} \textrm{Count}^{*}\textrm{(the, w)}}{\textrm{Count(the)}} \\
    &amp; = \frac{\textrm{Count(the)} - \sum_{w} \left\{ \textrm{Count(the, w)} - 0.3\right\}}{\textrm{Count(the)}} \\
    &amp; = \frac{\textrm{Count(the)} + \sum_{w}(0.3) - \textrm{Count(the)}}{\textrm{Count(the)}} \\
    &amp; = \frac{0.3w}{\textrm{Count(the)}} \\
    &amp; = \frac{(0.3)(15)}{70} = 0.064\;\textrm{(3 dp)}
    \end{aligned}
\end{align}
\]</span></p>
<hr />
<h4 id="katz-back-off-models-bigrams"><a href="#katz-back-off-models-bigrams">Katz Back-Off Models (Bigrams)</a></h4>
<ul>
<li>For a bigram model, define two sets</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        A(w_{i-1}) &amp; = \left\{w : \textrm{Count}(w_{i-1},w) \gt 0\right\} \\
        B(w_{i-1}) &amp; = \left\{w : \textrm{Count}(w_{i-1},w) = 0\right\}
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>Assuming <span class="math">\(\alpha\)</span> such that:</li>
</ul>
<p><span class="math">\[\alpha(w_{i-1}) = 1 - \sum_{w \in A(w_{i-1})} \frac{\textrm{Count}^{*}(w_{i-1},w)}{\textrm{Count}(w_{i-1})}\]</span></p>
<ul>
<li>And <span class="math">\(\textrm{Count}^{*}\)</span> is such that:</li>
</ul>
<p><span class="math">\[\textrm{Count}^{*}(w_{i-1},w_i) = \textrm{Count}(w_{i-1},w_i) - \gamma\\ \textrm{where $\gamma$ is a constant}\]</span></p>
<ul>
<li>A bigram model</li>
</ul>
<p><span class="math">\[
\begin{equation}
    q_{BO}(w_i\;|\;w_{i-1}) = \begin{cases}
        \frac{\textrm{Count}^{*}(w_{i-1},w_i)}{\textrm{Count}(w_{i-1})}, &amp; \textrm{If } w_i \in A(w_{i-1})\\
        \alpha(w_{i-1})\frac{q_{ML}(w_i)}{\sum_{w \in B(w_{i-1})} q_{ML}(w)}, &amp; \textrm{If } w_i \in B(w_{i-1}) 
    \end{cases}
\end{equation}
\]</span></p>
<ul>
<li><span class="math">\(A(w_{i-1})\)</span> is the set of words whose bigram count is greater than 0, so they follow e.g. “the”.</li>
<li><span class="math">\(B(w_{i-1})\)</span> is the set of words whose bigram count is 0, so they’re never seen to follow e.g. “the”.</li>
<li><span class="math">\(\alpha(w_{i-1})\)</span> is the missing probability mass.</li>
<li><span class="math">\(\frac{\textrm{Count}^{*}(w_{i-1},w_i)}{\textrm{Count}(w_{i-1})}\)</span> is the discounted count for the words who are seen to follow e.g. “the”.</li>
<li>If the word is never seen after e.g. “the”, rather than set its <span class="math">\(q(w_i|w_{i-1})\)</span> parameter to 0 we assign it a portion of the missing probabiliy mass <span class="math">\(\alpha(w_{i-1})\)</span>, in proportion to its the unigram maximum-likelihood estimate <span class="math">\(q_{ML}(w_i)\)</span> divided by the sum of all the unigram MLEs for other such words <span class="math">\(\sum_{w \in B(w_{i-1})} q_{ML}(w)\)</span>.</li>
</ul>
<hr />
<p>Quiz: Let’s return to a smaller version of our corpus:</p>
<ul>
<li>the book STOP</li>
<li>his house STOP</li>
</ul>
<p>This time we computer a bigram language model using Katz back-off with <span class="math">\(c^{*}(v,w) = c(v,w) - 0.5\)</span>.</p>
<p>What is the value of <span class="math">\(q_{BO}(\textrm{book | his})\)</span> estimated from this corpus?</p>
<p><span class="math">\[w_i = \textrm{book}, w_{i-1} = \textrm{his}\]</span></p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        A(\textrm{his}) &amp; = \textrm{{house}} \\
        B(\textrm{his}) &amp; = \textrm{{his, the, book, STOP}}
    \end{aligned}
\end{align}
\]</span></p>
<p>Draw a table for <span class="math">\(w_{i-1}\)</span> and all words that follow it, in order to determine <span class="math">\(\alpha(w_{i-1})\)</span></p>
<table>
<thead>
<tr class="header">
<th align="left">x</th>
<th align="left">Count(x)</th>
<th align="left">Count*(x)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">his</td>
<td align="left">1</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">his, house</td>
<td align="left">1</td>
<td align="left">0.5</td>
</tr>
</tbody>
</table>
<p><span class="math">\[\alpha(\textrm{his}) = 1 - (0.5)/(1) = 0.5\]</span></p>
<p>Since <span class="math">\(\textrm{book} \in B(\textrm{his})\)</span>, i.e. since “book” never follows “his” in the corpus:</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        \sum_{w \in B(w_{i-1})} q_{ML}(w) &amp; = q_{ML}(\textrm{his}) + q_{ML}(\textrm{the}) + q_{ML}(\textrm{book}) + q_{ML}(\textrm{STOP}) \\
        &amp; = (1/6) + (1/6) + (1/6) + (2/6) \\
        &amp; = 5/6
    \end{aligned}
\end{align}
\]</span></p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        q_{BO}(\textrm{book | his}) &amp; = \alpha(w_{i-1})\frac{q_{ML}(w_i)}{\sum_{w \in B(w_{i-1})} q_{ML}(w)} \\
        &amp; = (0.5) \times \frac{(1/6)}{(5/6)} \\
        &amp; = 0.1
    \end{aligned}
\end{align}
\]</span></p>
<hr />
<h3 id="discounting-methods-part-2"><a href="#discounting-methods-part-2">Discounting Methods (Part 2)</a></h3>
<h4 id="katz-back-off-models-trigrams"><a href="#katz-back-off-models-trigrams">Katz Back-Off Models (Trigrams)</a></h4>
<ul>
<li>For a trigram model, first define two sets</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        A(w_{i-2},w_{i-1}) &amp; = \left\{w : \textrm{Count}(w_{i-2},w_{i-1},w) \gt 0\right\} \\
        B(w_{i-2},w_{i-1}) &amp; = \left\{w : \textrm{Count}(w_{i-2},w_{i-1},w) = 0\right\}
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>A trigram model is defined in terms of the bigram model:</li>
</ul>
<p><span class="math">\[
\begin{equation}
    q_{BO}(w_i\;|\;w_{i-2},w_{i-1}) = \begin{cases}
        \frac{\textrm{Count}^{*}(w_{i-2},w_{i-1},w_i)}{\textrm{Count}(w_{i-2},w_{i-1})}, &amp; \textrm{If } w_i \in A(w_{i-2},w_{i-1})\\
        \alpha(w_{i-2},w_{i-1})\frac{q_{BO}(w_i|w_{i-1})}{\sum_{w \in B(w_{i-2},w_{i-1})} q_{BO}(w|w_{i-1})}, &amp; \textrm{If } w_i \in B(w_{i-2},w_{i-1}) 
    \end{cases}
\end{equation}
\]</span></p>
<p>where</p>
<p><span class="math">\[\alpha(w_{i-2},w_{i-1}) = 1 - \sum_{w \in A(w_{i-2},w_{i-1})} \frac{\textrm{Count}^{*}(w_{i-2},w_{i-1},w)}{\textrm{Count}(w_{i-2},w_{i-1})}\]</span></p>
<ul>
<li>The one variable is the discount constant. It is typically between 0 and 1, and it can also be chosen via optimization on a validation data set.</li>
</ul>
<h3 id="summary"><a href="#summary"> Summary</a></h3>
<ul>
<li>Three steps in deriving the language model probabilities:
<ol style="list-style-type: decimal">
<li>Expand <span class="math">\(p(w_1, w_2, \ldots, w_n)\)</span> using <em>Chain Rule</em>.</li>
<li>Make <em>Markov Independence Assumptions</em>, i.e. <span class="math">\(p(w_i\;|\;w_1, w_2, \ldots, w_{i-2}, w_{i-1}) = p(w_i\;|\;w_{i-2},w_{i-1})\)</span></li>
<li><em>Smooth</em> the estimates using low order counts; linear interpolation and discounting.</li>
</ol></li>
<li>Other methods used to improve language models
<ul>
<li>“Topic” or “long-range” features.
<ul>
<li>Condition on the topic of the document within which sentences belong.</li>
<li>Condition on words outside of the two-word window under the second-order Markov assumption.</li>
</ul></li>
<li>Syntactic models
<ul>
<li>Grammatical information.</li>
</ul></li>
</ul></li>
<li>It’s generally hard to improve on trigram models though!</li>
</ul>
<h2 id="week-2---tagging-problems-and-hidden-markov-models"><a href="#week-2---tagging-problems-and-hidden-markov-models"> Week 2 - Tagging Problems and Hidden Markov Models</a></h2>
<h3 id="the-tagging-problem"><a href="#the-tagging-problem">The Tagging Problem</a></h3>
<h4 id="part-of-speech-tagging"><a href="#part-of-speech-tagging"> Part-of-Speech Tagging</a></h4>
<ul>
<li>We’d like to model <em>pairs of sequences</em>, rather than just one sequence.
<ul>
<li>The general problem is the <strong>sequence labelling problem</strong>, aka the <strong>tagging problem</strong>.</li>
</ul></li>
<li><p>Two instances of this problem - POS tagging and named entity recognition.</p></li>
<li><strong>Part-of-Speech Tagging</strong>: a fundamental problem.
<ul>
<li>Input: sentence.</li>
<li>Output: a tag sequence, aka a state sequence.</li>
</ul></li>
<li><p>Input, some sequence of words, a sentence:</p></li>
</ul>
<pre><code>Profits soared at Boeing Co., easily topping forecasts on Wall
Street, as their CEO Alan Nulally announced first quarter
results.</code></pre>
<ul>
<li>Tags:</li>
</ul>
<pre><code>N   =   Noun
V   =   Verb
P   =   Preposition
Adv =   Adverb
Adj =   Adjective
...</code></pre>
<ul>
<li>Output, a <em>tag sequence</em>:</li>
</ul>
<pre><code>Profits/N soared/V at/P Boeing/N Co./N ,/, easily/ADV
topping/V forecasts/N on/P Wall/N Street/N ,/, as/P
their/POSS CEO/N Alan/N Mulally/N announced/V first/ADJ
quarter/N results/N ./.</code></pre>
<ul>
<li>But context matters.
<ul>
<li><code>profits</code> isn’t always a noun, it can sometimes be a verb.</li>
<li><code>topping</code> is a verb, but can sometimes be a noun.</li>
<li>…</li>
</ul></li>
<li>Also, individual words, regardless of context, have a preference for their part of speech.
<ul>
<li><code>quarter</code> can be a noun or a verb, but in general is more likely to be a noun.</li>
</ul></li>
<li>Also some words are very rare, and may not show up in the training data.
<ul>
<li>Important to be able to deal with this.</li>
</ul></li>
</ul>
<h4 id="named-entity-recognition"><a href="#named-entity-recognition"> Named Entity Recognition</a></h4>
<ul>
<li><strong>Named Entity Recognition</strong>
<ul>
<li>Input: a sentence.</li>
<li>Output: identify names and their type (company, location, person, …)</li>
</ul></li>
<li>Input: same as above</li>
<li>Output:</li>
</ul>
<pre><code>Profits soared at [Company: Boeing Co.], easily ...
[Location: Wall Street], ..., [Person: Alan Mulally]</code></pre>
<ul>
<li>At first blush named entity recognition looks like segmentation, not part-of-speech tagging. But really they’re the same.</li>
</ul>
<h4 id="named-entity-extraction-as-tagging"><a href="#named-entity-extraction-as-tagging"> Named Entity Extraction as Tagging</a></h4>
<ul>
<li>Input: same as above</li>
<li>Tags:</li>
</ul>
<pre><code>NA  =   No entity
SC  =   Start Company
CC  =   Continue Company
SL  =   Start Location
CL  =   Continue Location
...</code></pre>
<ul>
<li>Output:</li>
</ul>
<pre><code>Profits/NA soared/NA at/NA Boeing/SC Co./CC ,/NA easily/NA
topping/NA ... Wall/SL Street/CL ,/NA ... CEO/NA Alan/SP
Mulally/CP ...</code></pre>
<ul>
<li>We are <em>encoding</em> the named entity boundaries as a tag sequence.</li>
</ul>
<hr />
<p>Quiz: given sentence: <code>Profits are topping all estimates</code>.</p>
<p>We also know:</p>
<ul>
<li><code>Profits</code> can be N or V.</li>
<li><code>are</code> is V</li>
<li><code>topping</code> can be N, ADJ, or V.</li>
<li><code>all</code> can be DT, ADV, or N.</li>
<li><code>estimates</code> can be N or V.</li>
</ul>
<p>How many tag sequences are possible?</p>
<p><span class="math">\[= 2 \times 1 \times 3 \times 3 \times 2 = 36\]</span></p>
<hr />
<ul>
<li>Objective: treating this like a supervised machine learning problem
<ul>
<li>Use a very common resource, called the “Wall Street Journal Treebank”.</li>
<li>Features: sentences (not individual words).</li>
<li>Training set: 38,219 sentences, each with tagged words.
<ul>
<li>Annotated by hand (!)</li>
</ul></li>
<li>Label: a sentence with each word tagged.</li>
<li>!!AI there are a lot of tags here. A reference list of tags is available in the <a href="http://bulba.sdsu.edu/jeanette/thesis/PennTags.html">Penn Treebank Tags</a>.</li>
<li>Output: a functon that maps sentences to tagged words.</li>
</ul></li>
<li>There are now many corpora available, across many languages.</li>
</ul>
<h4 id="two-types-of-contraints"><a href="#two-types-of-contraints">Two Types of Contraints</a></h4>
<pre><code>Influential/JJ members/NNS of/IN ... bailout/NN agency/NN
can/MD raise/VB capital/NN ./.</code></pre>
<ul>
<li>What will help us in this problem? Two constraints:
<ol style="list-style-type: decimal">
<li><strong>Local</strong>: e.g. <em>can</em> is more likely to be a modal verb (MD) than a noun (NN).
<ul>
<li>A <a href="http://en.wikipedia.org/wiki/Modal_verb">modal verb</a> (MD) is an auxillary verb used to indicate likelihood, ability, permission, and obligation.</li>
</ul></li>
<li><strong>Contextual</strong>: e.g. a noun (NN) is more likely than a verb (VB*) to follow a determiner (DT).
<ul>
<li>(e.g. <code>the can</code> is more likely to refer to a can of soup than talk about <code>the</code>’s ability to do something)</li>
<li>A <a href="http://en.wikipedia.org/wiki/Determiner">determiner</a> (DT) is a word, phrase, or affix that occurs together with a noun (NN).</li>
<li>DT can be indefinite articles (<code>the</code>, <code>a</code>, <code>an</code>), demonstratives (<code>this</code>, <code>that</code>), quantifiers (<code>many</code>, <code>few</code>, <code>several</code>).</li>
<li>Recall that an affix is a morpheme that attaches to word stems. Can be prefix, suffix, infix (in the middle of a word) or circumfix (on both sides of the word)</li>
</ul></li>
</ol></li>
<li>Sometimes the contraints are in conflict:</li>
</ul>
<pre><code>The trash can is in the garage.</code></pre>
<ul>
<li><code>can</code> has a <em>local</em> preference to be a modal verb (MD) because it follows a noun.</li>
<li>But clearly <code>can</code> belongs as a whole with <code>trash can</code>, so it depends on <em>context</em>.</li>
<li>We can build a model that balances these two contraints.</li>
</ul>
<h3 id="generative-models-for-supervised-learning"><a href="#generative-models-for-supervised-learning">Generative Models for Supervised Learning</a></h3>
<h4 id="supervised-learning-problems"><a href="#supervised-learning-problems">Supervised Learning Problems</a></h4>
<ul>
<li>We have training examples <span class="math">\(x^{(i)}, y^{(i)}\)</span> for <span class="math">\(i = 1 \ldots m\)</span>.</li>
<li>Each <span class="math">\(x^{(i)}\)</span> is an <strong>input</strong>, each <span class="math">\(y^{(i)}\)</span> is a <strong>label</strong>.</li>
<li>Objective: learn a function <span class="math">\(f\)</span> that maps inputs <span class="math">\(x\)</span> to labels <span class="math">\(f(x)\)</span>.</li>
<li>e.g.</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        &amp; x^{(1)} = \textrm{the dog laughs}, &amp; y^{(1)} = \textrm{DT NN VB} \\
        &amp; x^{(2)} = \textrm{the dog barks}, &amp; y^{(2)} = \textrm{DT NN VB} \\
        &amp; \ldots &amp; \ldots
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>The first model you may consider is a <strong>conditional model</strong>.
<ul>
<li>Learn a distribution <span class="math">\(p(y|x)\)</span> from training examples.</li>
<li>For any test input <span class="math">\(x\)</span>, define <span class="math">\(f(x) = \textrm{arg max}_{y}p(y|x)\)</span>.
<ul>
<li>The <span class="math">\(y\)</span> that maximizes this conditional probability.</li>
<li>Input <span class="math">\(x\)</span>, search through all possible <span class="math">\(y\)</span>’s, return most likely <span class="math">\(y\)</span>.</li>
</ul></li>
</ul></li>
<li>Alternative are generative models.</li>
</ul>
<h4 id="generative-models"><a href="#generative-models"> Generative Models</a></h4>
<ul>
<li>Same problem.</li>
<li>Learn a <em>joint distribution</em> <span class="math">\(p(x,y)\)</span> from training examples.
<ul>
<li>Before we had <span class="math">\(p(y|x)\)</span>.</li>
</ul></li>
<li>Often we have <span class="math">\(p(x,y)\)</span> = <span class="math">\(p(y)p(x|y)\)</span>.
<ul>
<li><strong>Bayes Rule</strong>.</li>
<li><span class="math">\(p(y)\)</span> is the <strong>prior</strong> probability; how likely is <span class="math">\(y\)</span> a-priori?</li>
<li><span class="math">\(p(x|y)\)</span> is the <strong>conditional</strong> probability. <em>Given</em> <span class="math">\(y\)</span> how likely is <span class="math">\(x\)</span>?</li>
</ul></li>
<li>Note: by the total probability variant of Bayes Rule we have:</li>
</ul>
<p><span class="math">\[p(y|x) = \frac{p(y)p(x|y)}{p(x)}\]</span></p>
<ul>
<li>where:</li>
</ul>
<p><span class="math">\[p(x) = \sum_y p(y)p(x|y)\]</span></p>
<ul>
<li>Estimating <span class="math">\(p(y|x)\)</span> <em>directly</em> is often referred to as a <strong>discriminative model</strong>.
<ul>
<li>We will see a lot of discriminative models later in the course.</li>
</ul></li>
<li>Estimating <span class="math">\(p(x,y)\)</span> is a <strong>generative model</strong>.</li>
<li>There are pros and cons to each, a lot of research, back and forth.</li>
<li>Still confused about generative vs. discriminative? <a href="http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm">http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm</a>
<ul>
<li>A generative algorithm takes into account some model about how the data was generated, and then classifies the input.
<ul>
<li>By calculating <span class="math">\(p(x,y)\)</span> we have enough to shove it into Bayes’ rule in order to generate <span class="math">\(p(y|x)\)</span>.</li>
<li>However, if we want, we can generate more <span class="math">\((x,y)\)</span> that fit the model.</li>
</ul></li>
<li>A discriminative model doesn’t care and just classifies.
<ul>
<li>Given some input it discerns what’s necessary to map it onto the most likely output.</li>
</ul></li>
</ul></li>
<li>How do we apply a generative model to a new test example?</li>
<li>Output from the model:</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        f(x) &amp; = \textrm{argmax}_{y}\;p(y|x) \\
             &amp; = \textrm{argmax}_{y}\;\frac{p(y)p(x|y)}{p(x)} \\
             &amp; = \textrm{argmax}_{y}\;p(y)p(x|y)
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>Second line: assuming we have a generative model, by Bayes Rule.</li>
<li>Third line: <span class="math">\(p(x)\)</span> does not vary with <span class="math">\(y\)</span>. <span class="math">\(\textrm{argmax}\)</span> implies we’re searching over <span class="math">\(y\)</span>, but denominator is constant and hence we can discard it.
<ul>
<li>This is computationally very useful, can be expensive to calculate.</li>
</ul></li>
<li>Models that decompose the joint probability <span class="math">\(p(x,y)\)</span> into <span class="math">\(p(y)\)</span> and <span class="math">\(p(x|y)\)</span> are called <strong>noisy-channel models</strong>.
<ul>
<li>Intuitively, the input <span class="math">\(x\)</span> is generated in two steps.
<ol style="list-style-type: decimal">
<li>Label <span class="math">\(y\)</span> is chosen with probability <span class="math">\(p(y)\)</span>.</li>
<li>Input <span class="math">\(x\)</span> is generated from the distribution <span class="math">\(p(x|y)\)</span>.</li>
</ol></li>
</ul></li>
</ul>
<h3 id="hidden-markov-models"><a href="#hidden-markov-models">Hidden Markov Models</a></h3>
<ul>
<li>We have an input sentence <span class="math">\(x = x_1, x_2, \ldots, x_n\)</span>. (<span class="math">\(x_i\)</span> is the <span class="math">\(i\)</span>’th word in the sentence).</li>
<li>We have a tag sequence <span class="math">\(y = y_1, y_2, \ldots, y_n\)</span>. (<span class="math">\(y_i\)</span> is the <span class="math">\(i\)</span>’th tag in the sentence).</li>
<li>We’ll use an HMM to define:</li>
</ul>
<p><span class="math">\[p(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_n)\]</span></p>
<ul>
<li>for any sentence <span class="math">\(x_1 \ldots x_n\)</span> and tag sequence <span class="math">\(y_1 \ldots y_n\)</span> of the same length.
<ul>
<li>Note this is <strong>generative</strong> (<span class="math">\(p(x,y)\)</span>), not <strong>discriminative</strong> (<span class="math">\(p(y|x)\)</span>).</li>
<li>Think of the <span class="math">\(x_i\)</span> as an input and the <span class="math">\(y_i\)</span> as a label.</li>
</ul></li>
<li>Then the most likely tag sequence for <span class="math">\(x\)</span> is:</li>
</ul>
<p><span class="math">\[\textrm{arg}\underset{y_1 \ldots y_n}{\textrm{max}} p(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_n)\]</span></p>
<ul>
<li>The number of total possible sequences is <span class="math">\(O(2^n)\)</span>, so brute force search is not feasible.</li>
</ul>
<h4 id="trigram-hidden-markov-models-triagram-hmms"><a href="#trigram-hidden-markov-models-triagram-hmms">Trigram Hidden Markov Models (Triagram HMMs)</a></h4>
<ul>
<li>For any sentence <span class="math">\(x_1, x_2, \ldots, x_n\)</span>, where <span class="math">\(x_i \in V\)</span> for <span class="math">\(i = 1, 2, \ldots, n\)</span>, and</li>
<li>For any tag sequence <span class="math">\(y_1, y_2, \ldots, y_{n+1}\)</span>, where <span class="math">\(y_i \in S\)</span> for <span class="math">\(i = 1, 2, \ldots, n\)</span> and <span class="math">\(y_{n+1} = \textrm{STOP}\)</span>.</li>
<li>The joint probability of the sentence and tag sequence is:</li>
</ul>
<p><span class="math">\[p(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_{n+1}) = \prod_{i=1}^{n+1} q(y_i|y_{i-2},y_{i-1}) \prod_{i=1}^{n} e(x_i|y_i)\]</span></p>
<ul>
<li>An example of the joint probability could be <span class="math">\(p(\textrm{the, dog barks, DT, NN, VB, STOP})\)</span>.</li>
<li>The first product is a trigram model applied to tag sequences! Very similar to before.
<ul>
<li>One <span class="math">\(q\)</span> term for each tag <em>including the STOP symbol</em>.</li>
</ul></li>
<li>The second product could have e.g. <span class="math">\(e(\textrm{the | DT})\)</span> is the probability of a tag emitting or generating a word.
<ul>
<li>One <span class="math">\(e\)</span> term for each (tagged) word.</li>
</ul></li>
<li>where we’ve assumed, as before in Markov Models, that <span class="math">\(y_0 = y_{-1} = {*}\)</span> (the start symbol).</li>
<li><span class="math">\(V\)</span> is the set of possible words in the language, e.g. <span class="math">\(\{\textrm{the, dog, book, ate, his}\}\)</span></li>
<li><span class="math">\(S\)</span> is the set of possible tags, e.g. <span class="math">\(\{\textrm{DT, NN, VB, P, ADV, ...}\}\)</span>.
<ul>
<li><span class="math">\(\simeq\)</span> hundreds of tags; the Wall Street Journal courpus has <span class="math">\(\simeq\)</span> 50 tags.</li>
</ul></li>
<li>Parameters of the model:
<ul>
<li><span class="math">\(q(s|u,v)\;\forall\;s \in S \cup \{\textrm{STOP}\},\;u,v \in S \cup \{\textrm{*}\}\)</span>
<ul>
<li><strong>Trigram parameters</strong> (but referred to in a quiz as <strong>transition parameters</strong>).</li>
</ul></li>
<li><span class="math">\(e(x|s)\;\forall\;s \in S, x \in V\)</span>
<ul>
<li><strong>Emission parameters</strong>.</li>
</ul></li>
</ul></li>
<li>This model has the same form as a noisy-channel model.
<ul>
<li>The first <span class="math">\(q\)</span> parameters are the prior probability of the tags, i.e. <span class="math">\(p(y)\)</span>.</li>
<li>The second <span class="math">\(e\)</span> parameters are the conditional probabilities, i.e. <span class="math">\(p(x|y)\)</span>.</li>
</ul></li>
<li>Notice that the <span class="math">\(e\)</span> parameters have an independence assumption.
<ul>
<li>Any value for random variable <span class="math">\(X_i = x_i\)</span> is only dependent on <span class="math">\(Y_i = y_i\)</span>.</li>
<li>More formally given the value of <span class="math">\(Y_i\)</span> the value for <span class="math">\(X_i\)</span> is conditionally independent of both previous observations <span class="math">\(X_1 \ldots X_{i-1}\)</span> and other state values <span class="math">\(Y_1 \ldots Y_{i-1}, Y_{i+1}, \ldots Y_{n+1}\)</span>.</li>
<li>See notes p12.</li>
</ul></li>
<li>Useful thinking exercise - how do I generate sequence pairs <span class="math">\(y_1, \ldots, y_{n+1}, x_1, \ldots, x_n\)</span>?
<ol style="list-style-type: decimal">
<li>Initialize <span class="math">\(i=1\)</span> and <span class="math">\(y_0 = y_{-1} = \textrm{*}\)</span>.</li>
<li>Generate <span class="math">\(y_i\)</span> from distribution <span class="math">\(q(y_i|y_{i-2},y_{i-1})\)</span>.</li>
<li>If <span class="math">\(y_i = \textrm{STOP}\)</span> then return <span class="math">\(y_1 \ldots y_i, x_1 \ldots x_{i-1}\)</span>. Else, generate <span class="math">\(x\)</span> from distribution <span class="math">\(e(x_i|y_i)\)</span>.</li>
<li>Set <span class="math">\(i=i+1\)</span>, return to step 2.</li>
</ol></li>
</ul>
<hr />
<p>Quiz: Given tagset <span class="math">\(S = \{\textrm{D, N}\}\)</span>, a vocabulary <span class="math">\(V = \{\textrm{the, dog}\}\)</span>, and a HMM with transition parameters:</p>
<ul>
<li><span class="math">\(q(\textrm{D | *, *}) = 1\)</span></li>
<li><span class="math">\(q(\textrm{N | *, D}) = 1\)</span></li>
<li><span class="math">\(q(\textrm{STOP | D, N}) = 1\)</span></li>
<li><span class="math">\(q(s|u,v) = 0\)</span> for all other <span class="math">\(q\)</span> params.</li>
</ul>
<p>and emission parameters:</p>
<ul>
<li><span class="math">\(e(\textrm{the | D}) = 0.9\)</span></li>
<li><span class="math">\(e(\textrm{dog | D}) = 0.1\)</span></li>
<li><span class="math">\(e(\textrm{dog | N}) = 1\)</span></li>
</ul>
<p>Under this model how many pairs of sequences <span class="math">\(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_{n+1}\)</span> satisfy <span class="math">\(p(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_{n+1}) \gt 0\)</span>?</p>
<p>First: how many non-zero-probability tag sequences are there? Enumerate them by drawing a graph of nodes and edges, where a node is a word and an edge is labelled with the transition probability to another word. Then follow all paths from any start symbol to any stop symbol whose product of probabilities is <span class="math">\(\gt\)</span> 0.</p>
<pre><code>D, N, STOP</code></pre>
<p>There’s only one! OK. Refer back to your taq sequence graph and copy it for each possible word that a given tag (i.e. node) that it may “generate”. If e.g. N could generate two words, not one, we would have <em>four</em> possible sentences.</p>
<pre><code>the dog
dog dog</code></pre>
<p>There’s only two! OK. Hence the answer itself is two, because we have just generated a sentence for each possible (tag, word) pair.</p>
<hr />
<h4 id="an-example"><a href="#an-example">An example</a></h4>
<p>If we have:</p>
<ul>
<li><span class="math">\(n = 3\)</span>,</li>
<li>The sentence <span class="math">\(\{x_1, x_2, x_3\} = \{\textrm{the, dog, laughs}\}\)</span>, and</li>
<li>The tag sequence <span class="math">\(\{y_1, y_2, y_3, y_4\} = \{\textrm{D, N, V, STOP}\}\)</span>.</li>
</ul>
<p>Then:</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        &amp; p(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_{n+1}) \\
      = &amp; q(\textrm{D | *, *}) \times q(\textrm{N | *, D}) \times q(\textrm{V | D, N}) \times q(\textrm{STOP | N, V}) \times \\
        &amp; e(\textrm{the | D}) \times e(\textrm{dog | N}) \times e(\textrm{laughs | V})
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>STOP is a special tag that terminates the sequence.</li>
<li>We take <span class="math">\(y_0 = y_{-1} = \textrm{*}\)</span>, where <span class="math">\(\textrm{*}\)</span> is a special “padding” symbol.</li>
<li>The <span class="math">\(e\)</span> parameters can be interpreted as the conditional probability <span class="math">\(p(\textrm{the dog laughs | D N V STOP})\)</span>.</li>
</ul>
<hr />
<p>Quiz: given set <span class="math">\(S = \{\textrm{D, N, V}\}\)</span>, and vocabulary <span class="math">\(V = \{\textrm{the, cat, drinks, milk, dog}\}\)</span>, and an HMM model:</p>
<ul>
<li>transition parameters <span class="math">\(q(s|u,v) = \frac{1}{4}\;\forall\;s, u, v\)</span></li>
<li>generative parameters <span class="math">\(e(x|s) = \frac{1}{5}\;\forall\; \textrm{tags}\;s\;\textrm{and words}\;x\)</span>.</li>
</ul>
<p>What is the value, under this model, of:</p>
<p><span class="math">\[p(\textrm{the, cat, drinks, milk, D, N, V, N, STOP})\]</span></p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        &amp; p(\textrm{the, cat, drinks, milk, STOP, D, N, V, N}) \\
      = &amp; \prod_{i=1}^{n+1} q(y_i|y_{i-2},y_{i-1}) \prod_{i=1}^{n} e(x_i|y_i) \\
      = &amp; \{ p(\textrm{the | *, *}) \times p(\textrm{cat | *, the}) \times p(\textrm{drinks | the, cat}) \times p(\textrm{milk | cat, drinks}) \times p(\textrm{STOP | drinks, milk}) \} \times \\
        &amp; e(\textrm{the | D}) \times e(\textrm{cat | N}) \times e(\textrm{drinks | V}) \times e(\textrm{milk | N}) \\
      = &amp; \left(\frac{1}{4}\right)^5 \times \left(\frac{1}{5}\right)^4
    \end{aligned}
\end{align}
\]</span></p>
<hr />
<h4 id="why-the-name"><a href="#why-the-name">Why the Name?</a></h4>
<ul>
<li>The first product is a <strong>second-order Markov Chain</strong>
<ul>
<li>Recall <span class="math">\(p(x,y) = p(y) \times p(x|y)\)</span></li>
<li>This product is solving for <span class="math">\(p(y)\)</span>.</li>
</ul></li>
<li>The second project is <span class="math">\(x_j\)</span>’s <strong>being observed</strong>.
<ul>
<li>Strong independence assumption that each word depends only on its underlying, generating tag.</li>
</ul></li>
<li>The generative process: we choose a sequence of tags, and then for each tag generate an associated word.
<ul>
<li>The <span class="math">\(y\)</span>’s are <em>not observed</em>.</li>
<li>The <span class="math">\(x\)</span>’s are <em>observed</em>.</li>
</ul></li>
<li>And so we will flip this: given an observation find the most likely underlying (<strong>hidden</strong>) tag sequence.</li>
</ul>
<hr />
<p>Quiz: for a bigram HMM:</p>
<p><span class="math">\[p(x_1, x_2, \ldots, x_n, y_1, y_2, \ldots, y_n) = \prod_{i=1}^{n+1} q(y_i|y_{i-1}) \prod_{i=1}^{n} e(x_i|y_i)\]</span></p>
<hr />
<h3 id="parameter-estimation-in-hmms"><a href="#parameter-estimation-in-hmms">Parameter Estimation in HMMs</a></h3>
<h4 id="smoothed-estimation"><a href="#smoothed-estimation">Smoothed Estimation</a></h4>
<p>e.g.</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        q(\textrm{Vt | DT, JJ}) &amp; = \lambda_1 \times \frac{\textrm{Count(Dt, JJ, Vt)}}{\textrm{Count(Dt, JJ)}} \\
                                &amp; + \lambda_2 \times \frac{\textrm{Count(JJ, Vt)}}{\textrm{Count(JJ)}} \\
                                &amp; + \lambda_3 \times \frac{\textrm{Count(Vt)}}{\textrm{Count()}}
    \end{aligned}
\end{align}
\]</span></p>
<p><span class="math">\[\lambda_1 + \lambda_2 + \lambda_3 = 1\]</span> <span class="math">\[\forall\;i, \lambda_i \ge 0\]</span></p>
<p><span class="math">\[e(\textrm{base | Vt}) = \frac{\textrm{Count(Vt, base)}}{\textrm{Count(Vt)}}\]</span></p>
<ul>
<li>For trigram / transition parameters:
<ul>
<li>We can of course induce counts of tag sequences directly from our corpus, and then determine <strong>maximum-likelihood estimates</strong>.
<ul>
<li><span class="math">\(\lambda_1\)</span> for <strong>trigram MLE</strong>.</li>
<li><span class="math">\(\lambda_2\)</span> for <strong>bigram MLE</strong>.</li>
<li><span class="math">\(\lambda_3\)</span> for <strong>unigram MLE</strong>.</li>
</ul></li>
<li>Linear interpolation is used, as seen before.</li>
</ul></li>
<li>For emission parameters:
<ul>
<li>Can use <strong>bigram MLEs</strong>.</li>
</ul></li>
<li>One problem.</li>
<li><span class="math">\(e(x|y) = 0\;\forall\;y\)</span> if <span class="math">\(x\)</span> is never seen in the training data.
<ul>
<li>!!AI sounds familiar! Will we do Laplacian smoothing, we we “add fudge” to everything, or back-off smoothing, where high mass gets re-distributed to zero mass, or something else?</li>
</ul></li>
</ul>
<hr />
<p>Quiz: Given the following corpus:</p>
<ul>
<li>the dog barks -&gt; D N V STOP</li>
<li>the cat sings -&gt; D N V STOP</li>
</ul>
<p>Assume we’ve calculated MLEs of a trigram HMM from this data. What is the value of the emission parameter <span class="math">\(e(\textrm{cat | N})\)</span> from this HMM?</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        e(\textrm{cat | N}) = &amp; \frac{\textrm{Count(N, cat)}}{\textrm{Count(N)}} \\
                            = &amp; \frac{(1)}{(2)}
    \end{aligned}
\end{align}
\]</span></p>
<p>Say we estimate the transition parameters for a trigram HMM using linear interpolation, such that <span class="math">\(\lambda_i = \frac{1}{3}\)</span> for <span class="math">\(i = \{1, 2, 3\}\)</span>. What is the value of the transition parameter <span class="math">\(q(\textrm{STOP | N, V})\)</span> under this model?</p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        q(\textrm{STOP | N, V}) = &amp; \lambda_1 \times \frac{\textrm{Count(N, V, STOP)}}{\textrm{Count(N, V)}} \\
                                + &amp; \lambda_2 \times \frac{\textrm{Count(V, STOP)}}{\textrm{Count(V)}} \\
                                + &amp; \lambda_3 \times \frac{\textrm{Count(STOP)}}{\textrm{Count()}} \\
                                = &amp; \left(\frac{1}{3} \times \frac{(2)}{(2)}\right) \\
                                + &amp; \left(\frac{1}{3} \times \frac{(2)}{(2)}\right) \\
                                + &amp; \left(\frac{1}{3} \times \frac{(2)}{(8)}\right) \\
                                = &amp; 0.75
    \end{aligned}
\end{align}
\]</span></p>
<hr />
<h4 id="dealing-with-low-frequency-words-an-example"><a href="#dealing-with-low-frequency-words-an-example">Dealing with Low-Frequency Words: An Example</a></h4>
<ul>
<li>Test sentence</li>
</ul>
<pre><code>Profits soared at Boeing Co., easily topping ...
CEO Alan Mulally.</code></pre>
<ul>
<li><code>topping</code> and <code>Mulally</code> are likely to be infrequent.</li>
<li>Long tail: you will frequently encounter words in test data that you have never encountered in training data.</li>
<li>And hence: <span class="math">\(e(\textrm{Mulally | y}) = 0\)</span> for all tags <span class="math">\(y\)</span>.</li>
<li>And it can be verified that the joint probability <span class="math">\(p(x_1, \ldots, x_n, y_1, \ldots, y_{n+1}) = 0\)</span> for all tag sequences <span class="math">\(y_1, \ldots, y_{n+1}\)</span>.</li>
<li>This is because all tag sequences will involve this emission parameter.</li>
<li><p>And hence all tag sequences are equally likely; applying argmax to an expression that <em>always</em> evaluates to zero implies that <span class="math">\(y\)</span> is equally maximum everywhere!</p></li>
<li>A common way of dealing with this:
<ol style="list-style-type: decimal">
<li><strong>Split the vocabulary into two sets</strong>.
<ul>
<li><em>Frequent words</em>: words occurring <span class="math">\(\ge\)</span> 5 times in training (or some threshold).</li>
<li><em>Low frequency words</em>: all other words.</li>
</ul></li>
<li><strong>Map</strong> low frequency words into a small, finite set, depending on affixes.</li>
</ol></li>
<li>The set of low frequency words is very large.</li>
<li><p>Map each low frequency word to a small set of e.g. 20 new words.</p></li>
<li><p>from [Bikel et. al 1999] for named-entity recognition.</p></li>
</ul>
<table>
<thead>
<tr class="header">
<th align="left">Word class</th>
<th align="left">Example</th>
<th align="left">Intuition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">twoDigitNum</td>
<td align="left">90</td>
<td align="left">Two digit year</td>
</tr>
<tr class="even">
<td align="left">fourDigitNum</td>
<td align="left">1990</td>
<td align="left">Four digit year</td>
</tr>
<tr class="odd">
<td align="left">containsDigitAndAlpha</td>
<td align="left">A8956-67</td>
<td align="left">Product code</td>
</tr>
<tr class="even">
<td align="left">containsDigitAndDash</td>
<td align="left">09-96</td>
<td align="left">Date</td>
</tr>
<tr class="odd">
<td align="left">containsDigitAndSlash</td>
<td align="left">11/9/89</td>
<td align="left">Date</td>
</tr>
<tr class="even">
<td align="left">containsDigitAndComma</td>
<td align="left">23,000.00</td>
<td align="left">Monetary amount</td>
</tr>
<tr class="odd">
<td align="left">containsDigitAndPeriod</td>
<td align="left">1.00</td>
<td align="left">Monetary, financial</td>
</tr>
<tr class="even">
<td align="left">othernum</td>
<td align="left">456789</td>
<td align="left">Other</td>
</tr>
<tr class="odd">
<td align="left">allCaps</td>
<td align="left">BBN</td>
<td align="left">Organization</td>
</tr>
<tr class="even">
<td align="left">capsPeriod</td>
<td align="left">M.</td>
<td align="left">Initial</td>
</tr>
<tr class="odd">
<td align="left">firstWord</td>
<td align="left">first</td>
<td align="left">no useful capitalisation infomation</td>
</tr>
<tr class="even">
<td align="left">initCap</td>
<td align="left">Sally</td>
<td align="left">Capitalized word</td>
</tr>
<tr class="odd">
<td align="left">lowercase</td>
<td align="left">can</td>
<td align="left">Uncapitalized word</td>
</tr>
<tr class="even">
<td align="left">other</td>
<td align="left">,</td>
<td align="left">Punctuation, other words</td>
</tr>
</tbody>
</table>
<ul>
<li>These were chosen by hand with intuition.</li>
<li>We want to preserve some useful information for the specific task at hand, i.e. named entity recognition.</li>
<li>e.g. <code>firstWord</code> will be capitalized in the corpus, but we lowercase it because the capitalization does not give us useful information, because all words at the start of a sentence are capitalized.</li>
<li>We’re mapping low-frequency words to classes that preserve spelling features.</li>
</ul>
<p>Return to an old example. Before transformation:</p>
<pre><code>Profits/NA soared/NA at/NA Boeing/SC Co./CC easily/NA
topping/NA forecasts/NA on/NA Wall/SL Street/CL ,/NA their/NA
CEO/NA Alan/SP Mulally/CP announced/NA first/NA quarter/NA
results/NA ./NA</code></pre>
<p>After transformation:</p>
<pre><code>firstword/NA soared/NA at/NA initCap/SC Co./CC ,/NA easily/NA
lowercase/NA forecasts/NA on/NA initCap/SL Street/CL ,/NA as/NA
their/NA CEO/NA Alan/SP initCap/CP announced/NA first/NA
quarter/NA results/NA ./NA</code></pre>
<ul>
<li>Resolving low-frequency words in a way that preserves their spelling is useful for the named-entity recognition problem.</li>
<li>Build our HMM on this transformed data.
<ul>
<li><span class="math">\(e(\textrm{firstword | NA})\)</span></li>
<li><span class="math">\(e(\textrm{initCap | SC})\)</span></li>
</ul></li>
<li>We’re <strong>closing</strong> the vocabulary.</li>
<li>This is a simple method, but requires human heuristics.</li>
</ul>
<h3 id="the-viterbi-algorithm-for-hmms"><a href="#the-viterbi-algorithm-for-hmms">The Viterbi Algorithm for HMMs</a></h3>
<ul>
<li>How to apply HMMs to new test sentences?</li>
</ul>
<h4 id="problem"><a href="#problem">Problem</a></h4>
<ul>
<li>For a <em>new</em> test input sentence <span class="math">\(x_1, \ldots, x_n\)</span>, map it onto the most likely set of tags, i.e. find:</li>
</ul>
<p><span class="math">\[\textrm{arg}\underset{y_1 \dots y_{n+1}}{\textrm{max}} p(x_1 \ldots x_n, y_1 \ldots y_{n+1})\]</span></p>
<ul>
<li><p>where the arg max is taken over all sequences <span class="math">\(y_1 \ldots y_{n+1}\)</span> such that <span class="math">\(y_i \in S\)</span> for <span class="math">\(i = 1, \ldots, n\)</span> and <span class="math">\(y_{n+1} = \textrm{STOP}\)</span>.</p></li>
<li><p>We assume that <span class="math">\(p\)</span> again takes the form:</p></li>
</ul>
<p><span class="math">\[p(x_1 \ldots x_n, y_1 \ldots y_{n+1}) = \prod_{i=1}^{n+1} q(y_i | y_{i-2}, y_{i-1}) \prod_{i=1}^{n} e(x_i | y_i)\]</span></p>
<ul>
<li><p>Recall the assumptions that <span class="math">\(y_0 = y_{-1} = \textrm{*}\)</span> and <span class="math">\(y_{n+1} = \textrm{STOP}\)</span>.</p></li>
<li>!!AI from a practical perspective the product of many small numbers will rapidly become unrepresentable on a machine.</li>
<li>Logarithms come up again and again in machine learning because it is a <strong>monotonic increasing</strong> function with some <strong>very useful rules</strong>.
<ul>
<li><a href="http://en.wikipedia.org/wiki/Monotonic_function">http://en.wikipedia.org/wiki/Monotonic_function</a></li>
<li>A monotically increasing function <span class="math">\(f(x)\)</span> is such that for all <span class="math">\(x, y\)</span> such that <span class="math">\(x \le y\)</span> the following is always true: <span class="math">\(f(x) \le f(y)\)</span>.</li>
<li>Colloquially, <span class="math">\(f\)</span> <em>preserves order</em>.</li>
</ul></li>
<li>Also recall that <span class="math">\(\textrm{log}(a \times b) = \textrm{log}(a) + \textrm{log}(b)\)</span>.</li>
<li>Hence if we apply logarithms to everything in <span class="math">\(p\)</span> we can not only <strong>add instead of multiply</strong> but also <strong>retain</strong> the ability to calculate argmax over the y’s, i.e. determine the most likely tag sequence.</li>
<li><p>To clarify what I mean by <strong>retain</strong>:</p></li>
</ul>
<p><span class="math">\[\textrm{arg}\underset{y_1 \dots y_{n+1}}{\textrm{max}} p(x_1 \ldots x_n, y_1 \ldots y_{n+1}) = \textrm{arg}\underset{y_1 \dots y_{n+1}}{\textrm{max}} \textrm{log} \left\{ p(x_1 \ldots x_n, y_1 \ldots y_{n+1}) \right\}\]</span></p>
<ul>
<li>And to clarify what I mean by <strong>add instead of multiply</strong>:</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        p(x_1 \ldots x_n, y_1 \ldots y_{n+1}) &amp; = \prod_{i=1}^{n+1} q(y_i | y_{i-2}, y_{i-1}) \prod_{i=1}^{n} e(x_i | y_i) \\
        \textrm{log} \left\{ p(x_1 \ldots x_n, y_1 \ldots y_{n+1}) \right\} &amp; = \textrm{log} \left\{ \prod_{i=1}^{n+1} q(y_i | y_{i-2}, y_{i-1}) \prod_{i=1}^{n} e(x_i | y_i) \right\} \\
        &amp; = \textrm{log} \left\{ \prod_{i=1}^{n+1} q(y_i | y_{i-2}, y_{i-1}) \right\} + \textrm{log} \left\{ \prod_{i=1}^{n} e(x_i | y_i) \right\}
    \end{aligned}
\end{align}
\]</span></p>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        &amp; = \textrm{log} \left\{ q(y_1|y_{-1},y_0) \times q(y_2|y_0,y_1) \times \ldots \times q(y_{n+1}|y_{n-1},y_{n}) \right\} \\
        &amp; + \textrm{log} \left\{ e(x_0|y_0) \times e(x_1|y_1) \times \ldots \times e(x_n|y_n) \right\} \\
        &amp; = \textrm{log} \left\{ q(y_1|y_{-1},y_0) \right\} + \textrm{log} \left\{ q(y_2|y_0,y_1) \right\} + \ldots + \textrm{log} \left\{ q(y_{n+1}|y_{n-1},y_{n}) \right\} \\
        &amp; + \textrm{log} \left\{ e(x_0|y_0) \right\} + \textrm{log} \left\{ e(x_1|y_1) \right\} + \ldots + \textrm{log} \left\{ e(x_n|y_n) \right\}
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>To be absolutely clear: it is irrelevant what base you use in the logarithm.
<ul>
<li>In this course I think we’re expected to use base 2.</li>
<li>In the ARPA file format of backoff language models base 10 is used.</li>
<li>But use whatever you want! <strong>Just don’t forget</strong> which one you used ;).</li>
</ul></li>
</ul>
<h4 id="brute-force-search-is-hopelessly-inefficient"><a href="#brute-force-search-is-hopelessly-inefficient">Brute Force Search is Hopelessly Inefficient</a></h4>
<ul>
<li>For example
<ul>
<li><span class="math">\(x_1 \ldots x_n = \{\textrm{the, dog, laughs}\}\)</span>.</li>
<li><span class="math">\(y_1 \ldots y_n = \{\textrm{D, N, V}\}\)</span> (the correct answer).</li>
<li><span class="math">\(S = \{\textrm{D, N, V}\}\)</span> (assume that the set of all possible tags is just this).</li>
</ul></li>
<li>So <span class="math">\(|S| = 3\)</span>, and all possible tag sequences are all combinations (<em>not</em> permutations):
<ul>
<li>D D D STOP</li>
<li>D D N STOP</li>
<li>D D U STOP</li>
<li>D U D STOP</li>
<li>…</li>
</ul></li>
<li>Only <span class="math">\(3^3 = 27\)</span> possible tag sequences.</li>
<li>Use the transmission and emissions parameters of the HMM model to assign probabilities to each particular tag sequnce, then choose the most likely tag sequence.</li>
<li><p>However, in the general case <span class="math">\(|S|^n\)</span>, where <span class="math">\(n\)</span> is sentence length, is the number of possible sequences.</p></li>
<li>The transmission parameters only depend on sequences of length three for trigram HMMs.
<ul>
<li>This structure allows a more efficient solution.</li>
</ul></li>
</ul>
<h3 id="the-viterbi-algorithm"><a href="#the-viterbi-algorithm">The Viterbi Algorithm</a></h3>
<ul>
<li>Define <span class="math">\(n\)</span> to be length of sentence.</li>
<li>Define <span class="math">\(S_k\)</span> for <span class="math">\(k = -1, 0, \ldots, n\)</span>, to be set of possible tags at position <span class="math">\(k\)</span>:</li>
</ul>
<p><span class="math">\[S_{-1} = S_0 = \{\textrm{*}\}\]</span> <span class="math">\[S_k = S\;\textrm{for}\;k \in \{1, 2, \ldots n\}\]</span></p>
<ul>
<li>Define:</li>
</ul>
<p><span class="math">\[r(y_{-1}, y_0, y_1, \ldots, y_k) = \prod_{i=1}^{k} q(y_i | y_{i-2}, y_{i-1}) \prod_{i=1}^{k} e(x_i|y_i)\]</span></p>
<ul>
<li>Note that, always, <span class="math">\(y_{-1} = y_0 = \{\textrm{*}\}\)</span>.</li>
<li>This is a truncated <span class="math">\(q\)</span>, as it only goes <span class="math">\(i=1\)</span> to <span class="math">\(k\)</span>.</li>
<li>Define a dynamic programming table</li>
</ul>
<p><span class="math">\[\pi(k,u,v) = \textrm{maximum probability of a tag sequence ending in tags}\;u, v\;\textrm{at position}\;k\]</span></p>
<p>i.e.</p>
<p><span class="math">\[\pi(k,u,v) = max_{(y_{-1},y_0,y_{1},\ldots,y_k):y_{k-1}=u,\;y_k=v} r(y_{-1},y_0,y_1,\ldots,y_k)\]</span></p>
<ul>
<li><span class="math">\(k\)</span> takes any value <span class="math">\(\{\textrm{1,2,...,n}\}\)</span>.</li>
<li><span class="math">\(u \in S_{k-1}\)</span>.</li>
<li><p><span class="math">\(v \in S_k\)</span>.</p></li>
<li>What do the <span class="math">\(S\)</span> and <span class="math">\(k\)</span> expressions at the begining imply:
<ul>
<li>For example, (the, dog, laughs, D, N, V) implies <span class="math">\(k = 3\)</span>.</li>
<li>Each tag in <span class="math">\(S\)</span> could be responsible for generating a word in <span class="math">\(x\)</span>.
<ul>
<li>If <span class="math">\(S = \{\textrm{D, N, V, P}\}\)</span>, then <span class="math">\(x_1\)</span> could be one of D, N, V, P, as is <span class="math">\(x_2\)</span>, etc.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="an-example-1"><a href="#an-example-1">An Example</a></h4>
<p><span class="math">\[\underset{-1}{\textrm{*}}\;\underset{0}{\textrm{*}}\;\underset{1}{\textrm{The}}\;\underset{2}{\textrm{man}}\;\underset{3}{\textrm{saw}}\;\underset{4}{\textrm{the}}\;\underset{5}{\textrm{dog}}\;\underset{6}{\textrm{with}}\;\underset{7}{\textrm{the}}\;\underset{8}{\textrm{telescope}}\;\]</span></p>
<ul>
<li>Assume <span class="math">\(S = \{\textrm{D, N, V, P}\}\)</span></li>
<li>What does <span class="math">\(\pi(7, \textrm{P}, \textrm{D})\)</span> mean, intuitively?
<ul>
<li>The probability of the most likely tag sequence ending at the word in position 7 such that the last two tags are (P, D).</li>
<li>Fix ‘with’ (6) to P.</li>
<li>Fix ‘the’ (7) with D.</li>
<li>Each preceding word has four possible tags.
<ul>
<li>‘dog’ (5) could be D, N, V, P.</li>
<li>‘the’ (4) could be D, N, V, P.</li>
<li>‘saw’ (3) could be D, N, V, P.</li>
<li>‘man’ (2) could be D, N, V, P.</li>
<li>‘The’ (1) could be D, N, V, P.</li>
</ul></li>
</ul></li>
</ul>
<hr />
<p>Quiz: We have a trigram HMM model with the following transition parameters:</p>
<ul>
<li><span class="math">\(q(\textrm{D | *, *}) = 1\)</span></li>
<li><span class="math">\(q(\textrm{N | *, D}) = 1\)</span></li>
<li><span class="math">\(q(\textrm{V | D, N}) = 1\)</span></li>
<li><span class="math">\(q(\textrm{STOP | N, V}) = 1\)</span></li>
</ul>
<p>and emission parameters:</p>
<ul>
<li><span class="math">\(e(\textrm{the | D}) = 0.8\)</span></li>
<li><span class="math">\(e(\textrm{dog | D}) = 0.2\)</span></li>
<li><span class="math">\(e(\textrm{dog | N}) = 0.8\)</span></li>
<li><span class="math">\(e(\textrm{the | N}) = 0.2\)</span></li>
<li><span class="math">\(e(\textrm{barks | V}) = 1.0\)</span></li>
</ul>
<p>Say we have the sentence:</p>
<pre><code>the dog barks</code></pre>
<p>What is the value of <span class="math">\(\pi(3, \textrm{N}, \textrm{V})\)</span>?</p>
<ul>
<li>Intuitively, this reads as ‘what is the probability of the most likely tag sequence that ends at position 3 such that the last two tags are N and V?’</li>
<li>First, expand and label your test sentence, omitting the STOP symbol:</li>
</ul>
<p><span class="math">\[\underset{-1}{\textrm{*}}\;\underset{0}{\textrm{*}}\;\underset{1}{\textrm{the}}\;\underset{2}{\textrm{dog}}\;\underset{3}{\textrm{barks}}\]</span></p>
<ul>
<li>Draw a Markov Chain graph of your transmission parameters, covering every single possible path.
<ul>
<li>Think of every tag as a node (including the start symbols), and an edge as moving from one tag to another with a certain probability.</li>
<li>In our case this is very easy; there is only one path, i.e. <span class="math">\(\textrm{* -&gt; * -&gt; D -&gt; N -&gt; V -&gt; STOP}\)</span>, with probabilities of <span class="math">\(1\)</span> for each edge.</li>
</ul></li>
<li>Eliminate all paths from the Markov Chain graph that do not meet the constraints of <span class="math">\(\pi(3,\textrm{N},\textrm{V})\)</span>. Also eliminate any paths that contain an edge with zero probability.</li>
<li>For us, we only have one path, and this path meets the contraints of this function.</li>
<li><p>Prove this to yourself by putting one finger on the start of the test sentence, and one finger on the start of the Markon Chain graph, and counting until <span class="math">\(k=3\)</span>.</p></li>
<li>Your Markov Chain graph now covers every possible combination of tags that <em>could</em> match this test sentence. For each path calculate the product of probabilities from a start symbol to <span class="math">\(k=3\)</span>. Determine which path gives you the highest probability.</li>
<li>In our case there is only one path, so the <strong>most likely tag sequence</strong> is (D, N, V).</li>
<li>This gives us the <span class="math">\(q\)</span> part of the <span class="math">\(r\)</span> expression.</li>
<li>For this tag sequence use the emission parameters to “generate” the appropriate word in order to calculate the <span class="math">\(e\)</span> parameters.</li>
<li><p>Mathematically:</p></li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        r(y_{-1},y_0,y_1,\ldots,y_n) &amp; = \prod_{i=1}^{k} q(y_i|y_{i-2},y_{i-1}) \prod_{i=1}^{k} e(x_i|y_i) \\
        r(\textrm{*, *, D, N, V}) &amp; = \left\{ q(\textrm{D | *, *}) \times q(\textrm{N | *, D}) \times q(\textrm{V | D, N}) \right\} \times \\
        &amp; \left\{ e(\textrm{the | D}) \times e(\textrm{dog | N}) \times e(\textrm{barks | V}) \right\} \\
        &amp; = \left\{ 1 \times 1 \times 1\right\} \times \left\{0.8 \times 0.8 \times 1.0 \right\} \\
        &amp; = 0.64
    \end{aligned}
\end{align}
\]</span></p>
<hr />
<h4 id="a-recursive-definition"><a href="#a-recursive-definition">A Recursive Definition</a></h4>
<ul>
<li>Base case: <span class="math">\(\pi(0, \textrm{*}, \textrm{*}) = 1\)</span>
<ul>
<li>Every tag sequence starts with <span class="math">\(\textrm{* *}\)</span>.</li>
</ul></li>
<li><strong>Recursive definition</strong>: <span class="math">\(\forall\; k \in \{1 \ldots n\},\;\forall\; u \in S_{k-1}\;\textrm{and}\;v \in S_k:\)</span></li>
</ul>
<p><span class="math">\[\pi(k,u,v) = \underset{w \in S_{k-2}}{\textrm{max}} (\pi(k-1),w,u) \times q(v|w,u) \times e(x_k|v))\]</span></p>
<ul>
<li><span class="math">\(u\)</span> can take any tag in <span class="math">\(S_{k-1}\)</span>, <span class="math">\(v\)</span> can take any tag in <span class="math">\(S_k\)</span>.</li>
<li>Notice how we’re working backwards in the sentence back to the base case, the start.</li>
</ul>
<h4 id="justification-for-the-recursive-definition"><a href="#justification-for-the-recursive-definition">Justification for the Recursive Definition</a></h4>
<p>(part 2)</p>
<p><span class="math">\[\underset{-1}{\textrm{*}}\;\underset{0}{\textrm{*}}\;\underset{1}{\textrm{The}}\;\underset{2}{\textrm{man}}\;\underset{3}{\textrm{saw}}\;\underset{4}{\textrm{the}}\;\underset{5}{\textrm{dog}}\;\underset{6}{\textrm{with}}\;\underset{7}{\textrm{the}}\;\underset{8}{\textrm{telescope}}\;\]</span></p>
<p>What is <span class="math">\(\pi(7, P, D)\)</span>?</p>
<ul>
<li>Recall this puts ‘with’ (6) = P, ‘the’ (7) = D.</li>
<li><span class="math">\(u = \textrm{P}, v = \textrm{D}\)</span></li>
<li>Note that <span class="math">\(S_5 = S_4 = \ldots = S = \{\textrm{D, N, V, P}\}\)</span>.</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        \pi(7, \textrm{P}, \textrm{D}) = &amp; \underset{w \in \{\textrm{D,N,V,P}\}}{\textrm{max}} \left\{ \pi(6, w, \textrm{P}) \times q(\textrm{D} | w, \textrm{P}) \times e(\textrm{the} | \textrm{D}) \right\}
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>Any tag sequence ending in (P, D) must have included one previous tag in (D, N, V, P). The ‘max’ explicitly searches over these.</li>
</ul>
<hr />
<p>Quiz: assume <span class="math">\(S = \{\textrm{D, N, V, P}\}\)</span> and a trigram HMM with parameters:</p>
<ul>
<li><span class="math">\(q(\textrm{D | N, P}) = 0.4\)</span></li>
<li><span class="math">\(q(\textrm{D | w, P}) = 0\)</span> for <span class="math">\(w \neq N\)</span>.</li>
<li><span class="math">\(e(\textrm{the | D}) = 0.6\)</span></li>
</ul>
<p>We are also given the sentence:</p>
<pre><code>Ella walks to the red house</code></pre>
<p>Say the dynamic programming table for this sentence has the following entries:</p>
<ul>
<li><span class="math">\(\pi(\textrm{3, D, P}) = 0.1\)</span></li>
<li><span class="math">\(\pi(\textrm{3, N, P}) = 0.2\)</span></li>
<li><span class="math">\(\pi(\textrm{3, V, P}) = 0.01\)</span></li>
<li><span class="math">\(\pi(\textrm{3, P, P}) = 0.5\)</span></li>
</ul>
<p>What is the value of <span class="math">\(\pi(\textrm{4, P, D})\)</span>?</p>
<ul>
<li><span class="math">\(u = \textrm{P}\)</span></li>
<li><span class="math">\(v = \textrm{D}\)</span></li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        \pi(k,u,v) = &amp; \underset{w \in S_{k-2}}{\textrm{max}} (\pi(k-1),w,u) \times q(v|w,u) \times e(x_k|v)) \\
        \pi(4, \textrm{P}, \textrm{D}) = &amp; \underset{w \in \{\textrm{D, N, V, P}\}}{\textrm{max}} \left\{ \pi(3, w, \textrm{P}) \times q(\textrm{D} | w, \textrm{P}) \times e(\textrm{the | D}) \right\} \\
        = &amp; \textrm{max} \left\{ 0.1 \times 0 \times 0.6, 0.2 \times 0.4 \times 0.6, 0.01 \times 0 \times 0.6, 0.5 \times 0 \times 0.6 \right\} \\
        = &amp; 0.048
    \end{aligned}
\end{align}
\]</span></p>
<hr />
<h3 id="the-viterbi-algorithm-1"><a href="#the-viterbi-algorithm-1">The Viterbi Algorithm</a></h3>
<ul>
<li><strong>Inputs</strong>:
<ul>
<li>a sentence <span class="math">\(x_1 \ldots x_n\)</span>, a sequence of words</li>
<li>transmisson parameters <span class="math">\(q(s|u,v)\)</span>,</li>
<li>emission parameters <span class="math">\(e(x|s)\)</span>.</li>
</ul></li>
<li><strong>Output</strong>:
<ul>
<li><span class="math">\(\underset{y_1 \ldots y_{n+1}}{\textrm{max}} p(x_1 \ldots x_n, y_1 \ldots y_{n+1})\)</span></li>
<li>Notice this is <em>not argmax</em>; just returns max probability. A simple change later will fix this.</li>
</ul></li>
<li><strong>Initializtion</strong>:
<ul>
<li>Set <span class="math">\(\pi(0,\textrm{*},\textrm{*}) = 1\)</span>.
<ul>
<li>Base case of the recursion.</li>
</ul></li>
</ul></li>
<li><strong>Definition</strong>:
<ul>
<li><span class="math">\(S_{-1} = S_0 = \{\textrm{*}\}\)</span>
<ul>
<li>Can only have the star symbols at positions -1 and 0.</li>
</ul></li>
<li><span class="math">\(S_k = S\;\forall\;k \in \{1 \ldots n\}\)</span>
<ul>
<li>Recall e.g. {D, N, V, P}</li>
</ul></li>
</ul></li>
<li><strong>Algorithm</strong>
<ul>
<li>For <span class="math">\(k = 1 \ldots n\)</span>:
<ul>
<li>For <span class="math">\(u \in S_{k-1}\)</span>, <span class="math">\(v \in S_k\)</span>:
<ul>
<li><span class="math">\(\pi(k,u,v) = \underset{w \in S_{k-2}}{\textrm{max}} (\pi(k-1,w,u) \times q(v|w,u) \times e(x_k|v))\)</span></li>
</ul></li>
</ul></li>
<li><strong>Return</strong>: <span class="math">\(\textrm{max}_{u \in S_{n-1},v \in S_n} (\pi(n,u,v) \times q(\textrm{STOP}|u,v))\)</span></li>
</ul></li>
</ul>
<h4 id="the-viterbi-algorithm-with-backpointers"><a href="#the-viterbi-algorithm-with-backpointers">The Viterbi Algorithm with Backpointers</a></h4>
<p>We want ‘argmax’, not ‘max’, i.e. the actual most-likely tag sequence.</p>
<ul>
<li><strong>Inputs</strong>:
<ul>
<li>a sentence <span class="math">\(x_1 \ldots x_n\)</span>, a sequence of words</li>
<li>transmisson parameters <span class="math">\(q(s|u,v)\)</span>,</li>
<li>emission parameters <span class="math">\(e(x|s)\)</span>.</li>
</ul></li>
<li><strong>Output</strong>:
<ul>
<li><span class="math">\(\textrm{arg}\underset{y_1 \ldots y_{n+1}}{\textrm{max}} p(x_1 \ldots x_n, y_1 \ldots y_{n+1})\)</span></li>
</ul></li>
<li><strong>Initialization</strong>:
<ul>
<li>Set <span class="math">\(\pi(0,\textrm{*},\textrm{*}) = 1\)</span>.</li>
</ul></li>
<li><strong>Definition</strong>:
<ul>
<li><span class="math">\(S_{-1} = S_0 = \{\textrm{*}\}\)</span></li>
<li><span class="math">\(S_k = S\;\forall\;k \in \{1 \ldots n\}\)</span><br /></li>
</ul></li>
<li><strong>Algorithm</strong>
<ul>
<li>For <span class="math">\(k = 1 \ldots n\)</span>:
<ul>
<li>For <span class="math">\(u \in S_{k-1}\)</span>, <span class="math">\(v \in S_k\)</span>:
<ul>
<li><span class="math">\(\pi(k,u,v) = \underset{w \in S_{k-2}}{\textrm{max}} (\pi(k-1,w,u) \times q(v|w,u) \times e(x_k|v))\)</span></li>
<li><span class="math">\(bp(k,u,v) = arg \underset{w \in S_{k-2}}{max} (\pi(k-1,w,u) \times q(v|w,u) \times e(x_k|v))\)</span></li>
</ul></li>
</ul></li>
<li>Set <span class="math">\((y_{n-1},y_n) = \textrm{argmax}_{(u,v)} (\pi(n,u,v) \times q(\textrm{STOP}|u,v))\)</span></li>
<li>For <span class="math">\(k = (n-2) \ldots 1\)</span>, <span class="math">\(y_k = bp(k+1, y_{k+1}, y_{k+2})\)</span></li>
<li><strong>Return</strong> the tag sequence <span class="math">\(y_1 \ldots y_n\)</span>.</li>
</ul></li>
<li>What is different?
<ul>
<li>Don’t just record <span class="math">\(\pi\)</span> at each point but also a backpointer <span class="math">\(bp\)</span>; which tag achieved this max. Which tag is most likely at <span class="math">\(k\)</span> given <span class="math">\(u,v\)</span>.</li>
<li>We then have <span class="math">\(\pi\)</span> and <span class="math">\(bp\)</span> values.</li>
<li>At the end we go backwards in the sequence.</li>
</ul></li>
<li>Run-time complexity is <span class="math">\(O(n \times |S|^3)\)</span>.
<ul>
<li>We enter the <span class="math">\(u,v\)</span> loop <span class="math">\(n \times |S|^2\)</span> times.</li>
<li>Each time we enter we need to search over <span class="math">\(|S|\)</span> possible tags.</li>
<li>It is <strong>linear</strong> with respect to sentence length.</li>
<li>Much better than brute force, which was <span class="math">\(O(|S|^n)\)</span>.</li>
</ul></li>
</ul>
<h3 id="summary-1"><a href="#summary-1">Summary</a></h3>
<ul>
<li>HMM taggers are <strong>very simple to train</strong>.
<ul>
<li>Just compile counts from training corpus, calculate MLEs.</li>
</ul></li>
<li>Perform relatively well, over 90% on named entity recognition.</li>
<li>Main difficulty is modelling: e(word|tag), especially if words are low-frequency.
<ul>
<li>One approach is to group low-frequency words into classes, but very clumsy and heuristic.</li>
<li>When words are complex even worse.</li>
<li>Later in the course we develop more complex methods.</li>
</ul></li>
</ul>
<h2 id="week-3---parsing-and-context-free-grammars-cfgs"><a href="#week-3---parsing-and-context-free-grammars-cfgs">Week 3 - Parsing, and Context-Free Grammars (CFGs)</a></h2>
<h3 id="parsing-syntatic-structure"><a href="#parsing-syntatic-structure"> Parsing (Syntatic Structure)</a></h3>
<ul>
<li>Input: a sentence, e.g. “Boeing is located in Seattle”.</li>
<li>Output: a parse tree.
<ul>
<li><em>leaves</em> of tree: words.</li>
<li><em>internal nodes</em>: labels (e.g. S, NP, VP, …)<br /></li>
</ul></li>
<li>Hierarchical decomposition.</li>
</ul>
<h3 id="syntactic-formalisms"><a href="#syntactic-formalisms">Syntactic Formalisms</a></h3>
<ul>
<li>Work in formal syntax goes back to Chomsky’s PhD thesis in 1950s.
<ul>
<li>Syntactic Structures, Chomsky, 1957</li>
</ul></li>
<li>More syntactic formalisms:
<ul>
<li>Minimalism</li>
<li>Lexical functional grammar (LFG)</li>
<li>Head-driven phrase-structure grammar (HPSG)</li>
<li>Tree adjoining grammars (TAG)</li>
<li>Categorical grammars.</li>
</ul></li>
<li>But we are going to use <em>context-free grammars</em> (CFGs), which forms the basis of other formalisms.</li>
<li>We will (again) treat parsing as a supervised learning problem.
<ul>
<li>Penn Wall Street Journal Treebank.</li>
<li>50k sentences with associated trees, annotated by hands.</li>
<li><strong>Treebank</strong>: set of sentences with associated parse trees.</li>
</ul></li>
</ul>
<h3 id="the-information-conveyed-by-parse-trees"><a href="#the-information-conveyed-by-parse-trees">The Information Conveyed By Parse Trees</a></h3>
<ul>
<li>“The burglar robbed the apartment”</li>
<li><strong>First level</strong>: <em>part-of-speech tags</em> for each word.
<ul>
<li>(N = noun, V = verb, DT = determiner).</li>
<li>DT -&gt; the</li>
<li>N -&gt; burglar</li>
<li>V -&gt; robbed</li>
<li>DT -&gt; the</li>
<li>N -&gt; apartment</li>
</ul></li>
<li><strong>Second level</strong>: <em>phrases</em>.
<ul>
<li>(NP = noun phrases, VP = verb phrases, S = sentences)</li>
<li>“the/DT burglar/N” is dominated by an internal node “NP”.
<ul>
<li>This means the two tagged words are grouped as a noun phrase.</li>
</ul></li>
<li>“robbed the apartment” is a VP.
<ul>
<li>robbed/V.</li>
<li>an NP
<ul>
<li>the/DT apartment/N.</li>
</ul></li>
</ul></li>
<li>“the burglar robbed the apartment” is an S.</li>
</ul></li>
<li><strong>Third level</strong>: useful relationships
<ul>
<li>subject to verb.
<ul>
<li>NP then VP -&gt; V.</li>
<li>“the/DT burglar/N”/NP is the subject of “robbed”/V, by looking at fragments of the tree.</li>
</ul></li>
<li>verb to direct object.
<ul>
<li>VP to V and NP.</li>
<li>This is verb to direct object.</li>
<li>“robbed”/V “the/DT apartment/N”/NP.</li>
</ul></li>
</ul></li>
</ul>
<h3 id="an-example-application-machine-translation"><a href="#an-example-application-machine-translation">An Example Application: Machine Translation</a></h3>
<ul>
<li>English word order: subject -&gt; verb -&gt; object.</li>
<li><p>Japanese word order: subject -&gt; object -&gt; verb.</p></li>
<li>English: IBM bought Lotus</li>
<li><p>Japanese: IBM Lotus bought.</p></li>
<li>English: Sources said that IBM bought Lotus yesterday.</li>
<li><p>Japanese: Sources yesterday IBM Lotus bought that said.</p></li>
<li>The reordering has been applied <em>recursively</em>.</li>
<li><p>Such reording is difficult to see in a sentence, but consists of <strong>rotations</strong> in the parse tree (swapping the order).</p></li>
</ul>
<h3 id="context-free-grammars"><a href="#context-free-grammars">Context-Free Grammars</a></h3>
<ul>
<li>Hopcroft and Ullman, 1979</li>
<li>A CFG <span class="math">\(G = (N, \Sigma, R, S)\)</span>, where
<ul>
<li><span class="math">\(N\)</span> is a finite set of <strong>non-terminal</strong> symbols.</li>
<li><span class="math">\(\Sigma\)</span> is a finite set of <strong>terminal</strong> symbols.</li>
<li><span class="math">\(R\)</span> is a finite set of <strong>rules</strong> of the form <span class="math">\(X \rightarrow Y_1 Y_2 \ldots Y_n\)</span> for <span class="math">\(n \ge 0\)</span>, <span class="math">\(X \in N\)</span>, <span class="math">\(Y_i \in (N \cup \Sigma)\)</span>.
<ul>
<li>Each rule’s left hand side must be a non-terminal.</li>
<li>Each rule’s right hand side may be “empty” ($), a non-terminal, or terminal, or both.</li>
</ul></li>
<li><span class="math">\(S \in N\)</span> is a distinguished <strong>start symbol</strong>.</li>
</ul></li>
</ul>
<h3 id="example-cfg-for-english"><a href="#example-cfg-for-english">Example CFG for English</a></h3>
<ul>
<li><span class="math">\(N\)</span> = {S, NP, VP, PP, DT, Vi, Vt, NN, I}</li>
<li><span class="math">\(S\)</span> = S</li>
<li><span class="math">\(\Sigma\)</span> = {sleeps, saw, man, woman, telescope, the, with in}.</li>
<li><span class="math">\(R\)</span>:
<ul>
<li>S -&gt; NP VP</li>
<li>VP -&gt; Vi</li>
<li>VP -&gt; Vt NP</li>
<li>VP -&gt; VP PP</li>
<li>NP -&gt; DT NN</li>
<li>NP -&gt; NP PP</li>
<li>PP -&gt; IN NP</li>
<li>Vi -&gt; sleeps</li>
<li>Vt -&gt; saw</li>
<li>NN -&gt; man</li>
<li>NN -&gt; woman</li>
<li>NN -&gt; telescope</li>
<li>DT -&gt; the</li>
<li>IN -&gt; with</li>
<li>IN -&gt; in</li>
</ul></li>
<li>Symbol meanings:
<ul>
<li>S -&gt; sentence</li>
<li>VP -&gt; verb phrase</li>
<li>NP -&gt; noun phrase</li>
<li>PP -&gt; prepositional phrase</li>
<li>DT -&gt; determiner</li>
<li>Vi -&gt; intransitive verb</li>
<li>Vt -&gt; transitive verb</li>
<li>NN -&gt; noun</li>
<li>IN -&gt; preposition</li>
</ul></li>
</ul>
<h3 id="left-most-derivations"><a href="#left-most-derivations">Left-Most Derivations</a></h3>
<ul>
<li>A <strong>derivation</strong> is a sequence of strings <span class="math">\(s_1 \ldots s_n\)</span> such that:
<ul>
<li><span class="math">\(s_1 = S\)</span>; first symbol is the start symbol</li>
<li><span class="math">\(s_n \in \Sigma^{*}\)</span>, i.e. <span class="math">\(s_n\)</span> is made up of terminal symbols only such that it is the set of all possible strings.</li>
<li>If <span class="math">\(\Sigma\)</span> = {the, dog, a}, then <span class="math">\(\Sigma^{*}\)</span> = {<span class="math">\(\epsilon\)</span>, a, dog, the, a dog, the dog, …}.</li>
</ul></li>
<li>A <strong>left-most derivation</strong> is such that:
<ul>
<li>Each <span class="math">\(s_i\)</span> for <span class="math">\(i = 2, \ldots, n\)</span> is derived from <span class="math">\(s_{i-1}\)</span> by picking the <strong>left-most non-terminal</strong> <span class="math">\(X\)</span> in <span class="math">\(s_{i-1}\)</span> and replacing it by some <span class="math">\(\beta\)</span> where <span class="math">\(X \rightarrow \beta\)</span> is a rule in <span class="math">\(R\)</span>.</li>
</ul></li>
<li>For example, “the man sleeps”.</li>
<li>[S], [NP VP], [D N VP], [the N VP], [the man VP], [the man Vi], [the man sleeps]</li>
<li>We “pop as left as possible”.</li>
</ul>
<h3 id="an-example-2"><a href="#an-example-2"> An Example</a></h3>
<table>
<thead>
<tr class="header">
<th align="left">Derivation</th>
<th align="left">Rules Used</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">S</td>
<td align="left">S -&gt; NP VP</td>
</tr>
<tr class="even">
<td align="left">NP VP</td>
<td align="left">NP -&gt; DT N</td>
</tr>
<tr class="odd">
<td align="left">DT N VP</td>
<td align="left">DT -&gt; the</td>
</tr>
<tr class="even">
<td align="left">the N VP</td>
<td align="left">N -&gt; dog</td>
</tr>
<tr class="odd">
<td align="left">the dog VP</td>
<td align="left">VP -&gt; VB</td>
</tr>
<tr class="even">
<td align="left">the dog VB</td>
<td align="left">VB -&gt; laughs</td>
</tr>
<tr class="odd">
<td align="left">the dog laughs</td>
<td align="left"></td>
</tr>
</tbody>
</table>
<h3 id="properties-of-cfgs"><a href="#properties-of-cfgs"> Properties of CFGs</a></h3>
<ul>
<li>CFG defines a set of valid derivations.
<ul>
<li>Set may be countably infinite.</li>
</ul></li>
<li>A string <span class="math">\(s \in \Sigma^{*}\)</span> is <strong>in the language defined by CFG</strong> if there is at least one derivation that yields <span class="math">\(s\)</span>.</li>
<li><p>Each string in the language generated by the CFG may have <strong>more than one derivation</strong>, i.e. may be <strong>ambiguous</strong>.</p></li>
<li>!!AI at this stage of the course there is no magic algorithm which will tell you the answer to this question.</li>
<li>Start with the string and <strong>go backwards</strong>, trying to get to <span class="math">\(S\)</span>, <strong>whilst obeying left-most derivation</strong>.
<ul>
<li>You <em>must</em> consider the left-most symbol first. If <strong>any rule</strong> applies to the left-most symbol then you must use it. You can’t skip to the right.</li>
<li>For small grammars <strong>going forwards</strong> in a brute-force fashion might be faster, but if the grammar is too large it’s too difficult to keep track of.</li>
</ul></li>
</ul>
<h3 id="the-problem-with-parsing-ambiguity"><a href="#the-problem-with-parsing-ambiguity">The Problem with Parsing: Ambiguity</a></h3>
<ul>
<li>Input: “She announced a program to promote safety in trucks and vans”.</li>
<li>Output:
<ul>
<li>Correctly: she’s announcing a program that will promote safety in both trucks and vans.</li>
<li>Also: she’s announcing a program that will promote safety in trucks, and “vans” (just throwing in the word vans).</li>
<li>Also: she’s announcing a program that wil promote safety. This program is located within trucks and vans.</li>
<li>14 different syntactic structures.</li>
</ul></li>
</ul>
<hr />
<p>Quiz: given “Jon saw Bill in Paris in June”, and grammar:</p>
<ul>
<li>S -&gt; NP VP</li>
<li>PP -&gt; P NP</li>
<li>NP -&gt; N</li>
<li>NP -&gt; NP PP</li>
<li>VP -&gt; V NP</li>
<li>VP -&gt; VP PP</li>
<li>P -&gt; in</li>
<li>V -&gt; saw</li>
<li>N -&gt; Jon</li>
<li>N -&gt; Bill</li>
<li>N -&gt; June</li>
<li>N -&gt; Paris</li>
</ul>
<p>How many parse trees are there for this sentence?</p>
<ul>
<li>!!AI I couldn’t get this. First step is probably to draw one valid parse tree. Then, according to the quiz answer:</li>
</ul>
<blockquote>
<p>There are three places to attach the two PPs: the verb, the first noun, the second noun. The <strong>five</strong> valid derivations are (1) verb, verb, (2) first noun, verb, (3) first noun, second noun, (4) first noun, first noun, (5) verb, second noun.</p>
</blockquote>
<div class="figure">
<img src="nlp_03_parsetree1.png" />
</div>
<h3 id="a-brief-sketch-of-the-syntax-of-english"><a href="#a-brief-sketch-of-the-syntax-of-english">A brief sketch of the syntax of English</a></h3>
<ul>
<li><p>“A Comprehensive Grammar of the English Language”, 1800 pages, 4.6lbs, 10 inches x 8.4 inches x 2.4 inches :).</p></li>
<li>Parts of Speech (tags from the <em>Brown corpus</em>, from the early 1960’s).
<ul>
<li>Nouns
<ul>
<li>NN = singular noun (e.g. man, dog, park)</li>
<li>NNS = plural noun (e.g. telescopes, houses, buildings)</li>
<li>NNP = proper noun (e.g. Smith, Gates, IBM)</li>
</ul></li>
<li>Determiners
<ul>
<li>DT = determiner (e.g. the, a, some, every)</li>
</ul></li>
<li>Adjectives
<ul>
<li>JJ = adjective (e.g. red, green, large, idealistic)</li>
</ul></li>
</ul></li>
</ul>
<h3 id="a-fragment-of-a-noun-phrase-grammar"><a href="#a-fragment-of-a-noun-phrase-grammar"> A Fragment of a Noun Phrase Grammar</a></h3>
<ul>
<li>NP is a Noun Phrase.</li>
<li><span class="math">\(\bar{N} \rightarrow NN\)</span></li>
<li><span class="math">\(\bar{N} \rightarrow NN\enspace\bar{N}\)</span></li>
<li><span class="math">\(\bar{N} \rightarrow JJ\enspace\bar{N}\)</span></li>
<li><span class="math">\(\bar{N} \rightarrow \bar{N}\enspace\bar{N}\)</span></li>
<li><span class="math">\(NP \rightarrow DT\enspace\bar{N}\)</span></li>
</ul>
<hr />
<p>An example</p>
<ul>
<li>NN -&gt; box</li>
<li>NN -&gt; car</li>
<li>NN -&gt; mechanic</li>
<li>NN -&gt; pigeon</li>
<li>DT -&gt; the</li>
<li>DT -&gt; a</li>
<li>JJ -&gt; fast</li>
<li>JJ -&gt; metal</li>
<li>JJ -&gt; idealistic</li>
<li><p>JJ -&gt; clay</p></li>
<li>NP</li>
<li>[DT, <span class="math">\(\bar{N}\)</span>]</li>
<li>[the, <span class="math">\(\bar{N}\)</span>]</li>
<li>[the, <span class="math">\(NN\)</span>]</li>
<li><p>[the, car]</p></li>
</ul>
<p>and</p>
<ul>
<li>NP</li>
<li>[DT, <span class="math">\(\bar{N}\)</span>]</li>
<li>[the, <span class="math">\(\bar{N}\)</span>]</li>
<li>[the, JJ, <span class="math">\(\bar{N}\)</span>]</li>
<li>[the, fast, <span class="math">\(\bar{N}\)</span>]</li>
<li>[the, fast, <span class="math">\(NN\)</span>]</li>
<li>[the, fast, car]</li>
</ul>
<p>and</p>
<ul>
<li>NP</li>
<li>[DT, <span class="math">\(\bar{N}\)</span>]</li>
<li>[the, <span class="math">\(\bar{N}\)</span>]</li>
<li>[the, JJ, <span class="math">\(\bar{N}\)</span>]</li>
<li>[the, JJ, JJ, <span class="math">\(\bar{N}\)</span>]</li>
<li>[the, fast, red, car]</li>
</ul>
<hr />
<ul>
<li>So, intuitively, <span class="math">\(\bar{N} \rightarrow JJ\enspace\bar{N}\)</span> is prepending any number of adjectives to a noun.</li>
<li>Very similarly, <span class="math">\(\bar{N} \rightarrow NN\enspace\bar{N}\)</span> prepends any number of nouns to a noun (“the car factory”).</li>
<li>The <span class="math">\(\bar{N}\)</span> category is an intermediate category with these noun phrases. It always follows a determiner.</li>
</ul>
<hr />
<p>Quiz: given “the fast car mechanic”, there are <strong>three</strong> parse trees.</p>
<ol style="list-style-type: decimal">
<li>[ NP [ D the ] [ <span class="math">\(\bar{N}\)</span> [ JJ fast ] [ <span class="math">\(\bar{N}\)</span> [ NN car ] [ <span class="math">\(\bar{N}\)</span> [ NN mechanic ] ] ] ] ]
<ul>
<li>“the”, “fast car mechanic”</li>
</ul></li>
<li>[ NP [ D the ] [ <span class="math">\(\bar{N}\)</span> [ <span class="math">\(\bar{N}\)</span> [ JJ fast ] [ <span class="math">\(\bar{N}\)</span> [ NN car ] ] ] [ <span class="math">\(\bar{N}\)</span> [ NN mechanic ] ] ] ]
<ul>
<li>“the”, “fast car”, “mechanic”</li>
</ul></li>
<li>[ NP [ D the ] [ <span class="math">\(\bar{N}\)</span> [ JJ fast ] [ <span class="math">\(\bar{N}\)</span> [ <span class="math">\(\bar{N}\)</span> [ NN car ] ] [ <span class="math">\(\bar{N}\)</span> [ NN mechanic ] ] ] ] ]
<ul>
<li>“the”, “fast”, “car mechanic”</li>
</ul></li>
</ol>
<p>(draw them out to see).</p>
<h3 id="prepositions-and-prepositional-phrases"><a href="#prepositions-and-prepositional-phrases">Prepositions and Prepositional Phrases</a></h3>
<ul>
<li>Brown corpus uses IN = preposition (e.g. of, in, out, beside, as).</li>
<li>e.g. “of the man”, “in the room”.</li>
<li>We add two rules to hand prepositional phrases:
<ul>
<li><span class="math">\(PP \rightarrow IN \enspace NP\)</span></li>
<li><span class="math">\(\bar{N} \rightarrow \bar{N} \enspace PP\)</span></li>
</ul></li>
<li>Roughly 100 prepositions in English.</li>
<li>a full triangle in a parse tree means that there are some undefined set of other nodes that make up the words.
<ul>
<li>NP, then full triangle, then “the room” as only child.</li>
</ul></li>
<li><span class="math">\(\bar{N} \rightarrow \bar{N} \enspace PP\)</span> means the prepositional phrase (PP) is a post-modifier to the noun.
<ul>
<li>“the dog in the car”; “in the car” is a post-modifier to “dog”.</li>
</ul></li>
<li>!!AI last slide in “An Extended Grammar”, for lecture &quot;A Simple Grammar for English (Part 2) is worth re-watching.</li>
</ul>
<h3 id="verbs-verb-phrases-and-sentences"><a href="#verbs-verb-phrases-and-sentences">Verbs, Verb Phrases, and Sentences</a></h3>
<ul>
<li>Basic Verb Types
<ul>
<li>Vi = instransitive verb (e.g. sleeps, walks, laughs)</li>
<li>Vt = transitive verb (e.g. sees, saw, likes)</li>
<li>Vd = ditransitive verb (e.g. gave)</li>
</ul></li>
<li>Basic VP rules
<ul>
<li><span class="math">\(VP \rightarrow Vi\)</span></li>
<li><span class="math">\(VP \rightarrow Vt \enspace NP\)</span></li>
<li><span class="math">\(VP \rightarrow Vd \enspace NP \enspace NP\)</span></li>
</ul></li>
<li>Basic S rule
<ul>
<li><span class="math">\(S \rightarrow NP \enspace VP\)</span>
<ul>
<li>The NP is the <strong>subject</strong>.</li>
<li>The VP generates some <strong>verb</strong> followed by zero or more noun phrases.</li>
</ul></li>
</ul></li>
<li>Examples of VP
<ul>
<li>sleeps</li>
<li>walks</li>
<li>likes the mechanic</li>
<li>gave the mechanic the fast car</li>
</ul></li>
<li>Examples of S
<ul>
<li>the man sleeps</li>
<li>the dog walks</li>
<li>the dog gave the mechanic the fast car</li>
</ul></li>
</ul>
<h3 id="pps-modifying-verb-phrases"><a href="#pps-modifying-verb-phrases">PPs Modifying Verb Phrases</a></h3>
<ul>
<li>A new rule: <span class="math">\(VP \rightarrow VP \enspace PP\)</span></li>
<li>New examples of VP:
<ul>
<li>sleeps in the car</li>
<li>walks like the mechanic</li>
<li>gave the mechanic the fast car on Tuesday</li>
</ul></li>
<li>The Prepositional Phrase (PP) usually adds information about the location or the time etc. of the Verb Phrase (VP).</li>
</ul>
<h3 id="complementizers-and-sbars"><a href="#complementizers-and-sbars">Complementizers, and SBARs</a></h3>
<ul>
<li>Complementizers
<ul>
<li>COMP = complementizer (e.g. that)</li>
</ul></li>
<li>SBAR
<ul>
<li><span class="math">\(SBAR \rightarrow COMP \enspace S\)</span></li>
</ul></li>
<li>Examples
<ul>
<li>that the man sleeps</li>
<li>that the mechanic saw the dog</li>
</ul></li>
</ul>
<h3 id="more-verbs"><a href="#more-verbs">More Verbs</a></h3>
<ul>
<li>New Verb Types
<ul>
<li>V[5] (e.g. said, reported)</li>
<li>V[6] (e.g. told, informed)</li>
<li>V[7] (e.g. bet)</li>
</ul></li>
<li>New VP rules
<ul>
<li><span class="math">\(VP \rightarrow V[5] \enspace SBAR\)</span></li>
<li><span class="math">\(VP \rightarrow V[6] \enspace NP \enspace SBAR\)</span></li>
<li><span class="math">\(VP \rightarrow V[7] \enspace NP \enspace NP \enspace SBAR\)</span></li>
</ul></li>
<li>Examples of new VPs:
<ul>
<li>said that the man sleeps.</li>
<li>told the dog that the mechanic likes the pigeon.
<ul>
<li>the NP is “the dog”</li>
<li>the SBAR is “that the mechanic likes the pigeon”</li>
</ul></li>
<li>bet the pigeon $50 that the mechanic owns a fast car.
<ul>
<li>the V[7] is “bet”. This is a very rare verb category.</li>
<li>the NP is “the pigeon”</li>
<li>another NP is “$50”</li>
<li>the SBAR is “that the mechanic owns a fast car”</li>
</ul></li>
</ul></li>
</ul>
<h3 id="coordination"><a href="#coordination">Coordination</a></h3>
<ul>
<li>A new part-of-speech
<ul>
<li>CC = coordinator (e.g. and, or, but)</li>
</ul></li>
<li>New Rules
<ul>
<li><span class="math">\(NP \rightarrow NP \enspace CC \enspace NP\)</span>
<ul>
<li>NP made up of “the man”/NP “and”/CC “the dog”/NP</li>
</ul></li>
<li><span class="math">\(\bar{N} \rightarrow \bar{N} \enspace CC \enspace \bar{N}\)</span></li>
<li><span class="math">\(VP \rightarrow VP \enspace CC \enspace VP\)</span>
<ul>
<li>VP made up of “sleeps”/VP “and”/CC “likes the dog”/VP</li>
</ul></li>
<li><span class="math">\(S \rightarrow S \enspace CC \enspace S\)</span></li>
<li><span class="math">\(SBAR \rightarrow SBAR \enspace CC \enspace SBAR\)</span></li>
</ul></li>
</ul>
<h3 id="weve-only-scratched-the-surface"><a href="#weve-only-scratched-the-surface"> We’ve only scratched the surface…</a></h3>
<ul>
<li>Agreement
<ul>
<li>“The dogs laugh” vs. “The dog laughs”</li>
<li>Some notion of agreement between verb and main subject. Our current grammar fails to capture this constraint.</li>
</ul></li>
<li>Wh-movement
<ul>
<li>“The dog that the cat liked …”
<ul>
<li>“cat” is a transitive verb (Vt). Really “dog” should come after it, but it’s moved to the start.</li>
</ul></li>
</ul></li>
<li>Active vs. passive
<ul>
<li>The dog saw the cat vs.</li>
<li>The cat was seen by the dog</li>
</ul></li>
<li>For more information: <em>Syntactic Theory: A Formal Introduction, 2nd edition. Ivan A. Sag, Thomas Wasow, and Emily M. Bender.</em></li>
</ul>
<h3 id="sources-of-ambiguity"><a href="#sources-of-ambiguity">Sources of Ambiguity</a></h3>
<hr />
<ul>
<li><p>One source: <strong>part-of-speech ambiguity</strong>.</p></li>
<li><span class="math">\(NN \rightarrow \textrm{duck}\)</span></li>
<li><span class="math">\(Vi \rightarrow \textrm{duck}\)</span></li>
<li>Which one is it? Affects the parse tree, not just a tag.</li>
<li><p>e.g. “saw her duck with the telescope”.</p></li>
</ul>
<hr />
<ul>
<li>Second source: <strong>prepositional phrase attachment</strong>.</li>
<li>“I drove down the road in the car”.
<ul>
<li>“I” “drove down the road” “in the car”</li>
<li>“I” “drown down” “the road in the car”</li>
</ul></li>
<li>“John was believed to have been shot by Bill”
<ol style="list-style-type: decimal">
<li>Bill shot John.</li>
<li>Bill believed that John has been shot.</li>
</ol></li>
<li>Despite both interpretations being, a prior, equally likely, humans have a strong tendency to attach prepositional phrases to the most recent verb. Hence 1) is more intuitive.</li>
</ul>
<hr />
<ul>
<li>Third source: <strong>noun premodifiers</strong></li>
<li>“the fast car mechanic”
<ul>
<li>“the” “fast” “car mechanic”</li>
<li>“the” “fast car” “mechanic”</li>
</ul></li>
</ul>
<h2 id="week-3---probabilistic-context-free-grammars-pcfgs"><a href="#week-3---probabilistic-context-free-grammars-pcfgs">Week 3 - Probabilistic Context-Free Grammars (PCFGs)</a></h2>
<ul>
<li>Probability assigned to each rule.</li>
<li>Hence probability assigned to each possible parse tree when ambiguity presence.</li>
<li>Naive application is very poor, but simple to make better.</li>
</ul>
<h3 id="a-probabilistic-context-free-grammar-pcfg"><a href="#a-probabilistic-context-free-grammar-pcfg">A Probabilistic Context-Free Grammar (PCFG)</a></h3>
<ul>
<li>Every rule, even the rules that lead only to terminal symbols (e.g. <span class="math">\(Vi \rightarrow \textrm{sleeps}\)</span>), has a probability associated with it.</li>
<li>Probability of a tree <span class="math">\(t\)</span> with rules:</li>
</ul>
<p><span class="math">\[\alpha_1 \rightarrow \beta_1, \alpha_2 \rightarrow \beta_2, \ldots, \alpha_n \rightarrow \beta_n\]</span></p>
<ul>
<li><span class="math">\(p(t) = \prod_{i=1}^{n} q(\alpha_i \rightarrow \beta_i)\)</span>, where <span class="math">\(q(\alpha \rightarrow \beta)\)</span> is the probability for rule <span class="math">\(\alpha \rightarrow \beta\)</span>.</li>
<li>Note that the sum of all probabilities for a particular <span class="math">\(\alpha_i\)</span> left-hand-side (LHS) non-terminal is 1.</li>
</ul>
<hr />
<ul>
<li>Top-down stochastic processe where we can sample parse trees.</li>
</ul>
<table>
<thead>
<tr class="header">
<th align="left">Derivation</th>
<th align="left">Rules used</th>
<th align="left">Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">S</td>
<td align="left">S <span class="math">\(\rightarrow\)</span> NP VP</td>
<td align="left">1.0</td>
</tr>
<tr class="even">
<td align="left">NP VP</td>
<td align="left">NP <span class="math">\(\rightarrow\)</span> DT NN</td>
<td align="left">0.3</td>
</tr>
<tr class="odd">
<td align="left">DT NN VP</td>
<td align="left">DT <span class="math">\(\rightarrow\)</span> the</td>
<td align="left">1.0</td>
</tr>
<tr class="even">
<td align="left">the NN VP</td>
<td align="left">NN <span class="math">\(\rightarrow\)</span> dog</td>
<td align="left">0.1</td>
</tr>
<tr class="odd">
<td align="left">the dog VP</td>
<td align="left">VP <span class="math">\(\rightarrow\)</span> Vi</td>
<td align="left">0.4</td>
</tr>
<tr class="even">
<td align="left">the dog Vi</td>
<td align="left">Vi <span class="math">\(\rightarrow\)</span> laughs</td>
<td align="left">0.5</td>
</tr>
<tr class="odd">
<td align="left">the dog laughs</td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<ul>
<li>The process ends when we have no non-terminals left.</li>
</ul>
<hr />
<p>Quiz: consider the following PCFG:</p>
<ul>
<li>q(S -&gt; NP VP) = 0.9</li>
<li>q(S -&gt; NP) = 0.1</li>
<li>q(NP -&gt; D N) = 1</li>
<li>q(VP -&gt; V) = 1</li>
<li>q(D -&gt; the) = 0.8</li>
<li>q(D -&gt; a) = 0.2</li>
<li>q(N -&gt; cat) = 0.5</li>
<li>q(N -&gt; dog) = 0.5</li>
<li>q(V -&gt; sings) = 1</li>
</ul>
<p>And parse tree:</p>
<p>(S, (NP, (D, the), (N, cat)), (VP, (V, sings)))</p>
<p>Probability:</p>
<table>
<thead>
<tr class="header">
<th align="left">Derivation</th>
<th align="left">Rules used</th>
<th align="left">Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">S</td>
<td align="left">S -&gt; NP VP</td>
<td align="left">0.9</td>
</tr>
<tr class="even">
<td align="left">NP VP</td>
<td align="left">NP -&gt; D N</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">D N VP</td>
<td align="left">D -&gt; the</td>
<td align="left">0.8</td>
</tr>
<tr class="even">
<td align="left">the N VP</td>
<td align="left">N -&gt; cat</td>
<td align="left">0.5</td>
</tr>
<tr class="odd">
<td align="left">the cat VP</td>
<td align="left">VP -&gt; V</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">the cat V</td>
<td align="left">V -&gt; sings</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">the cat sings</td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>Total probability = <span class="math">\(0.9 \times 1 \times 0.8 \times 0.5 \times 0.5 \times 1 \times 1 = 0.36\)</span></p>
<hr />
<h3 id="properties-of-cfgs-1"><a href="#properties-of-cfgs-1">Properties of CFGs</a></h3>
<ul>
<li>Assigns a probability to each <em>left-most derivation</em>, or parse-tree, allowed by the underlying CFG.</li>
<li>Assume:
<ul>
<li>sentence <span class="math">\(s\)</span>,</li>
<li>set of derivations for that sentence <span class="math">\(T(s)\)</span></li>
</ul></li>
<li>Thus PCFG assigns probability <span class="math">\(p(t)\)</span> to each derivation in <span class="math">\(T(s)\)</span>.</li>
<li>There is a <em>ranking in order of probability</em>.</li>
<li>The most likely parse tree for sentence <span class="math">\(s\)</span> is <span class="math">\(\textrm{arg} \underset{t \in T(s)}{\textrm{max}} p(t)\)</span></li>
</ul>
<hr />
<p>Quiz: consider the following PCFG:</p>
<ul>
<li>q(S -&gt; NP VP) = 1.0</li>
<li>q(VP -&gt; VP PP) = 0.9</li>
<li>q(VP -&gt; V NP) = 0.1</li>
<li>q(NP -&gt; NP PP) = 0.5</li>
<li>q(NP -&gt; N) = 0.5</li>
<li>q(PP -&gt; P NP) = 1.0</li>
<li>q(N -&gt; Ted) = 0.2</li>
<li>q(N -&gt; Jill) = 0.2</li>
<li>q(N -&gt; town) = 0.6</li>
<li>q(V -&gt; saw) = 1.0</li>
<li>q(P -&gt; in) = 1.0</li>
</ul>
<p>Given sentence “Ted saw Jill in Town”, what is highest probability for any parse tree under this PCFG?</p>
<p>Right now there’s no clever way, just brute force. Also note that just because a PCFG parse tree has a non-zero probability doesn’t mean it’s a <strong>valid left-most derivation</strong>.</p>
<p>I could only find two valid left-most derivations; intuitively they are:</p>
<ol style="list-style-type: decimal">
<li>Ted “saw” “Jill in town”, i.e. Jill was in town when Ted saw her.</li>
<li>Ted “saw Jill” “in town”, i.e. Ted was in town and saw Jill.</li>
</ol>
<p>Strictly speaking the parse trees and corresponding probabilities are:</p>
<p>(S, (NP, N, Ted), (VP, (V, saw), (NP, (NP, N, Jill), (PP, (P, in), (NP, N, town))))), = 0.00015</p>
<p>(S, (NP, N, Ted), (VP, (VP, (V, saw), (NP, N, Jill)), (PP, (P, in), (NP, in town)))) = 0.00027</p>
<p>Note that these probabilities do not add up to 1; not all possible PCFG parse-trees are valid left-most derivations.</p>
<h3 id="data-for-parsing-experiments-treebanks"><a href="#data-for-parsing-experiments-treebanks">Data for Parsing Experiments: Treebanks</a></h3>
<ul>
<li>Penn WSJ Treebank = 50,000 sentences with associated trees</li>
<li>Usual set up: 40,000 training sentences (80%), 2,400 test sentences (4.8%).</li>
</ul>
<h3 id="deriving-a-pcfg-from-a-treebank"><a href="#deriving-a-pcfg-from-a-treebank">Deriving a PCFG from a Treebank</a></h3>
<ul>
<li>Given a set of examples (a <strong>treebank</strong>), the underlying CFG can simply be <em>all the rules seen in the corpus</em>.</li>
<li>Maximum Likelihood estimates:</li>
</ul>
<p><span class="math">\[q_{ML}(\alpha \rightarrow \beta) = \frac{\textrm{Count}(\alpha \rightarrow \beta)}{\textrm{Count}(\alpha)}\]</span></p>
<ul>
<li>e.g.:</li>
</ul>
<p><span class="math">\[q_{ML}(\textrm{VP} \rightarrow \textrm{Vt NP}) = \frac{\textrm{Count}(\textrm{VP} \rightarrow \textrm{Vt NP})}{\textrm{Count}(\textrm{VP})}\]</span></p>
<ul>
<li>where the counts are taken from a training set of example trees.</li>
<li><strong>If the training data is generated by a PCFG</strong>, then as training data size goes to infinite, the maximum-likelihood PCFG will converge to the same distribution as the “true” PCFG.</li>
</ul>
<hr />
<p>Quiz: given these three parse trees, what’s <span class="math">\(q_{ML}(\textrm{NP} \rightarrow \textrm{NP PP})\)</span>? It’s 2/7.</p>
<hr />
<h3 id="pcfgs"><a href="#pcfgs">PCFGs</a></h3>
<ul>
<li>Booth and Thompson (1973) show that a CFG with rule probabilities correctly defines a distribution over the set of derivations provided that:</li>
</ul>
<ol style="list-style-type: decimal">
<li>The rule probabilities define conditional distributions over the different ways of rewriting each non-terminal.</li>
<li>A technical condition on the rule probabilities ensuring that the probability of the derivation terminating in a finite number of steps is 1. (This condition is not really a practical concern).
<ul>
<li>Consider “S -&gt; S S” with probability 1.0, “S -&gt; a” with probability 0.</li>
<li>But more interesting is that “S -&gt; S S” with probability 0.6, “S -&gt; a” with probability 0.4 <em>also</em> fails. Not elaborated on.</li>
</ul></li>
</ol>
<h3 id="parsing-with-a-pcfg"><a href="#parsing-with-a-pcfg">Parsing with a PCFG</a></h3>
<ul>
<li>Given a PCFG and a sentence <span class="math">\(s\)</span>, define <span class="math">\(T(s)\)</span> to be the set of trees with <span class="math">\(s\)</span> as the yield.</li>
<li>Given a PCFG and a sentence <span class="math">\(s\)</span>, how do we find:</li>
</ul>
<p><span class="math">\[\textrm{arg} \underset{t \in T(s)}{\textrm{max}} p(t)\]</span></p>
<ul>
<li>We could enumerate, brute force.
<ol style="list-style-type: decimal">
<li>Find all trees.</li>
<li>Calculate probabilities for each tree.</li>
<li>Find tree with maximum probability.</li>
</ol></li>
<li>This is exponential, but can be efficiently solved using dynamic programming.</li>
</ul>
<h3 id="chomsky-normal-form"><a href="#chomsky-normal-form">Chomsky Normal Form</a></h3>
<ul>
<li>Assume CFG <span class="math">\(G = (N, \Sigma, R, S)\)</span> is in <strong>Chomsky Normal Form</strong>, as follows:
<ul>
<li><span class="math">\(N\)</span> is a set of non-terminal symbols.</li>
<li><span class="math">\(\Sigma\)</span> is a set of terminal symbols.</li>
<li><span class="math">\(R\)</span> is a set of rules which take one of two forms:
<ul>
<li><span class="math">\(X \rightarrow Y_1 Y_2\)</span> for <span class="math">\(X \in N\)</span>, and <span class="math">\(Y_1, Y_2 \in N\)</span>
<ul>
<li><span class="math">\(X, Y_1, Y_2\)</span> are all non-terminals</li>
</ul></li>
<li><span class="math">\(X \rightarrow Y\)</span> for <span class="math">\(X \in N\)</span>, and <span class="math">\(Y \in \Sigma\)</span>.
<ul>
<li><span class="math">\(X\)</span> is non-terminal, <span class="math">\(Y\)</span> is terminal.</li>
</ul></li>
</ul></li>
<li><span class="math">\(S \in N\)</span> is a distinguished start symbol.</li>
</ul></li>
</ul>
<hr />
<ul>
<li>You can take any PCFG and convert it to a PCFG in Chomsky Normal Form.</li>
<li>“VP -&gt; Vt NP PP”, with probability 0.2</li>
<li>Introduce a new symbol.</li>
<li>“VP -&gt; Vt-NP PP”, with probability 0.2</li>
<li>“Vt-NP -&gt; Vt NP”, with probability 1.0</li>
<li>This just introduces intermediary non-terminals; to move from Chomsky Normal Form back to the original PCFG just remove these dummy non-terminals.</li>
<li>Notice that the original rule with three non-terminals has the same probability as the new rule with two non-terminals; this is also the answer to the quiz.</li>
</ul>
<h3 id="a-dynamic-programming-algorithm"><a href="#a-dynamic-programming-algorithm">A Dynamic Programming Algorithm</a></h3>
<ul>
<li>Given a PCFG and a sentence <span class="math">\(s\)</span>, how do we find:</li>
</ul>
<p><span class="math">\[\underset{t \in T(s)}{\textrm{max}} p(t)\]</span></p>
<ul>
<li>(we really want argmax, but we’ll do that later).</li>
<li>Notation:
<ul>
<li><span class="math">\(n\)</span> = number of words in the sentence</li>
<li><span class="math">\(w_i\)</span> = <span class="math">\(i\)</span>’th word in the sentence</li>
<li><span class="math">\(N\)</span> = the set of non-terminals in the grammar</li>
<li><span class="math">\(S\)</span> = the start symbol in the grammar</li>
</ul></li>
<li>Define a dynamic programming table
<ul>
<li><span class="math">\(\pi[i,j,X]\)</span> = maximum probability of a constituent with non-terminal <span class="math">\(X\)</span> spanning words <span class="math">\(i \ldots j\)</span> inclusive</li>
<li><span class="math">\(i = 1, 2, \ldots, n\)</span></li>
<li><span class="math">\(j = 1, 2, \ldots, n\)</span></li>
<li><span class="math">\(X \in N\)</span></li>
</ul></li>
<li>Our goal is to calculate <span class="math">\(max_{i \in T(S)} p(t) = \pi[1,n,S]\)</span>.
<ul>
<li>“What is the proability of the most likely parse tree that has S at the root and spans all the words in the sentence?”</li>
</ul></li>
</ul>
<hr />
<ul>
<li>Assume sentence <span class="math">\(w_1, w_2, w_3, w_4, w_5, w_6\)</span>.</li>
<li><span class="math">\(\pi(2, 5, NP)\)</span> means &quot;what is the probability of the most likely way of having an NP in the parse tree such that it dominates / spans <span class="math">\(w_2, w_3, w_4, w_5\)</span> inclusive?</li>
</ul>
<hr />
<h3 id="an-example-3"><a href="#an-example-3"> An Example</a></h3>
<p><span class="math">\[\underset{1}{\textrm{the}} \enspace \underset{2}{\textrm{dog}} \enspace \underset{3}{\textrm{saw}} \enspace \underset{4}{\textrm{the}} \enspace \underset{5}{\textrm{man}} \enspace \underset{6}{\textrm{with}} \enspace \underset{7}{\textrm{the}} \enspace \underset{8}{\textrm{telescope}}\]</span></p>
<p><span class="math">\(\pi(3, 8, \textrm{VP})\)</span> is all VPs that span words 3 to 8 inclusive.</p>
<ul>
<li>the dog using the telescope to see the man</li>
<li>the dog seeing a man with the telescope</li>
<li>The <span class="math">\(\pi\)</span> statement is “which of the two is the more likely spanning VP?”</li>
</ul>
<hr />
<h3 id="a-dynamic-programming-algorithm-1"><a href="#a-dynamic-programming-algorithm-1"> A Dynamic Programming Algorithm</a></h3>
<ul>
<li>Base case definition: <span class="math">\(\forall i = 1, 2, \ldots, n, \forall X \in V\)</span>:</li>
</ul>
<p><span class="math">\[\pi[i,i,X] = q(X \rightarrow w_i)\]</span></p>
<ul>
<li>Note: define <span class="math">\(q(X \rightarrow w_i) = 0\)</span> if <span class="math">\(X \rightarrow w_i\)</span> is not in the grammar.</li>
<li><p>e.g. “the/1 dog/2 laughs/3”, <span class="math">\(\pi(2,2,\textrm{NN})\)</span> only has one possible parse tree because it must be “NN -&gt; dog”, hence <span class="math">\(= q(\textrm{NN} \rightarrow \textrm{dog})\)</span></p></li>
<li><p>Recursive definition: <span class="math">\(\forall i = 1, 2, \ldots, (n-1)\)</span>, <span class="math">\(j = (i+1), \ldots, n\)</span>, <span class="math">\(X \in N\)</span>:</p></li>
</ul>
<p><span class="math">\[\pi(i,j,X) = \underset{\underset{s \in i \ldots (j-1)}{X \rightarrow Y \enspace Z \in R}}{max} (q(X \rightarrow Y \enspace Z) \times \pi(i,s,Y) \times \pi(s+1,j,Z))\]</span></p>
<ul>
<li><span class="math">\(s\)</span> is the <strong>split point</strong>.</li>
</ul>
<h3 id="an-example-4"><a href="#an-example-4">An Example</a></h3>
<p><span class="math">\[\underset{1}{\textrm{the}} \enspace \underset{2}{\textrm{dog}} \enspace \underset{3}{\textrm{saw}} \enspace \underset{4}{\textrm{the}} \enspace \underset{5}{\textrm{man}} \enspace \underset{6}{\textrm{with}} \enspace \underset{7}{\textrm{the}} \enspace \underset{8}{\textrm{telescope}}\]</span></p>
<p><span class="math">\(\pi(3,8,\textrm{VP})\)</span></p>
<p>Suppose we have:</p>
<ul>
<li>VP -&gt; Vt NP, probability 0.4</li>
<li>VP -&gt; VP PP, probability 0.6</li>
</ul>
<p>We’re searching over two possible things:</p>
<ul>
<li>We’re searching over all possible rules in the grammar (<span class="math">\(X \rightarrow Y \enspace Z \in R\)</span>).</li>
<li>We’re searching over all possible split points (<span class="math">\(s \in i \ldots (j-1)\)</span>).
<ul>
<li><span class="math">\(s \in \{3, 4, 5, 6, 7\}\)</span>.</li>
</ul></li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        &amp; q(\textrm{VP} \rightarrow \textrm{VT NP}) \times \pi(3,3,\textrm{Vt}) \times \pi(4,8,NP) \\
        &amp; q(\textrm{VP} \rightarrow \textrm{VT NP}) \times \pi(3,4,\textrm{Vt}) \times \pi(5,8,NP) \\
        &amp; \vdots \\
        &amp; q(\textrm{VP} \rightarrow \textrm{VT NP}) \times \pi(3,7,\textrm{Vt}) \times \pi(8,8,NP) \\
        &amp; q(\textrm{VP} \rightarrow \textrm{VT PP}) \times \pi(3,3,\textrm{Vt}) \times \pi(4,8,PP) \\
        &amp; q(\textrm{VP} \rightarrow \textrm{VT PP}) \times \pi(3,4,\textrm{Vt}) \times \pi(5,8,PP) \\
        &amp; \vdots \\
        &amp; q(\textrm{VP} \rightarrow \textrm{VT PP}) \times \pi(3,7,\textrm{Vt}) \times \pi(8,8,PP)
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>Each of these products will have a different value, and the max is the value for <span class="math">\(\pi(3,8,\textrm{VP})\)</span>.</li>
<li>We will calculate the <span class="math">\(pi\)</span> values bottom-up to ensure correctness.</li>
</ul>
<h3 id="justification"><a href="#justification">Justification</a></h3>
<ul>
<li>Consider the above example yet again.</li>
<li>Suppose for <span class="math">\(\pi(3,8,\textrm{VP})\)</span> we choose “VP -&gt; VP PP”.</li>
<li>And suppose we also choose split point <span class="math">\(s = 5\)</span>.</li>
<li>This means that the first right-hand-side <span class="math">\(VP\)</span> spans <span class="math">\(w_3, w_4, w_5\)</span>, and the second right-hand-side <span class="math">\(PP\)</span> spans <span class="math">\(w_6, w_7, w_8\)</span>.</li>
<li>The highest probability tree is formed of two child trees, which themselves must be the highest probabilities trees, etc.</li>
<li>This is a classic observation of “dividing a problem into equivalent subproblems”, i.e. dynamic programming.</li>
</ul>
<h3 id="the-full-dynamic-programming-algorithm"><a href="#the-full-dynamic-programming-algorithm"> The Full Dynamic Programming Algorithm</a></h3>
<p><strong>Input</strong>: a sentence <span class="math">\(s = x_1 \ldots x_n\)</span>, a PCFG in Chomsky Normal Form (CNF) <span class="math">\(G = (N, \Sigma, S, R, q)\)</span>.</p>
<p><strong>Initialization</strong>:</p>
<p><span class="math">\(\forall \enspace i \in \{1, \ldots, n\}, \forall \enspace X \in N,\)</span></p>
<p><span class="math">\[
\begin{equation}
    \pi(i,i,X) = \begin{cases}
        q(X \rightarrow x_i), &amp; \textrm{if} \enspace X \rightarrow x_i \in R \\
        0, &amp; \textrm{Otherwise}
    \end{cases}
\end{equation}
\]</span></p>
<p><strong>Algorithm</strong></p>
<ul>
<li>For <span class="math">\(l = 1 \ldots (n-1)\)</span>
<ul>
<li>For <span class="math">\(i = 1 \ldots (n-l)\)</span>
<ul>
<li>Set <span class="math">\(j = i + l\)</span></li>
<li><span class="math">\(\pi(i,j,X) = \underset{\underset{s \in i \ldots (j-1)}{X \rightarrow Y \enspace Z \in R}}{max} (q(X \rightarrow Y \enspace Z) \times \pi(i,s,Y) \times \pi(s+1,j,Z))\)</span></li>
<li>and</li>
<li><span class="math">\(bp(i,j,X) = \textrm{arg}\underset{\underset{s \in i \ldots (j-1)}{X \rightarrow Y \enspace Z \in R}}{max} (q(X \rightarrow Y \enspace Z) \times \pi(i,s,Y) \times \pi(s+1,j,Z))\)</span></li>
</ul></li>
</ul></li>
</ul>
<hr />
<ul>
<li><span class="math">\(l\)</span> is the length of the segment we’re filling in.
<ul>
<li>Hence we go from smallest to largest, satisfying the rule that we do small lengths first.</li>
<li>l = 1
<ul>
<li>i = 1, j = 2</li>
<li>i = 2, j = 3</li>
<li>i = 3, j = 4</li>
</ul></li>
<li>l = 2
<ul>
<li>i = 1, j = 3</li>
<li>i = 2, j = 4</li>
</ul></li>
</ul></li>
</ul>
<hr />
<ul>
<li>The runtime is <span class="math">\(O(n^3 \times |N|^3)\)</span>
<ul>
<li>Cubic with respect to number of words <span class="math">\(n\)</span>.</li>
<li>Also cubic with respect to the number of non-terminals in the grammar, <span class="math">\(|N|\)</span>.</li>
<li>Consider that there are <span class="math">\(n^2\)</span> choices for <span class="math">\((i,j)\)</span>.</li>
<li>There are <span class="math">\(|N|\)</span> choices for X, then <span class="math">\(\times |N|^2\)</span> when considering <span class="math">\(Y\)</span> and <span class="math">\(Z\)</span>.</li>
<li>And <span class="math">\(s\)</span> also varies with <span class="math">\(n\)</span>.</li>
<li>This is way easier than brute-force search.</li>
</ul></li>
</ul>
<h3 id="summary-2"><a href="#summary-2"> Summary</a></h3>
<ul>
<li>PCFGs augments CFGs by including a probability for each rule in the grammar.</li>
<li>The probability for a parse tree is the product of probabilities for the rules in the tree.</li>
<li>To build a PCFG-parsed parser:
<ol style="list-style-type: decimal">
<li>Learn a PCFG from a treebank.</li>
<li>Given a test data sentence, use the CKY algorithm to computer the highest probability tree for the sentence under the PCFG.</li>
</ol></li>
</ul>
<h2 id="week-4---weaknesses-of-pcfgs"><a href="#week-4---weaknesses-of-pcfgs"> Week 4 - Weaknesses of PCFGs</a></h2>
<ul>
<li>Lack of sensitivity to lexical information</li>
<li><p>Lack of sensitivity to structural frequencies</p></li>
<li>History:
<ul>
<li>First treebanks in early 1990s.</li>
<li>PCFGs applies as seen before, but only around 72% accuracy for WSJ corpus.</li>
</ul></li>
<li><p>Modern parsers get around 92% accuracy for WSJ corpus.</p></li>
</ul>
<h3 id="lack-of-sensitivity-to-lexical-information"><a href="#lack-of-sensitivity-to-lexical-information">Lack of sensitivity to lexical information</a></h3>
<ul>
<li>Note that every rule to a terminal is independent of every other part of the parse tree; very strong and wrong independence assumption.</li>
<li><strong>Attachment ambiguity</strong> Attachment decision for prepositional phrases (PP) is completely independent of the words.
<ul>
<li>We know this is a bad decision.</li>
</ul></li>
<li><strong>Coordination ambiguity</strong>: for the coordinator (CC), could be e.g.
<ul>
<li>“dogs in houses” “and” “cats”, or</li>
<li>“dogs” “in” “houses and cats”</li>
</ul></li>
</ul>
<h3 id="lack-of-sensitivity-to-structural-frequencies"><a href="#lack-of-sensitivity-to-structural-frequencies">Lack of sensitivity to structural frequencies</a></h3>
<ul>
<li>e.g. “president of a company in Africa”
<ul>
<li>“president” “of a” “company in Africa”, or</li>
<li>“president” “of a company” “in Africa” (president in Africa)</li>
</ul></li>
<li>Both parse have the same rules, therefore receive the same probability under a PCFG.</li>
<li><strong>Close attachment</strong> (“Africa” attaching to “company” because it’s close) is twice as likely in WSJ test.</li>
</ul>
<hr />
<ul>
<li>Previous example: “John was believed to have been shot by Bill”
<ul>
<li>Here the low attachment analysis (Bill does the shooting) contains the same rules as the high attachment analysis (Bill does the believing), so the two analyses receive same probability.</li>
</ul></li>
</ul>
<h2 id="week-4---lexicalized-pcfgs"><a href="#week-4---lexicalized-pcfgs">Week 4 - Lexicalized PCFGs</a></h2>
<h3 id="heads-in-context-free-rules"><a href="#heads-in-context-free-rules">Heads in Context-Free Rules</a></h3>
<ul>
<li><p>Add <strong>annotations</strong> specifying the <strong>“head”</strong> of each rule:</p></li>
<li>S -&gt; NP <strong>VP</strong></li>
<li>VP -&gt; <strong>Vi</strong></li>
<li>VP -&gt; <strong>Vt</strong> NP</li>
<li>VP -&gt; <strong>VP</strong> PP</li>
<li>NP -&gt; DT <strong>NN</strong></li>
<li>NP -&gt; <strong>NP</strong> PP</li>
<li><p>PP -&gt; <strong>IN</strong> NP</p></li>
</ul>
<hr />
<ul>
<li>Vi -&gt; sleeps</li>
<li>Vt -&gt; saw</li>
<li>NN -&gt; man</li>
<li>NN -&gt; woman</li>
<li>NN -&gt; telescope</li>
<li>DT -&gt; the</li>
<li>IN -&gt; with</li>
<li>IN -&gt; in</li>
</ul>
<hr />
<ul>
<li>For each rule that give non-terminals, identify one of the children to be the head of the rule. An additional piece of information.</li>
</ul>
<h3 id="more-about-heads"><a href="#more-about-heads">More about Heads</a></h3>
<ul>
<li>Each context-free rule has one “special” child that is the head of the rule, the most special part, the syntactic centre, e.g.
<ul>
<li>S -&gt; NP <strong>VP</strong>, (VP is the head)</li>
<li>VP -&gt; <strong>Vt</strong> NP, (Vt is the head)</li>
<li>NP -&gt; DT NN <strong>NN</strong> (NN is the head)</li>
</ul></li>
<li>A core idea in syntax
<ul>
<li>See X-bar Theory, Head-Driven Phrase Structure Grammar</li>
</ul></li>
<li>Some intuitions:
<ul>
<li>The central sub-constituent of each rule.</li>
<li>The semantic predicate in each rule.</li>
</ul></li>
</ul>
<h3 id="rules-which-recover-heads-an-example-for-nps"><a href="#rules-which-recover-heads-an-example-for-nps">Rules which Recover Heads: An Example for NPs</a></h3>
<ul>
<li>Many early treebanks, e.g. WSJ, do not contain annotations for the heads.</li>
<li>If the rule contains NN, NNS, or NNP:
<ul>
<li>Choose the rightmost NN, NNS, or NNP.</li>
</ul></li>
<li>Else If the rule contains an NP:
<ul>
<li>Choose the leftmost NP</li>
</ul></li>
<li>Else If the rule contains a JJ
<ul>
<li>Choose the right-most JJ</li>
</ul></li>
<li>Else If the rule contains a CD (number, e.g. “100”, “1000”)
<ul>
<li>Choose the right-most CD</li>
</ul></li>
<li>Else
<ul>
<li>Choose the right-most child</li>
</ul></li>
</ul>
<hr />
<p>e.g.</p>
<p>NP -&gt; DT NNP <strong>NN</strong> NP -&gt; DT NN <em>NNP</em>* NP -&gt; <strong>NP</strong> PP NP -&gt; DT <strong>JJ</strong> NP -&gt; <strong>DT</strong></p>
<h3 id="rules-which-recover-heads-an-example-for-vps"><a href="#rules-which-recover-heads-an-example-for-vps">Rules which Recover Heads: An Example for VPs</a></h3>
<ul>
<li>If the rule contains Vi or Vt (or indeed all subcategories of verbs):
<ul>
<li>Choose the leftmost Vi or Vt</li>
</ul></li>
<li>Else If the rule contains a VP
<ul>
<li>Choose the leftmost VP</li>
</ul></li>
<li>Else
<ul>
<li>Choose the leftmost child</li>
</ul></li>
</ul>
<hr />
<p>e.g.</p>
<p>VP -&gt; <strong>Vt</strong> NP VP -&gt; <strong>VP</strong> PP</p>
<h3 id="adding-headwords-to-trees"><a href="#adding-headwords-to-trees">Adding Headwords to Trees</a></h3>
<ul>
<li>We’re going to add lexical information to each non-terminal to our tree.
<ul>
<li>Rather than one single “S”, we could have e.g. “S(questioned)”</li>
<li>Whereas we had around 50 non-terminals before, now we could have <span class="math">\(50 \times |V|\)</span>, where <span class="math">\(V\)</span> is vocabulary size, so in the thousands.</li>
</ul></li>
<li>We will propogate heads up the tree.</li>
</ul>
<h3 id="adding-headwords-to-trees-continued"><a href="#adding-headwords-to-trees-continued">Adding Headwords to Trees (Continued)</a></h3>
<ul>
<li>A constituent receives its <strong>headword</strong> from its <strong>head child</strong>.
<ul>
<li>S -&gt; NP <strong>VP</strong>, (S receives headword from VP)</li>
<li>VP -&gt; <strong>Vt</strong> NP, (VP receives headword from Vt)</li>
<li>NP -&gt; DT <strong>NN</strong>, (NP receives headword from NN).</li>
</ul></li>
<li>For rules that go to a terminal (e.g. DT -&gt; the), the contituent receives the word as its headword (e.g. DT becomes DT(the)).</li>
</ul>
<h3 id="chomsky-normal-form-1"><a href="#chomsky-normal-form-1">Chomsky Normal Form</a></h3>
<ul>
<li>(see earlier slide about CNF)</li>
<li>(recall the complexity of CYK algorithm is <span class="math">\(O(n^3 \times |N|^3)\)</span>, where <span class="math">\(n\)</span> is length of string being parsed, <span class="math">\(N\)</span> is number of non-terminals).</li>
</ul>
<h3 id="lexicalized-context-free-grammars-in-chomsky-normal-form"><a href="#lexicalized-context-free-grammars-in-chomsky-normal-form"> Lexicalized Context-Free Grammars in Chomsky Normal Form</a></h3>
<ul>
<li><span class="math">\(N\)</span> is a set of non-terminal symbols</li>
<li><span class="math">\(\Sigma\)</span> is a set of terminal symbols</li>
<li><span class="math">\(R\)</span> is a set of rules which take one of three forms:
<ul>
<li><span class="math">\(X(h) \rightarrow_1 Y_1(h) Y_2(w)\)</span> for <span class="math">\(X \in N\)</span>, and <span class="math">\(Y_1, Y_2 \in N\)</span>, and <span class="math">\(h, w \in \Sigma\)</span>.
<ul>
<li>e.g. VP(saw) <span class="math">\(\rightarrow_1\)</span> Vt(saw) NP(dog)</li>
<li>X = VP</li>
<li><span class="math">\(Y_1\)</span> = Vt</li>
<li><span class="math">\(Y_2\)</span> = NP</li>
<li>h = saw</li>
<li>w = dog</li>
<li>The subscript on the arrow indicates where the head word comes from, the first <span class="math">\(Y\)</span>.</li>
</ul></li>
<li><span class="math">\(X(h) \rightarrow_2 Y_1(w) Y_2(h)\)</span> for <span class="math">\(X \in N\)</span> and <span class="math">\(Y_1, Y_2 \in N\)</span> and <span class="math">\(h, w \in \Sigma\)</span>.
<ul>
<li>e.g. S(saw) <span class="math">\(\rightarrow_2\)</span> NP(man) VP(saw).</li>
</ul></li>
<li><span class="math">\(X(h) \in h\)</span> for <span class="math">\(X \in N\)</span> and <span class="math">\(h \in \Sigma\)</span>.
<ul>
<li>e.g. DT(the) -&gt; the, where X = DT and h = the.</li>
</ul></li>
</ul></li>
<li><span class="math">\(S \in N\)</span> is a distinguished start symbol.</li>
</ul>
<hr />
<ul>
<li>The subscript on the arrows are important because it disambiguates e.g.
<ul>
<li>NP(dog) -&gt; NN(dog) NN(dog).</li>
</ul></li>
</ul>
<h3 id="an-example-5"><a href="#an-example-5"> An Example</a></h3>
<p>S(saw) <span class="math">\(\rightarrow_2\)</span> NP(man) VP(saw)<br />NP(saw) <span class="math">\(\rightarrow_1\)</span> Vt(saw) NP(dog)<br />NP(man) <span class="math">\(\rightarrow_2\)</span> DT(the) NN(man)<br />NP(dog) <span class="math">\(\rightarrow_2\)</span> DT(the) NN(dog)<br />Vt(saw) <span class="math">\(\rightarrow\)</span> saw<br />DT(the) <span class="math">\(\rightarrow\)</span> the<br />NN(man) <span class="math">\(\rightarrow\)</span> man<br />NN(dog) <span class="math">\(\rightarrow\)</span> dog</p>
<ul>
<li>When drawing parse tree put a big dot on the branch that indicate where the head word is coming from.</li>
</ul>
<h3 id="parameters-in-a-lexicalized-pcfg"><a href="#parameters-in-a-lexicalized-pcfg">Parameters in a Lexicalized PCFG</a></h3>
<ul>
<li>An example parameter in a regular PCFG</li>
</ul>
<p><span class="math">\[q(\textrm{S} \rightarrow \textrm{NP VP})\]</span></p>
<ul>
<li>An example parameter in a Lexicalized PCFG</li>
</ul>
<p><span class="math">\[q(\textrm{S(saw)} \rightarrow_2 \textrm{NP(man) VP(saw)})\]</span></p>
<ul>
<li>Technically a Lexicalized PCFG is just a regular PCFG with many, many more non-terminals.</li>
<li>However, qualitatively what’s happened is that we have much less data per terminal.
<ul>
<li>This is a similar problem to language modelling with more n-grams, and will have a similar solution.</li>
</ul></li>
</ul>
<h3 id="parsing-with-lexicalized-cfgs"><a href="#parsing-with-lexicalized-cfgs">Parsing with Lexicalized CFGs</a></h3>
<ul>
<li>The new form of grammar looks just like a Chomsky normal form CFG, but with potentially <span class="math">\(O(|\Sigma|^2 \times |N|^3)\)</span> possible rules.</li>
<li>Naively, parsing an <span class="math">\(n\)</span> word sentence using the dynamic programming algorithm will take <span class="math">\(O(n^3 |\Sigma|^2 |N|^3)\)</span> time.
<ul>
<li>But <span class="math">\(|\Sigma|\)</span> can be <strong>huge</strong>!!</li>
</ul></li>
<li>Crucial observation: at most <span class="math">\(O(n^2 \times |N|^3)\)</span> rules can be applicable to a given sentence <span class="math">\(w_1, w_2, \ldots, w_n\)</span>, of length n. This is because any rules which contain a lexical item that is not one of <span class="math">\(w_1 \ldots w_n\)</span> an be safely discarded.
<ul>
<li>e.g. given “the dog saw the cat”</li>
<li>If rule is “S(questioned) -&gt;2 NP(dog) VP(questioned)”, it can be discarded because “questioned” not in sentence.</li>
</ul></li>
<li>The result: we can parse in <span class="math">\(O(n^5 |N|^3)\)</span> time.
<ul>
<li><span class="math">\(n^3\)</span> comes from dynamic programming algorithm.</li>
<li><span class="math">\(n^2\)</span> comes from the idea we need to use words from the sentence.</li>
<li><span class="math">\(|N|^3\)</span> comes from CNF usage of non-terminals.</li>
</ul></li>
</ul>
<h3 id="a-model-from-charniak-1997"><a href="#a-model-from-charniak-1997">A Model from Charniak (1997)</a></h3>
<ul>
<li>An example parameter in a Lexicalized PCFG:</li>
</ul>
<p><span class="math">\[q(\textrm{S(saw)} \rightarrow_2 \textrm{NP(man) VP(saw)})\]</span></p>
<ul>
<li>First step: decompose this parameter into a product of two parameters</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
        &amp; q(\textrm{S(saw)} \rightarrow_2 \textrm{NP(man) VP(saw)}) \\
      = &amp; q(\textrm{S} \rightarrow_2 \textrm{NP VP|S, saw}) \times q(\textrm{man|S} \rightarrow_2 \textrm{NP VP, saw})
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>First parameter: given that I have S(saw) what is the probability of re-writing it as “NP VP”?</li>
<li>Second parameter: say we have S(saw) and we know we have NP on the left and VP(saw) on the right.
<ul>
<li>What will fill in the NP(???)</li>
<li>What is the probability that “man” is chosen for the NP?</li>
</ul></li>
<li>Second step: use smoothed estimation for the two parameter estimates</li>
</ul>
<p><span class="math">\[
\begin{align}
    &amp;\begin{aligned}
      &amp; q(\textrm{S} \rightarrow_2 \textrm{NP VP | S, saw}) \\
    = &amp; \lambda_1 \times q_{ML}(\textrm{S} \rightarrow_2 \textrm{NP VP | S, saw}) + \lambda_2 \times q_{ML} (S \rightarrow_2 \textrm{NP VP | S}) \\ \\
      &amp; q(\textrm{man | S} \rightarrow_2 \textrm{NP VP, saw}) \\
    = &amp; \lambda_3 \times q_{ML} (\textrm{man | S} \rightarrow_2 \textrm{NP VP, saw}) + \lambda_4 \times q_{ML}(\textrm{man | S} \rightarrow_2 \textrm{NP VP}) + \lambda_5 \times q_{ML} (\textrm{man | NP})
    \end{aligned}
\end{align}
\]</span></p>
<ul>
<li>The first parameter:
<ul>
<li><p><span class="math">\(\lambda_1, \lambda_2 \ge 0\)</span>, <span class="math">\(\lambda_1 + \lambda_2 = 1\)</span>.</p></li>
<li><span class="math">\(q_{ML}(\textrm{S} \rightarrow_2 \textrm{NP VP | S, saw}) = \frac{\textrm{Count}(\textrm{S(saw)} \rightarrow_2 \textrm{NP VP})}{\textrm{Count}(\textrm{S(saw)})}\)</span>
<ul>
<li>Probability of S(saw) re-writing with this particular rule.</li>
<li>This estimate will be more detailed; uses more information.</li>
</ul></li>
<li><span class="math">\(q_{ML} (S \rightarrow_2 \textrm{NP VP | S}) = \frac{\textrm{Count}(\textrm{S} \rightarrow_2 \textrm{NP VP})}{\textrm{Count}(\textrm{S})}\)</span>
<ul>
<li>A backed-off estimate that completely ignores the lexical information “saw”.</li>
<li>Almost identical to a rule in a regular PCFG.</li>
<li>This estimate will be more robust; counts will be robust and will more closely reflect the “true” value.</li>
</ul></li>
<li>Robustness vs. sensitivity to lexical information is balanced using linear interpolation.
<ul>
<li>Again, this is bias vs. variance.</li>
</ul></li>
</ul></li>
<li>The second parameter:
<ul>
<li>Very similar reasoning.</li>
<li><span class="math">\(\lambda_3, \lambda_4, \lambda_5 \ge 0\)</span>, <span class="math">\(\lambda_3 + \lambda_4 + \lambda_5 = 1\)</span>.</li>
<li><span class="math">\(\lambda_3\)</span> asks “given S(saw), NP(???), and VP(saw), what is the probability of the NP having man?”</li>
<li>Throw away “saw”, <span class="math">\(\lambda_4\)</span> asks “given S(h), NP(???) and VP(h), where h is some head word, what’s the probability of the NP having man?”</li>
<li><span class="math">\(\lambda_4\)</span> asks “given NP(???) what’s the probability of the NP having man?”</li>
<li>Again, robustness vs. sensitivity, bias vs. variance.</li>
</ul></li>
</ul>
<h3 id="other-important-details"><a href="#other-important-details">Other Important Details</a></h3>
<ul>
<li>Need to deal with rules with more than two children, e.g.
<ul>
<li>VP(told) -&gt; V(told) NP(him) PP(on) SBAR(that)</li>
<li>One way is to binarize using method covered before, adding intermediate non-terminals.</li>
</ul></li>
<li>Need to incorporate parts of speech (useful in smoothing)
<ul>
<li>VP-V(told) -&gt; V(told) NP-PRP(him) PP-IN(on) SBAR-COMP(that)</li>
</ul></li>
<li>Need to encode preferences for close attachment.
<ul>
<li>“John was believed to have been shot by Bill”.</li>
</ul></li>
<li>Won’t be covered in this course. Further reading:
<ul>
<li>Michael Collins, 2003. “Head-Driven Statistical Models for Natural Language Parsing”. In Computational Linguistics.</li>
</ul></li>
</ul>
<h3 id="evaluation-representing-trees-as-constituents"><a href="#evaluation-representing-trees-as-constituents">Evaluation: Representing Trees as Constituents</a></h3>
<ul>
<li>Take some parse tree.</li>
<li>Break up into constituents with labels, start points, end points.</li>
<li>“the/1 lawyer/2 questioned/3 the/4 witness/5”</li>
</ul>
<table>
<thead>
<tr class="header">
<th align="left">Label</th>
<th align="left">Start Point</th>
<th align="left">End Point</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">NP</td>
<td align="left">1</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">NP</td>
<td align="left">4</td>
<td align="left">5</td>
</tr>
<tr class="odd">
<td align="left">VP</td>
<td align="left">3</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">S</td>
<td align="left">1</td>
<td align="left">5</td>
</tr>
</tbody>
</table>
<ul>
<li>We <strong>do not</strong> include parts of speech in this defintion. What’s missing is “the/DT lawyer/NN questioned/Vt the/DT witness/NN”.</li>
</ul>
<h3 id="precision-and-recall"><a href="#precision-and-recall"> Precision and Recall</a></h3>
<ul>
<li>Take the output from human parsed data, this is our <em>gold standard</em>.</li>
<li>Take output parsed using our PCFG.</li>
<li>Create two sets of tables for the constituents.</li>
<li><span class="math">\(G\)</span> = number of constituents in <em>gold standard</em> = 7.</li>
<li><span class="math">\(P\)</span> = number of constituents in <em>parse output</em> = 6.</li>
<li>$C = number correct = 6.
<ul>
<li>How many rules in the parse output are also in the gold standard?</li>
</ul></li>
<li>Recall = <span class="math">\(\textrm{100%} \times \frac{C}{G} = \textrm{100%} \times \frac{6}{7}\)</span></li>
<li>Precision = <span class="math">\(\textrm{100%} \times \frac{C}{P} = \textrm{100%} \times {6}{6}\)</span>.</li>
</ul>
<h3 id="results"><a href="#results">Results</a></h3>
<ul>
<li>Training data: 40k sentences from the Penn Wall Street Journal treebank. Testing: around 2.4k sentences from the Penn Wall Street Journal treebank.</li>
<li>Results for a PCFG: 70.6% Recall, 74.8% Precision.</li>
<li>Magerman (1994): 84.0% Recall, 84.3% Precision.
<ul>
<li>Based on Decision trees and bottom-up parser.</li>
</ul></li>
<li>Results for a lexicalized PCFG: 88.1% recall, 88.3% precision (from Collins (1997, 2003)</li>
<li>More recent results:
<ul>
<li>90.7%/91.4% (Carreras et al 2008)
<ul>
<li>Discriminative estimation</li>
</ul></li>
<li>91.7%/92.0% (Petrov 2010)
<ul>
<li>Latent-variable PCFGs</li>
</ul></li>
<li>91.2%/91.8% (Charniak and Johnson, 2005)
<ul>
<li>Discriminative estimation</li>
</ul></li>
</ul></li>
</ul>
<h3 id="evaluation-dependencies"><a href="#evaluation-dependencies">Evaluation: Dependencies</a></h3>
<ul>
<li>Recall two of the three rules of lexicalised PCFGs:
<ul>
<li><span class="math">\(X(h) \rightarrow_1 Y_1(h) Y_2(w)\)</span></li>
<li><span class="math">\(X(h) \rightarrow_2 Y_1(w) Y_2(h)\)</span></li>
</ul></li>
<li>see slides for lexicalised PCFG parse tree of “the man saw the dog with the telescope”</li>
<li>Dependencies (format: (h, w, rule))
<ul>
<li>(ROOT_0, saw_3, ROOT)</li>
<li>(saw_3, man_2, S -&gt;2 NP VP)</li>
<li>(man_2, the_1, NP -&gt;2 DT NN)</li>
<li>(saw_3, with_6, VP -&gt;1 VP PP)</li>
<li>(saw_3, dog_5, VP -&gt;1 Vt NP)</li>
<li>(dog_5, the_4, NP -&gt;2 DT NN)</li>
<li>(with_6, telescope_8, PP -&gt;1 IN NP)</li>
<li>(telescope_8, the_7, NP -&gt;2 DT NN)</li>
</ul></li>
<li>Always same number of dependencies as number of words, in this case 8.</li>
<li>Special dependency for the start symbol.</li>
</ul>
<h3 id="strengths-and-weaknesses-of-modern-parsers"><a href="#strengths-and-weaknesses-of-modern-parsers"> Strengths and Weaknesses of Modern Parsers</a></h3>
<p>(Numbers taken from Collins (2003))</p>
<ul>
<li>Subject-verb pairs (S <span class="math">\(\rightarrow_2\)</span> NP VP): over 95%/95% recall/precision.</li>
<li>Object-verb pairs (VP <span class="math">\(\rightarrow_1\)</span> Vt NP$) (“saw the man”): over 92%/92% recall/precision.</li>
<li>Other arguments to verbs (VP <em>1 Y</em>1 Y_2$): 92%</li>
<li>Non-recursive NP boundaries: 93%</li>
<li>PP attachments (<span class="math">\(X(h) \rightarrow Y_1(h) PP(w)\)</span>): 82%</li>
<li>Coordination ambiguities: 61%.</li>
<li>Takeaway
<ul>
<li>Core structure is good.</li>
<li>Modifiers cause difficulties.</li>
<li>This is from 1997, but would see same breakdown of per-type dependency accuracies in modern parsers.</li>
</ul></li>
</ul>
<h3 id="summary-3"><a href="#summary-3">Summary</a></h3>
<ul>
<li>Key weakness of PCFGs: lack of sensitivity to lexical information</li>
<li>Lexicalised PCFGs:
<ul>
<li>Lexicalize a treebank using head rules.</li>
<li>Estimate the parameters of a lexicalized PCFG using smoothed estimaton.</li>
</ul></li>
<li>Accuracy of lexicalized PCFGs: around 88% in recovering constituents or dependencies.</li>
</ul>
<h3 id="dependency-accuracies"><a href="#dependency-accuracies">Dependency Accuracies</a></h3>
<ul>
<li>All parses for a sentence with <span class="math">\(n\)</span> words have <span class="math">\(n\)</span> dependencies. Report a single figure, dependency accuracy.</li>
<li>Results from Collins 2003: 88.3% dependency accuracy.</li>
<li>Can calculate precision/recall on particular dependency <em>types</em>,
<ul>
<li>e.g. look at all subject/verb dependencies <span class="math">\(\implies\)</span> all dependencies with label <span class="math">\(\textrm{S} \rightarrow_2 \textrm{NP VP}\)</span>.</li>
<li>Recall = <span class="math">\(\frac{\textrm{number of subject/verb dependencies correct}}{\textrm{number of subject/verb dependencies in gold standard}}\)</span></li>
<li>Precision = <span class="math">\(\frac{\textrm{number of subject/verb dependencies correct}}{\textrm{number of subject/verb dependencies in parser&#39;s output}}\)</span></li>
</ul></li>
</ul>
<h2 id="readings"><a href="#readings">Readings</a></h2>
<h3 id="speech-and-language-processing-chapter-3-words-and-transducers"><a href="#speech-and-language-processing-chapter-3-words-and-transducers">Speech and Language Processing, Chapter 3 (Words and Transducers)</a></h3>
<h4 id="word-and-sentence-tokenization"><a href="#word-and-sentence-tokenization">3.9: Word and Sentence Tokenization</a></h4>
<ul>
<li>p75: <strong>Tokenization</strong>: segmenting running text into words and sentences.</li>
<li><p>Consider:</p>
<pre><code>Mr.  Sherwood said reaction to Sea
Containers&#39; proposal has been &quot;very
positive.&quot; In New York Stock Exchange
composite tradying yesterday, Sea Containers
closed at $62.625, up 62.5 cents.</code></pre></li>
<li>Notice that:
<ul>
<li>There could be double-spaces, which are just typos and can be considered a word delimeter.</li>
<li>With quotation marks the end of sentence period is <em>within</em> the quotation marks. The word <em>is not</em> <code>positive.&quot;</code>.</li>
<li>There may be numbers in a sentence.</li>
</ul></li>
<li>You might be tempted to treat punctuation as a word boundary.
<ul>
<li>But what about <code>m.p.h.</code>, <code>Ph.D</code>, <code>AT&amp;T</code>, <code>cap'n</code>, <code>01/02/06</code>, <code>google.com</code>.</li>
</ul></li>
<li>Also want to expand clitic contractions.
<ul>
<li><code>what're</code> becomes <code>what are</code>.</li>
<li>But apostrophes aren’t always clitic contractions, e.g. <code>her books' covers</code>.</li>
<li>Segmenting and expanding clitics can be done using <strong>morpological parsing</strong> presented in this chapter.</li>
</ul></li>
<li>Depending on your application you may want to parse multiple words as single tokens, for example <code>New York</code> or <code>rock 'n' roll</code>.
<ul>
<li>This requires a multiword expression dictionary of some sort.</li>
<li>Tokenization is hence very closely reliant on <strong>named entity detection</strong>.</li>
</ul></li>
<li>This is all just word segmentation.</li>
<li><strong>Sentence segmentation</strong> is also important.
<ul>
<li><code>?</code> and <code>!</code> are relatively unambiguous markers of sentence endings.</li>
<li><code>.</code> is more ambiguous.
<ul>
<li><code>Mr.</code>, <code>Inc.</code>, <code>he said &quot;howdy.&quot;</code></li>
<li>Sentence tokenization and word tokenization hence tend to be addressed together,</li>
</ul></li>
</ul></li>
<li>Sentence tokenization methods build a <em>binary classifier</em>, either using rules or machine learning, to decide if a period is part of a word or a sentence boundary marker.
<ul>
<li>Abbreviation dictionaries help to deal with abbreviations.</li>
<li>State of the art methods use machine learning, but a sequence of regular expressions is still useful.</li>
</ul></li>
<li>p77: Perl script based on Grefenstette, 1999.</li>
<li>p78: this is so simple that this suggests Finite State Transducers (FSTs) may also be easily implemented.
<ul>
<li>This is the case. Karttunen et. al 1996 and Beesley and Karttunen 2003 give descriptions.</li>
</ul></li>
</ul>
<h3 id="speech-and-language-processing-chapter-4-n-gram-models"><a href="#speech-and-language-processing-chapter-4-n-gram-models">Speech and Language Processing, Chapter 4 (n-gram models)</a></h3>
<ul>
<li>p96: a <strong>word</strong> is the full inflected or derived form of a word.
<ul>
<li>In English n-gram models are based on wordforms, not the <strong>lemmas</strong>, i.e. root.</li>
<li>e.g. cat is the lemma, cats is the inflected wordform.</li>
</ul></li>
<li>p96: n-gram models, and counting words in general, requires tokenization or text normalization; separating out punctuation, dealing with abbreviations, normalizing spelling, etc.
<ul>
<li>Covered in Chapter 3.</li>
</ul></li>
<li>p96: a <strong>type</strong> is a distinct word in a corpus.</li>
<li>p96: a <strong>token</strong> is any instance of a word in the corpus.</li>
<li>p102: typically divide our data ito 80% training, 10% development, and 10% test.</li>
<li>p104: quadrigram sentences based on Shakespeare are actually real Shakespeare.
<ul>
<li>The n-gram probability matrices are very sparse.</li>
</ul></li>
<li>p104: be sure to choose similar training and test copurses. Don’t choose from different genres.</li>
<li>p105: <strong>closed vocabulary</strong> assumes we know all the words in the vocabulary.
<ul>
<li>This can’t possibly be exactly true.</li>
<li>There will be <strong>out of vocabulary (OOV)</strong> words.</li>
<li>The percentive of OOV words in the test set is called the <strong>OOV rate</strong>.</li>
<li>An <strong>open vocabulary</strong> is one where we model OOV words by adding a pseudo-word called <code>&lt;UNK&gt;</code>. We train these probabilities as follows:
<ol style="list-style-type: decimal">
<li><em>Choose a fixed vocabulary</em> in advance.</li>
<li><em>Convert</em> in the training set any OOV word to the unknown word token <code>&lt;UNK&gt;</code> in a text normalization step.</li>
<li><em>Estimate</em> the probabilities for <code>&lt;UNK&gt;</code> from its counts just like any other regular word in the training set.</li>
</ol></li>
</ul></li>
<li>p105: <strong>extrinsic evaluation</strong> of language models is best; apply them to your problem and see which is best.</li>
<li>difficult in practice, so use <strong>intrinsic evaluation</strong> instead, which measures quality independent of any application.</li>
<li><strong>perplexity</strong> is the most common intrinsic evaluation metric.
<ul>
<li>Perplexity is a <strong>weighted average branching factor</strong> of a language. The number of possible next words that can follow any word.</li>
<li>p107: It is closely related to the information theoretic notion of entropy.</li>
</ul></li>
<li>p108: <strong>smoothing</strong> is modifications made to address poor estimates that are due to variability in small data sets.
<ul>
<li>pull in probabiliy mass from higher counts, pile it on to zero counts.</li>
</ul></li>
<li>p108: Laplacian smoothing.</li>
</ul>
<hr />
<p>p111: Good-Turing Discounting</p>
<ul>
<li>Use count of things you’ve seen <em>once</em> (<strong>singletons</strong> or <strong>hapax legomenons</strong>) to re-estimate the frequency of zero-count things.</li>
<li>The <strong>frequency of frequency c</strong> is the number of n-grams that occur c times.</li>
<li>More formally:</li>
</ul>
<p><span class="math">\[N_c = \sum_{x\;:\;\textrm{Count(x)} = c} 1\]</span></p>
<ul>
<li>The MLE count for <span class="math">\(N_c\)</span> is <span class="math">\(c\)</span>. The Good-Turing estimate replaces this with a smoothed count <span class="math">\(c^*\)</span>, as a function of <span class="math">\(N_{c+1}\)</span>:</li>
</ul>
<p><span class="math">\[c^* = (c+1)\frac{N_{c+1}}{N_c}\]</span></p>
<ul>
<li>We can use the equation above to replace the MLE counts for all the bins <span class="math">\(N_1, N_2, \ldots\)</span>.</li>
<li>However, instead of using this equation directly to re-estimate the smoothed count <span class="math">\(c^*\)</span> for <span class="math">\(N_0\)</span>, use the following which we can call the <strong>missing mass</strong>:</li>
</ul>
<p><span class="math">\[P_{GT}^{*}\;\textrm{(things with frequency zero in training)} = \frac{N_1}{N}\]</span></p>
<ul>
<li>Here <span class="math">\(N_1\)</span> is the count of items in bin 1, i.e. seen once in the training set, and <span class="math">\(N\)</span> is the total number of items we have seen in training.</li>
<li>p113: some advanced issues in Good-Turing estimation</li>
<li>p114: Good-Turing discounting is not used by itself; it’s only used in combination with backoff and interpolation, discused later.</li>
</ul>
<hr />
<ul>
<li>We can use an n-gram “hierarchy”, i.e. trigrams, bigrams, and unigrams.</li>
<li>In <strong>backoff</strong> if there is evidence of a higher order N-gram we use it exclusively.</li>
<li><p>In <strong>interpolation</strong> we always mix the probability esitmates of all N-gram estimators.</p></li>
<li>p115: interpolation.</li>
<li>p116: backoff
<ul>
<li>is better than interpolation</li>
<li>takes into account Good-Turing discounting.</li>
</ul></li>
</ul>
<hr />
<ul>
<li>p118: practical issues: toolkits and data formats</li>
<li>Since probabilities by definition are less than 1, the more probabilities we multiply together tha smaller they become.</li>
<li>Hence we use log probabilities rather than raw probabilities, and add in log space rather than multiply in linear space.</li>
<li>In order to report probabilities just take the “exp” of the logprob:</li>
</ul>
<p><span class="math">\[p_1 \times p_2 \times p_3 \times p_4 = exp(log p_1 + log p_2 + log p_3 + log p_4)\]</span></p>
<ul>
<li>Backoff N-gram language models are generally stored in <strong>ARPA format</strong>
<ul>
<li>Small header.</li>
<li>List of all non-zero N-gram probabilities (all unigrams, followed by bigrams, followed by trigrams, etc).</li>
<li>Each N-gram entry is stored with its discounted log probabiliy (in <span class="math">\(\textrm{log}_{10}\)</span> format) and its backoff weight <span class="math">\(\alpha\)</span>.</li>
<li>Backoff weights only necessary if the N-gram forms a prefix of a longer N-gram.</li>
<li>Thus, for trigram grammar, the format of each N-gram is:</li>
</ul></li>
</ul>
<p><TODO></p>
<ul>
<li><p>p119: e.g.</p>
<pre><code>\data\ 
ngram 1=1000
ngram 2=10000
ngram 3=5000

\1-grams:
-0.4405     &lt;/s&gt;
-99         &lt;s&gt;
-4.34443    the         -1.43973
-4.5325     dog         -4.3438
&lt;snip&gt;

\2-grams:
-3.43535    &lt;s&gt;     i     -5.353535
-4.43333    i       went  0.0430843
...

\3-grams:
-3.3245     &lt;s&gt;     i     prefer     3.434
...</code></pre></li>
<li>In training mode each toolkit takes a raw text file, one sentence per line, words separated by white-space.</li>
<li>It also takes parameters such as order <span class="math">\(N\)</span>, thresholds, type of discounting.</li>
<li><p>It outputs a language model in ARPA format.</p></li>
<li><p>In perplexity or decoding mode the toolkit take a language model in ARPA format, a sentence or corpus, and produces the probability and perplexity of the sentence or corpus.</p></li>
</ul>
<hr />
<p><all TODO></p>
<ul>
<li>p119: Advanced smoothing methods: Kneser-Ney Smoothing</li>
<li>p121: it turns out that any interpolation model can be represented as a backoff model, hence stored in ARPA backoff format.</li>
<li>p121: class-based N-grams.</li>
<li>p122: language model adaptation and using the web</li>
<li>use web search hits to estimate trigram language model parameters.</li>
<li><p>works well in practice, even though only getting page counts and not word counts back.</p></li>
<li>p122: using longer distance information: a brief summary</li>
<li>state of the art systems use 4-grams and 5-grams.</li>
<li>After 6-grams up to 20-grams, Goodman found that no useful improvement.</li>
<li><strong>cache</strong> model: use the preceding part of a test corpus and mix it into your trained language model when making predictions.
<ul>
<li>words are often repeated.</li>
<li>only works well in domains where you have perfect knowledge of words.</li>
</ul></li>
<li><strong>topic-based</strong>: train different language models for different kinds of words.</li>
<li><strong>latent-semantic indexing</strong>: measure probability based on the word’s similarity to preceding words, mix it in.</li>
<li><strong>trigger</strong>: a word that is not adjacent but highly related, so we mix it in.</li>
<li><strong>skip N-grams</strong>: we skip over an intermediate word.</li>
<li><p><strong>variable-length N-grams</strong>: adjust context size.</p></li>
<li><p>pruning by removing low-probability events is important, and essential on low-power platforms like cellphones.</p></li>
</ul>
<h3 id="arpa-language-model-lm-file-format"><a href="#arpa-language-model-lm-file-format">ARPA language model (LM) file format</a></h3>
<p>Example:</p>
<pre><code>\data\
ngram 1=19979
ngram 2=4987955
ngram 3=6136155

\1-grams:
-1.6682  A      -2.2371
-5.5975  A&#39;S    -0.2818
-2.8755  A.     -1.1409
-4.3297  A.&#39;S   -0.5886
-5.1432  A.S    -0.4862
...

\2-grams:
-3.4627  A  BABY    -0.2884
-4.8091  A  BABY&#39;S  -0.1659
-5.4763  A  BACH    -0.4722
-3.6622  A  BACK    -0.8814
...

\3-grams:
-4.3813  !SENT_START    A       CAMBRIDGE
-4.4782  !SENT_START    A       CAMEL
-4.0196  !SENT_START    A       CAMERA
-4.9004  !SENT_START    A       CAMP
-3.4319  !SENT_START    A       CAMPAIGN
...
\end\</code></pre>
<ul>
<li>Official reference: <a href="http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html">http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html</a>
<ul>
<li>If you need to represent 0 probability (noting that log 0 is minus infinity) just put “-99”, and when parsing interpret this as 0.</li>
<li>You’re allowed to put in other <code>key=value</code> pairs in the <code>\data\</code> section at the top (e.g. lambda’s for linear interpolation models, bucket sizes, etc.).</li>
</ul></li>
<li>Concise blog interpretation: <a href="http://kered.org/blog/2008-08-12/arpa-language-model-file-format/">http://kered.org/blog/2008-08-12/arpa-language-model-file-format/</a></li>
<li>Grammar-like interpretation: <a href="http://www.ee.ucla.edu/~weichu/htkbook/node243_ct.html">http://www.ee.ucla.edu/~weichu/htkbook/node243_ct.html</a></li>
<li>!!AI I think “start sentence” is <code>&lt;s&gt;</code> and “end sentence” is <code>&lt;/s&gt;</code>.</li>
<li>Note that for unigrams you have <code>log_prob word1 log_alpha</code>.</li>
<li>Note that for bigrams you have <code>log_prob word1 word2 log_alpha</code>.</li>
<li>Note that for trigrams there is no backoff parameter; it is useless because, if you note the definition of Katz-backoff, it’s never used.</li>
</ul>
<hr />
</body>
</html>
